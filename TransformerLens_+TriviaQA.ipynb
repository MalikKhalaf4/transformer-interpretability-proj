{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T17:38:33.526903200Z",
     "start_time": "2025-06-23T17:38:33.401504800Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XwreR5PRr_ux",
    "outputId": "e6ab6ffe-2143-4339-e93d-2ca1c84fc364"
   },
   "outputs": [],
   "source": [
    "#%pip install transformer_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T17:38:54.945448600Z",
     "start_time": "2025-06-23T17:38:33.427190100Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ye2RpUerr4im",
    "outputId": "8e6acad9-13ec-46eb-c733-a06cc6f37897"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7f902def2d70>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformer_lens import HookedEncoderDecoder\n",
    "import transformer_lens.utils as utils\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from transformer_lens.loading_from_pretrained import OFFICIAL_MODEL_NAMES\n",
    "\n",
    "import torch\n",
    "\n",
    "torch.set_grad_enabled(False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "doufxfLpvbGN"
   },
   "source": [
    "## Loading the Model in TransformerLens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_QyxxC8cw0PF"
   },
   "source": [
    "Please download the model first: https://cloud.anja.re/s/Qpo8CZ6yRzDH7ZF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T17:45:11.999647100Z",
     "start_time": "2025-06-23T17:38:54.943332900Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "5xCPM0GUr0d5",
    "outputId": "7056f183-d1af-448e-980d-f135eb390b73"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for T5 in TransformerLens is currently experimental, until such a time when it has feature parity with HookedTransformer and has been tested on real research tasks. Until then, backward compatibility is not guaranteed. Please see the docs for information on the limitations of the current implementation.\n",
      "If using T5 for interpretability research, keep in mind that T5 has some significant architectural differences to GPT. The major one is that T5 is an Encoder-Decoder modelAlso, it uses relative positional embeddings, different types of Attention (without bias) and LayerNorm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model ../DSI-large-TriviaQA into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf11c0150be14d6a84c7626f70ba1163",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08e81ce3b7f94605842031d8f0ef5b7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "086931e6a40040d8a5eddbb43120eb68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# !wget \"https://cloud.anja.re/s/qckH8GQPyN6YK8w/download?path=%2F&files=DSI-large-TriviaQA.zip\"\n",
    "# !unzip \"download?path=%2F&files=DSI-large-TriviaQA.zip\"\n",
    "checkpoint = \"../DSI-large-TriviaQA\"\n",
    "\n",
    "OFFICIAL_MODEL_NAMES.append(checkpoint)\n",
    "\n",
    "hf_model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "device = utils.get_device()\n",
    "model = HookedEncoderDecoder.from_pretrained(checkpoint, hf_model=hf_model, device=device)\n",
    "\n",
    "tokenizer_t5 = AutoTokenizer.from_pretrained('google-t5/t5-large')\n",
    "\n",
    "\n",
    "# Our model has a new token for each document id that we trained it on.\n",
    "\n",
    "# token id of first document that was added\n",
    "first_added_doc_id = len(tokenizer_t5)\n",
    "# token id of the last document that was added\n",
    "last_added_doc_id = len(tokenizer_t5) + (len(tokenizer) - len(tokenizer_t5))\n",
    "del tokenizer_t5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GXJ_CdZaveA2"
   },
   "source": [
    "## Running a sample query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T17:45:12.058387500Z",
     "start_time": "2025-06-23T17:45:12.012423900Z"
    },
    "id": "WjIsOKZ8Etj4"
   },
   "outputs": [],
   "source": [
    "#wget \"https://cloud.anja.re/s/qckH8GQPyN6YK8w/download?path=%2FGenIR-Data&files=TriviaQAData.zip\"\n",
    "#unzip \"download?path=%2FGenIR-Data&files=TriviaQAData.zip\"\n",
    "\n",
    "import json\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "with open(\"../TriviaQAData/test_queries_trivia_qa.json\", mode='r') as f:\n",
    "  test_data = json.load(f)\n",
    "\n",
    "class QuestionsDataset(Dataset):\n",
    "    def __init__(self, inputs, targets, ids):\n",
    "        self.inputs = inputs\n",
    "        self.targets = targets\n",
    "        self.ids = ids\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.inputs[idx], self.targets[idx], self.ids[idx]\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        input_texts, target_texts, ids = zip(*batch)\n",
    "        return input_texts, target_texts, ids\n",
    "\n",
    "\n",
    "queries = [entry['query'] for entry in test_data]\n",
    "ground_truths = [entry['relevant_docs'] for entry in test_data]\n",
    "query_ids = [entry['id'] for entry in test_data]\n",
    "dataset = QuestionsDataset(queries, ground_truths, query_ids)\n",
    "data_loader = DataLoader(dataset, batch_size=16, shuffle = False, collate_fn=dataset.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_activated_neurons(hooks, layer_indices):\n",
    "    result_dict = {}\n",
    "    for layer_id in layer_indices:\n",
    "        hook_post = hooks[f'decoder.{layer_id}.mlp.hook_post']\n",
    "        activated_neuron_indices = (hook_post > 0).nonzero(as_tuple=False)\n",
    "        result_dict[f'layer_{layer_id}'] = activated_neuron_indices\n",
    "    return result_dict\n",
    "\n",
    "def pad_relevant_docs(relevant_docs):\n",
    "    relevant_docs = [[int(item) for item in sublist] for sublist in relevant_docs]\n",
    "    max_len = 0\n",
    "    for sublist in relevant_docs:\n",
    "        if len(sublist) > max_len:\n",
    "            max_len = len(sublist)\n",
    "    padded_relevant_docs = []\n",
    "    for sublist in relevant_docs:\n",
    "        # Calculate how many padding elements are needed\n",
    "        num_padding = max_len - len(sublist)\n",
    "        # Create the padded sublist\n",
    "        padded_sublist = sublist + [-1] * num_padding\n",
    "        padded_relevant_docs.append(padded_sublist)\n",
    "    return padded_relevant_docs\n",
    "\n",
    "def extract_doc_ids_from_output(decoder_output):\n",
    "    doc_out_ids = []\n",
    "    for out in doc_out:\n",
    "        doc_id = re.findall(r\"@DOC_ID_([0-9]+)@\", decoder_output)\n",
    "        assert len(doc_id) <= 1\n",
    "        doc_out_ids.append(doc_id[0] if len(doc_id) else '-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'regex' has no attribute 'find'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[190], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m3\u001b[39m)))\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mlen\u001b[39m(\u001b[43mre\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(test)\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtet\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'regex' has no attribute 'find'"
     ]
    }
   ],
   "source": [
    "next(iter(range(3)))\n",
    "len(re.findall(r\"(test)\", \"tet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import regex as re\n",
    "from collections import defaultdict\n",
    "\n",
    "# Create a defaultdict where new keys will get the 'default_item'\n",
    "result_dict = defaultdict(lambda: {})\n",
    "count = 0\n",
    "for batch in data_loader:\n",
    "    # if count < 161:\n",
    "    #     count += 1\n",
    "    #     continue\n",
    "    layer_indices = range(17,24)\n",
    "    inputs, relevant_docs, queries = batch\n",
    "    inputs = list(inputs)\n",
    "    padded_relevant_docs = pad_relevant_docs(relevant_docs)\n",
    "    logits, hooks = model.run_with_cache(inputs)\n",
    "    doc_out = tokenizer.batch_decode(torch.argmax(logits, dim=-1).squeeze(1))\n",
    "    # doc_out_ids = re.findall(r\"@DOC_ID_([0-9]+)@\", doc_out)\n",
    "    doc_out_ids = []\n",
    "    for out in doc_out:\n",
    "        doc_id = re.findall(r\"@DOC_ID_([0-9]+)@\", out)\n",
    "        assert len(doc_id) <= 1\n",
    "        doc_out_ids.append(doc_id[0] if len(doc_id) else '-1')\n",
    "    print(f'Batch {count}/{len(data_loader)}:', doc_out_ids, len(doc_out_ids))\n",
    "    assert(len(doc_out_ids) == len(inputs))\n",
    "    padded_relevant_docs = torch.tensor(padded_relevant_docs, dtype=int)\n",
    "    doc_out_ids = torch.tensor(list(map(int, doc_out_ids)), dtype=int).unsqueeze(1)\n",
    "    correctly_answered = (padded_relevant_docs == doc_out_ids).any(dim=1)\n",
    "    correct_inputs = []\n",
    "    correct_queries = []\n",
    "    correct_doc_out_ids = doc_out_ids[correctly_answered]\n",
    "    correct_relevant_docs = padded_relevant_docs[correctly_answered]\n",
    "    # print(hooks['decoder.22.mlp.hook_post'].shape)\n",
    "    activated_neurons_dict = 1234(hooks, layer_indices)\n",
    "    for i in range(len(inputs)):\n",
    "        reached_idx = defaultdict(lambda: 0)\n",
    "        if correctly_answered[i]:\n",
    "            current_input_dict = {}\n",
    "            correct_inputs.append(inputs[i])\n",
    "            correct_queries.append(queries[i])\n",
    "            for layer in layer_indices:\n",
    "                activated_neurons_tmp_list = []\n",
    "                while reached_idx[f'layer_{layer}'] < len(activated_neurons_dict[f'layer_{layer}']):\n",
    "                    if i == activated_neurons_dict[f'layer_{layer}'][reached_idx[f'layer_{layer}']][0]:\n",
    "                        activated_neurons_tmp_list.append(activated_neurons_dict[f'layer_{layer}'][reached_idx[f'layer_{layer}']][-1].item()) #add the neuron idx\n",
    "                        reached_idx[f'layer_{layer}'] += 1\n",
    "                        # print(f'stopped here and next idx is:{activated_neurons_dict[f\"layer_{layer}\"][reached_idx[f\"layer_{layer}\"]][0]}')\n",
    "                    else:\n",
    "                        if reached_idx[f'layer_{layer}'] == 0: #first iteration therefore should skip the items until we reach it\n",
    "                            while activated_neurons_dict[f\"layer_{layer}\"][reached_idx[f\"layer_{layer}\"]][0] != i:\n",
    "                                reached_idx[f'layer_{layer}'] += 1\n",
    "                            continue\n",
    "                        break\n",
    "                current_input_dict[f'layer_{layer}'] = activated_neurons_tmp_list\n",
    "            result_dict[queries[i]] = { 'activated_neurons': current_input_dict, \"input\": inputs[i], 'correct_doc_id': doc_out_ids[i], 'relevant_docs': relevant_docs[i] }\n",
    "    count += 1\n",
    "# Save the dictionary to a JSON file\n",
    "with open(\"activated_neurons.json\", \"w\") as f:\n",
    "    json.dump(result_dict, f, indent=4) # indent for pretty printing\n",
    "# print(hooks['decoder.17.mlp.hook_post'][0][0][23])\n",
    "# print(activated_neurons_dict['layer_17'][:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_mlp_hook_function(target_token_pos, target_neuron_index, new_activation_value):\n",
    "    def modify_mlp_neuron_hook(\n",
    "        activation_tensor: torch.Tensor, \n",
    "        hook\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        A hook function to modify a specific MLP neuron's activation.\n",
    "        activation_tensor shape: [batch, position, n_mlp_neurons]\n",
    "        \"\"\"\n",
    "        print(f\"Hook fired at {hook.name}. Original activation value at \"\n",
    "              f\"pos {target_token_pos}, neuron {target_neuron_index}: \"\n",
    "              f\"{activation_tensor[:, target_token_pos, target_neuron_index]}\")\n",
    "              # f\"{activation_tensor[0, target_token_pos, target_neuron_index].item():.4f}\")\n",
    "    \n",
    "        # Modify the specific neuron's activation in-place\n",
    "        # We use [0] for batch dimension assuming a single prompt\n",
    "        activation_tensor[:, target_token_pos, target_neuron_index] = new_activation_value\n",
    "        # activation_tensor[0, target_token_pos, target_neuron_index] = new_activation_value\n",
    "    \n",
    "        print(f\"Modified activation to: \"\n",
    "              f\"{activation_tensor[:, target_token_pos, target_neuron_index]}\")\n",
    "              # f\"{activation_tensor[0, target_token_pos, target_neuron_index].item():.4f}\")\n",
    "    \n",
    "        return activation_tensor # Always return the modified tensor\n",
    "\n",
    "    return modify_mlp_neuron_hook\n",
    "\n",
    "def run_model_with_activation_hook(model, prompt, mlp_hook_name, neuron_index, neuron_new_value):\n",
    "    # mlp_hook_name = f\"blocks.{target_layer}.mlp.hook_post\"\n",
    "    # Now, run with the hook\n",
    "    modified_logits = model.run_with_hooks(\n",
    "        prompt,\n",
    "        fwd_hooks=[(mlp_hook_name, make_mlp_hook_function(0, neuron_index, neuron_new_value))]\n",
    "    )\n",
    "    hook_result = tokenizer.batch_decode(torch.argmax(modified_logits, dim=-1).squeeze(-1))\n",
    "\n",
    "    logits = model(prompt)\n",
    "    orig_result = tokenizer.batch_decode(torch.argmax(logits, dim=-1).squeeze(-1))\n",
    "    \n",
    "    print(f'original result:{orig_result}, and after using the hook:{hook_result}')\n",
    "    correct_count = 0\n",
    "    for i in range(len(orig_result)):\n",
    "        if orig_result[i] == hook_result[i]:\n",
    "            correct_count += 1\n",
    "    print(f'Total correct answered:{correct_count}/ {len(orig_result)}')\n",
    "\n",
    "def get_affected_prompts(model, queries_dict, mlp_hook_name, layer_index, neuron_index, neuron_new_value):\n",
    "    layer_activated_neuron_inputs, layer_activated_neurons_correct_doc_ids = [], []\n",
    "    for key in queries_dict:\n",
    "        if neuron_index in queries_dict[key]['activated_neurons'][f'layer_{layer_id}']:\n",
    "            layer_activated_neuron_inputs.append(results_copy[key]['input'])\n",
    "            layer_activated_neurons_correct_doc_ids.append(results_copy[key]['correct_doc_id'])\n",
    "    return run_model_with_activation_hook(model, layer_activated_neuron_inputs, mlp_hook_name, neuron_index, neuron_new_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hook fired at decoder.17.mlp.hook_post. Original activation value at pos 0, neuron 3079: tensor([1.6953e+01, 4.2768e+00, 1.2313e+01, 2.8081e+00, 1.0876e+01, 2.3650e+00,\n",
      "        4.9777e+00, 1.6263e+00, 3.2894e+00, 6.7698e+00, 6.8917e+00, 6.1033e+00,\n",
      "        4.2770e-01, 3.2351e+00, 6.8706e+00, 2.7763e+00, 1.5245e+00, 4.8938e+00,\n",
      "        1.4285e+01, 2.7602e+00, 2.1249e-01, 1.1253e+00, 1.3686e+01, 1.7106e+00,\n",
      "        8.0304e+00, 1.6549e+01, 1.8619e+01, 3.0922e+01, 4.3147e+00, 1.4808e+01,\n",
      "        1.1608e+01, 1.4639e+01, 1.0112e+00, 3.3695e+00, 7.2568e-01, 1.6056e+00,\n",
      "        2.3771e-02, 1.1469e+00, 1.7134e+00, 3.3589e+00, 4.6800e+00, 7.8607e+00,\n",
      "        2.9926e+00, 1.2749e+01, 4.1194e-02, 2.3855e-01, 1.2381e+01, 3.8436e+00,\n",
      "        4.6980e-01, 2.3818e+01, 1.2509e+00, 6.4127e+00, 6.4346e-02, 1.7243e+01,\n",
      "        9.1733e+00, 3.5950e+00, 2.2917e+01, 1.2488e+00, 7.8565e+00, 4.2466e+00,\n",
      "        3.7660e+00, 4.8297e+00, 1.4659e+01, 1.1176e+01, 2.2397e+01, 3.8647e-02,\n",
      "        5.9159e+00, 8.9398e+00, 7.5718e+00, 1.3381e+00, 5.3751e+00, 3.5083e+00,\n",
      "        3.6577e-01, 2.3603e+01, 8.7384e+00, 5.0264e+00, 7.6056e+00, 2.0174e+01,\n",
      "        1.1599e+00, 1.2642e+01, 3.8387e+00, 3.1021e+00, 8.7085e+00, 3.8870e+00,\n",
      "        2.1280e+01, 1.6710e+01, 6.8240e+00, 3.1572e+00, 1.2471e+01, 3.4775e-01,\n",
      "        5.3969e+00, 9.3040e+00, 7.6261e+00, 1.4671e+00, 8.6537e+00, 5.6411e+00,\n",
      "        7.7309e+00, 3.7668e+00, 4.5417e+00, 1.5359e+00, 6.3864e+00, 6.2553e+00,\n",
      "        4.0942e+00, 4.9948e+00, 4.7557e-01, 6.5401e+00, 3.0811e+00, 5.0528e+00,\n",
      "        1.1987e-01, 8.5387e-01, 4.1923e+00, 1.7230e+00, 2.3077e+01, 3.4084e+00,\n",
      "        2.6748e+00, 2.6264e+01, 2.2762e+00, 4.7397e+00, 2.8673e+00, 2.7995e+01,\n",
      "        3.0501e+00, 3.5041e+00, 2.2056e+01, 1.7727e+01, 6.8983e-01, 4.3629e+00,\n",
      "        1.9772e-01, 5.0728e+00, 7.1179e+00, 1.0456e+01, 1.8382e+01, 5.6687e+00,\n",
      "        5.2398e-02, 2.7658e+00, 1.2864e+00, 1.1507e+01, 5.3562e-01, 1.0538e+01,\n",
      "        6.1175e-01, 4.2290e+00, 1.9839e+01, 5.1903e+00, 4.9799e+00, 8.7919e+00,\n",
      "        1.1428e+00, 6.9787e+00, 5.5670e+00, 2.1225e+00, 6.1973e+00, 5.2796e+00,\n",
      "        5.3458e+00, 1.3971e+01, 1.4241e+01, 1.3184e+01, 1.2617e+00, 1.1859e+00,\n",
      "        7.9179e+00, 2.9762e+01, 4.9733e+00, 7.0448e-01, 3.6722e+00, 3.3287e+00,\n",
      "        5.3013e+00, 6.8186e-01, 1.8128e+01, 9.6042e+00, 3.2248e+00, 7.6519e+00,\n",
      "        4.6517e-01, 1.0250e+01, 1.1272e+01, 2.2246e+01, 1.9531e+00, 7.7384e+00,\n",
      "        4.1143e+00, 9.2957e+00, 6.1957e+00, 1.8860e+01, 3.2679e+00, 1.3295e-01,\n",
      "        4.5103e+00, 4.8320e+00, 2.0595e+01, 4.3574e+00, 1.8984e+01, 4.0420e+00,\n",
      "        2.6873e+01, 1.3974e+01, 5.2584e+00, 9.4336e-01, 2.5555e+01, 2.7755e+01,\n",
      "        6.0396e+00, 1.9083e+01, 1.0334e+01, 4.8470e+00, 1.3619e-02, 4.4207e+00,\n",
      "        2.8907e+01, 1.3736e+00, 7.0973e+00, 8.7564e+00, 7.4775e+00, 7.3309e+00,\n",
      "        1.4235e+01, 1.7134e+01, 8.4898e+00, 1.1874e+00, 8.4544e+00, 1.0279e+01,\n",
      "        7.9000e+00, 1.2780e+00, 3.6387e+00, 9.5751e+00, 3.0902e+00, 2.0579e+01,\n",
      "        9.2412e-01, 1.5457e+01, 1.5445e+00, 1.6207e+01, 1.0969e+00, 1.3498e+00,\n",
      "        1.2089e+00, 8.0147e-01, 2.7602e+01, 1.6790e+01, 2.5852e+00, 2.4126e+00,\n",
      "        2.1522e+01, 3.8739e+00, 8.2552e+00, 2.2575e+00, 1.1449e+01, 7.9120e+00,\n",
      "        3.3239e+00, 7.7896e+00, 9.3727e+00, 5.6755e+00, 5.7078e+00, 1.1323e+01,\n",
      "        7.2830e+00, 2.9252e+00, 1.1110e+01, 1.0911e+00, 2.7465e+00, 6.4466e+00,\n",
      "        4.7890e+00, 2.0400e+00, 1.0436e+01, 8.6079e+00, 7.0763e+00, 1.2613e+01,\n",
      "        1.7648e-01, 1.1381e+01, 2.0324e+01, 1.7522e+01, 2.5603e+00, 2.5626e+00,\n",
      "        3.3192e+01, 2.8159e+01, 9.8876e+00, 5.3402e+00, 6.0544e-01, 5.1948e+00,\n",
      "        4.3680e+00, 1.0917e+01, 9.9243e+00, 1.2715e+01, 1.0591e+01, 1.3500e+00,\n",
      "        9.0612e+00, 1.2050e+01, 8.4980e+00, 1.7294e+00, 7.4913e+00, 1.7688e+01,\n",
      "        6.7408e+00, 4.2917e+00, 6.8965e+00, 4.7223e+00, 1.7151e+01, 9.8197e+00],\n",
      "       device='cuda:0')\n",
      "Modified activation to: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       device='cuda:0')\n",
      "original result:['@DOC_ID_21871@', '@DOC_ID_52288@', '@DOC_ID_73330@', '@DOC_ID_51159@', '@DOC_ID_46835@', '@DOC_ID_36924@', '@DOC_ID_45706@', '@DOC_ID_44277@', '@DOC_ID_64053@', '@DOC_ID_36924@', '@DOC_ID_59339@', '@DOC_ID_42477@', '@DOC_ID_2652@', '@DOC_ID_60098@', '@DOC_ID_35260@', '@DOC_ID_15709@', '@DOC_ID_31637@', '@DOC_ID_32719@', '@DOC_ID_61753@', '@DOC_ID_4844@', '@DOC_ID_10108@', '@DOC_ID_29884@', '@DOC_ID_15984@', '@DOC_ID_47511@', '@DOC_ID_11180@', '@DOC_ID_68679@', '@DOC_ID_3704@', '@DOC_ID_38319@', '@DOC_ID_24086@', '@DOC_ID_50826@', '@DOC_ID_62247@', '@DOC_ID_4001@', '@DOC_ID_42477@', '@DOC_ID_69817@', '@DOC_ID_1258@', '@DOC_ID_48443@', '@DOC_ID_10722@', '@DOC_ID_50644@', '@DOC_ID_46479@', '@DOC_ID_58069@', '@DOC_ID_51772@', '@DOC_ID_2398@', '@DOC_ID_17691@', '@DOC_ID_58362@', '@DOC_ID_30380@', '@DOC_ID_30136@', '@DOC_ID_18239@', '@DOC_ID_49706@', '@DOC_ID_16114@', '@DOC_ID_22955@', '@DOC_ID_68688@', '@DOC_ID_35931@', '@DOC_ID_20957@', '@DOC_ID_3431@', '@DOC_ID_50707@', '@DOC_ID_57300@', '@DOC_ID_53980@', '@DOC_ID_20933@', '@DOC_ID_57821@', '@DOC_ID_24845@', '@DOC_ID_42275@', '@DOC_ID_40624@', '@DOC_ID_14342@', '@DOC_ID_26276@', '@DOC_ID_31804@', '@DOC_ID_42220@', '@DOC_ID_55777@', '@DOC_ID_10891@', '@DOC_ID_46235@', '@DOC_ID_12794@', '@DOC_ID_24623@', '@DOC_ID_45802@', '@DOC_ID_53556@', '@DOC_ID_68784@', '@DOC_ID_41578@', '@DOC_ID_9697@', '@DOC_ID_46691@', '@DOC_ID_34947@', '@DOC_ID_57300@', '@DOC_ID_52420@', '@DOC_ID_22955@', '@DOC_ID_44989@', '@DOC_ID_10722@', '@DOC_ID_37630@', '@DOC_ID_52608@', '@DOC_ID_37524@', '@DOC_ID_1972@', '@DOC_ID_28808@', '@DOC_ID_73555@', '@DOC_ID_25690@', '@DOC_ID_59046@', '@DOC_ID_49060@', '@DOC_ID_16396@', '@DOC_ID_70261@', '@DOC_ID_58675@', '@DOC_ID_3767@', '@DOC_ID_48443@', '@DOC_ID_17678@', '@DOC_ID_73509@', '@DOC_ID_56830@', '@DOC_ID_60198@', '@DOC_ID_60162@', '@DOC_ID_1695@', '@DOC_ID_8930@', '@DOC_ID_24996@', '@DOC_ID_38345@', '@DOC_ID_69320@', '@DOC_ID_40999@', '@DOC_ID_53505@', '@DOC_ID_67711@', '@DOC_ID_39420@', '@DOC_ID_25404@', '@DOC_ID_68759@', '@DOC_ID_884@', '@DOC_ID_12890@', '@DOC_ID_16860@', '@DOC_ID_58056@', '@DOC_ID_72584@', '@DOC_ID_40853@', '@DOC_ID_58812@', '@DOC_ID_19936@', '@DOC_ID_19503@', '@DOC_ID_20415@', '@DOC_ID_14114@', '@DOC_ID_66232@', '@DOC_ID_24623@', '@DOC_ID_30796@', '@DOC_ID_69231@', '@DOC_ID_33147@', '@DOC_ID_35624@', '@DOC_ID_2179@', '@DOC_ID_33147@', '@DOC_ID_5028@', '@DOC_ID_49265@', '@DOC_ID_1258@', '@DOC_ID_45287@', '@DOC_ID_44266@', '@DOC_ID_63235@', '@DOC_ID_41909@', '@DOC_ID_7292@', '@DOC_ID_20774@', '@DOC_ID_48207@', '@DOC_ID_19318@', '@DOC_ID_24958@', '@DOC_ID_69231@', '@DOC_ID_20933@', '@DOC_ID_21681@', '@DOC_ID_29058@', '@DOC_ID_69244@', '@DOC_ID_49374@', '@DOC_ID_25817@', '@DOC_ID_23041@', '@DOC_ID_21107@', '@DOC_ID_16312@', '@DOC_ID_55172@', '@DOC_ID_14342@', '@DOC_ID_9775@', '@DOC_ID_62630@', '@DOC_ID_28182@', '@DOC_ID_6789@', '@DOC_ID_63446@', '@DOC_ID_4809@', '@DOC_ID_3534@', '@DOC_ID_27161@', '@DOC_ID_15791@', '@DOC_ID_53032@', '@DOC_ID_24845@', '@DOC_ID_65831@', '@DOC_ID_16419@', '@DOC_ID_49819@', '@DOC_ID_72118@', '@DOC_ID_32895@', '@DOC_ID_24845@', '@DOC_ID_4280@', '@DOC_ID_42665@', '@DOC_ID_8875@', '@DOC_ID_1755@', '@DOC_ID_60966@', '@DOC_ID_16485@', '@DOC_ID_7777@', '@DOC_ID_59339@', '@DOC_ID_60168@', '@DOC_ID_59753@', '@DOC_ID_9840@', '@DOC_ID_63429@', '@DOC_ID_59339@', '@DOC_ID_1505@', '@DOC_ID_45326@', '@DOC_ID_31951@', '@DOC_ID_54854@', '@DOC_ID_14463@', '@DOC_ID_14463@', '@DOC_ID_10045@', '@DOC_ID_24884@', '@DOC_ID_17835@', '@DOC_ID_11822@', '@DOC_ID_48442@', '@DOC_ID_24744@', '@DOC_ID_30191@', '@DOC_ID_3658@', '@DOC_ID_13239@', '@DOC_ID_15304@', '@DOC_ID_49697@', '@DOC_ID_63611@', '@DOC_ID_20316@', '@DOC_ID_14342@', '@DOC_ID_17985@', '@DOC_ID_53482@', '@DOC_ID_33613@', '@DOC_ID_14400@', '@DOC_ID_26743@', '@DOC_ID_5996@', '@DOC_ID_56228@', '@DOC_ID_33144@', '@DOC_ID_52740@', '@DOC_ID_53571@', '@DOC_ID_49298@', '@DOC_ID_46734@', '@DOC_ID_24845@', '@DOC_ID_43620@', '@DOC_ID_6952@', '@DOC_ID_61759@', '@DOC_ID_27161@', '@DOC_ID_59314@', '@DOC_ID_8653@', '@DOC_ID_47400@', '@DOC_ID_57821@', '@DOC_ID_36311@', '@DOC_ID_39195@', '@DOC_ID_72601@', '@DOC_ID_17549@', '@DOC_ID_30263@', '@DOC_ID_68784@', '@DOC_ID_20427@', '@DOC_ID_72978@', '@DOC_ID_815@', '@DOC_ID_23538@', '@DOC_ID_61130@', '@DOC_ID_24836@', '@DOC_ID_63911@', '@DOC_ID_17757@', '@DOC_ID_62599@', '@DOC_ID_18554@', '@DOC_ID_51796@', '@DOC_ID_61990@', '@DOC_ID_32331@', '@DOC_ID_52795@', '@DOC_ID_27161@', '@DOC_ID_8331@', '@DOC_ID_42730@', '@DOC_ID_60795@', '@DOC_ID_11896@', '@DOC_ID_2318@', '@DOC_ID_47868@', '@DOC_ID_21162@', '@DOC_ID_41414@', '@DOC_ID_17695@', '@DOC_ID_65644@', '@DOC_ID_43844@', '@DOC_ID_41153@', '@DOC_ID_61759@', '@DOC_ID_70182@', '@DOC_ID_34021@', '@DOC_ID_42665@', '@DOC_ID_52942@', '@DOC_ID_29935@', '@DOC_ID_57177@', '@DOC_ID_68663@', '@DOC_ID_53714@', '@DOC_ID_69122@', '@DOC_ID_44229@', '@DOC_ID_49819@', '@DOC_ID_71364@', '@DOC_ID_14342@', '@DOC_ID_70182@', '@DOC_ID_44658@', '@DOC_ID_7777@', '@DOC_ID_59753@', '@DOC_ID_12335@', '@DOC_ID_14192@', '@DOC_ID_28599@', '@DOC_ID_71576@'], and after using the hook:['@DOC_ID_21871@', '@DOC_ID_52288@', '@DOC_ID_73330@', '@DOC_ID_51159@', '@DOC_ID_46835@', '@DOC_ID_36924@', '@DOC_ID_45706@', '@DOC_ID_44277@', '@DOC_ID_64053@', '@DOC_ID_36924@', '@DOC_ID_59339@', '@DOC_ID_42477@', '@DOC_ID_2652@', '@DOC_ID_60098@', '@DOC_ID_35260@', '@DOC_ID_15709@', '@DOC_ID_31637@', '@DOC_ID_32719@', '@DOC_ID_61753@', '@DOC_ID_4844@', '@DOC_ID_10108@', '@DOC_ID_29884@', '@DOC_ID_15984@', '@DOC_ID_47511@', '@DOC_ID_11180@', '@DOC_ID_68679@', '@DOC_ID_3704@', '@DOC_ID_38319@', '@DOC_ID_24086@', '@DOC_ID_50826@', '@DOC_ID_62247@', '@DOC_ID_4001@', '@DOC_ID_42477@', '@DOC_ID_69817@', '@DOC_ID_1258@', '@DOC_ID_48443@', '@DOC_ID_10722@', '@DOC_ID_50644@', '@DOC_ID_46479@', '@DOC_ID_58069@', '@DOC_ID_51772@', '@DOC_ID_2398@', '@DOC_ID_17691@', '@DOC_ID_58362@', '@DOC_ID_30380@', '@DOC_ID_30136@', '@DOC_ID_18239@', '@DOC_ID_49706@', '@DOC_ID_16114@', '@DOC_ID_22955@', '@DOC_ID_68688@', '@DOC_ID_35931@', '@DOC_ID_20957@', '@DOC_ID_3431@', '@DOC_ID_50707@', '@DOC_ID_57300@', '@DOC_ID_53980@', '@DOC_ID_20933@', '@DOC_ID_57821@', '@DOC_ID_24845@', '@DOC_ID_42275@', '@DOC_ID_40624@', '@DOC_ID_14342@', '@DOC_ID_26276@', '@DOC_ID_31804@', '@DOC_ID_42220@', '@DOC_ID_55777@', '@DOC_ID_10891@', '@DOC_ID_46235@', '@DOC_ID_12794@', '@DOC_ID_24623@', '@DOC_ID_45802@', '@DOC_ID_53556@', '@DOC_ID_68784@', '@DOC_ID_41578@', '@DOC_ID_9697@', '@DOC_ID_46691@', '@DOC_ID_34947@', '@DOC_ID_57300@', '@DOC_ID_52420@', '@DOC_ID_22955@', '@DOC_ID_44989@', '@DOC_ID_10722@', '@DOC_ID_37630@', '@DOC_ID_52608@', '@DOC_ID_37524@', '@DOC_ID_1972@', '@DOC_ID_28808@', '@DOC_ID_73555@', '@DOC_ID_25690@', '@DOC_ID_59046@', '@DOC_ID_49060@', '@DOC_ID_16396@', '@DOC_ID_70261@', '@DOC_ID_58675@', '@DOC_ID_3767@', '@DOC_ID_48443@', '@DOC_ID_17678@', '@DOC_ID_73509@', '@DOC_ID_56830@', '@DOC_ID_60198@', '@DOC_ID_60162@', '@DOC_ID_1695@', '@DOC_ID_8930@', '@DOC_ID_24996@', '@DOC_ID_38345@', '@DOC_ID_69320@', '@DOC_ID_40999@', '@DOC_ID_53505@', '@DOC_ID_67711@', '@DOC_ID_39420@', '@DOC_ID_25404@', '@DOC_ID_68759@', '@DOC_ID_884@', '@DOC_ID_12890@', '@DOC_ID_16860@', '@DOC_ID_58056@', '@DOC_ID_72584@', '@DOC_ID_40853@', '@DOC_ID_58812@', '@DOC_ID_19936@', '@DOC_ID_19503@', '@DOC_ID_5604@', '@DOC_ID_14114@', '@DOC_ID_66232@', '@DOC_ID_24623@', '@DOC_ID_30796@', '@DOC_ID_69231@', '@DOC_ID_33147@', '@DOC_ID_35624@', '@DOC_ID_2179@', '@DOC_ID_33147@', '@DOC_ID_5028@', '@DOC_ID_49265@', '@DOC_ID_1258@', '@DOC_ID_45287@', '@DOC_ID_44266@', '@DOC_ID_63235@', '@DOC_ID_41909@', '@DOC_ID_7292@', '@DOC_ID_20774@', '@DOC_ID_48207@', '@DOC_ID_19318@', '@DOC_ID_24958@', '@DOC_ID_69231@', '@DOC_ID_20933@', '@DOC_ID_21681@', '@DOC_ID_29058@', '@DOC_ID_69244@', '@DOC_ID_49374@', '@DOC_ID_25817@', '@DOC_ID_23041@', '@DOC_ID_21107@', '@DOC_ID_16312@', '@DOC_ID_55172@', '@DOC_ID_14342@', '@DOC_ID_9775@', '@DOC_ID_62630@', '@DOC_ID_28182@', '@DOC_ID_6789@', '@DOC_ID_63446@', '@DOC_ID_4809@', '@DOC_ID_3534@', '@DOC_ID_27161@', '@DOC_ID_15791@', '@DOC_ID_53032@', '@DOC_ID_24845@', '@DOC_ID_65831@', '@DOC_ID_16419@', '@DOC_ID_49819@', '@DOC_ID_72118@', '@DOC_ID_32895@', '@DOC_ID_24845@', '@DOC_ID_4280@', '@DOC_ID_42665@', '@DOC_ID_8875@', '@DOC_ID_1755@', '@DOC_ID_60966@', '@DOC_ID_16485@', '@DOC_ID_7777@', '@DOC_ID_59339@', '@DOC_ID_60168@', '@DOC_ID_59753@', '@DOC_ID_9840@', '@DOC_ID_63429@', '@DOC_ID_59339@', '@DOC_ID_1505@', '@DOC_ID_45326@', '@DOC_ID_31951@', '@DOC_ID_54854@', '@DOC_ID_14463@', '@DOC_ID_14463@', '@DOC_ID_10045@', '@DOC_ID_24884@', '@DOC_ID_17835@', '@DOC_ID_11822@', '@DOC_ID_48442@', '@DOC_ID_24744@', '@DOC_ID_30191@', '@DOC_ID_3658@', '@DOC_ID_13239@', '@DOC_ID_15304@', '@DOC_ID_49697@', '@DOC_ID_63611@', '@DOC_ID_20316@', '@DOC_ID_14342@', '@DOC_ID_17985@', '@DOC_ID_53482@', '@DOC_ID_33613@', '@DOC_ID_14400@', '@DOC_ID_26743@', '@DOC_ID_5996@', '@DOC_ID_56228@', '@DOC_ID_33144@', '@DOC_ID_52740@', '@DOC_ID_53571@', '@DOC_ID_49298@', '@DOC_ID_46734@', '@DOC_ID_24845@', '@DOC_ID_43620@', '@DOC_ID_6952@', '@DOC_ID_61759@', '@DOC_ID_27161@', '@DOC_ID_59314@', '@DOC_ID_8653@', '@DOC_ID_47400@', '@DOC_ID_57821@', '@DOC_ID_36311@', '@DOC_ID_39195@', '@DOC_ID_72601@', '@DOC_ID_17549@', '@DOC_ID_30263@', '@DOC_ID_68784@', '@DOC_ID_20427@', '@DOC_ID_72978@', '@DOC_ID_815@', '@DOC_ID_23538@', '@DOC_ID_61130@', '@DOC_ID_24836@', '@DOC_ID_63911@', '@DOC_ID_17757@', '@DOC_ID_62599@', '@DOC_ID_18554@', '@DOC_ID_51796@', '@DOC_ID_61990@', '@DOC_ID_32331@', '@DOC_ID_52795@', '@DOC_ID_27161@', '@DOC_ID_8331@', '@DOC_ID_42730@', '@DOC_ID_60795@', '@DOC_ID_11896@', '@DOC_ID_2318@', '@DOC_ID_47868@', '@DOC_ID_21162@', '@DOC_ID_41414@', '@DOC_ID_17695@', '@DOC_ID_65644@', '@DOC_ID_43844@', '@DOC_ID_41153@', '@DOC_ID_61759@', '@DOC_ID_70182@', '@DOC_ID_34021@', '@DOC_ID_42665@', '@DOC_ID_52942@', '@DOC_ID_29935@', '@DOC_ID_57177@', '@DOC_ID_68663@', '@DOC_ID_53714@', '@DOC_ID_69122@', '@DOC_ID_44229@', '@DOC_ID_49819@', '@DOC_ID_71364@', '@DOC_ID_14342@', '@DOC_ID_70182@', '@DOC_ID_44658@', '@DOC_ID_7777@', '@DOC_ID_59753@', '@DOC_ID_12335@', '@DOC_ID_14192@', '@DOC_ID_28599@', '@DOC_ID_71576@']\n",
      "Total correct answered:281/ 282\n"
     ]
    }
   ],
   "source": [
    "hook_layer_id = 23\n",
    "hook_neuron_id = 3079\n",
    "mlp_hook_name = f'decoder.{layer_id}.mlp.hook_post'\n",
    "hook_new_value = 0.0\n",
    "# prompt = \"For which county does Jonathan Trott play cricket?\"\n",
    "# run_model_with_activation_hook(model, prompt, mlp_hook_name, hook_neuron_id, hook_new_value)\n",
    "get_affected_prompts(model, results_copy, mlp_hook_name, hook_layer_id, hook_neuron_id, hook_new_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron_id = 3031\n",
    "layer_id = 17\n",
    "# layer_17_neuron_3027 = []\n",
    "layer_activated_neuron_inputs = []\n",
    "layer_activated_neurons_correct_doc_ids = []\n",
    "for key in results_copy:\n",
    "    if neuron_id in results_copy[key]['activated_neurons'][f'layer_{layer_id}']:\n",
    "        layer_activated_neuron_inputs.append(results_copy[key]['input'])\n",
    "        layer_activated_neurons_correct_doc_ids.append(results_copy[key]['correct_doc_id'])\n",
    "        # layer_17_neuron_3027.append([results_copy[key]['input'], results_copy[key]['correct_doc_id']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "a Tensor with 2 elements cannot be converted to Scalar",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[290], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# print(layer_activated_neuron_inputs)\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# model(layer_activated_neuron_inputs)\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m tmp \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: a Tensor with 2 elements cannot be converted to Scalar"
     ]
    }
   ],
   "source": [
    "# print(layer_activated_neuron_inputs)\n",
    "# model(layer_activated_neuron_inputs)\n",
    "tmp = torch.Tensor([0,0]).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@DOC_ID_37981@'"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#['For which county does Jonathan Trott play cricket?', 37981],\n",
    "logits = model(\"For which county does Jonathan Trott play cricket?\")\n",
    "tokenizer.decode(torch.argmax(logits, dim=-1).squeeze(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21871\n",
      "{'activated_neurons': {'layer_17': [10, 22, 28, 42, 43, 51, 56, 59, 79, 89, 107, 108, 128, 130, 134, 137, 157, 184, 221, 245, 277, 286, 349, 361, 381, 382, 396, 406, 408, 431, 442, 457, 501, 502, 558, 564, 576, 587, 595, 603, 622, 626, 639, 641, 652, 672, 678, 679, 686, 694, 695, 702, 719, 727, 734, 765, 775, 789, 793, 807, 812, 819, 853, 906, 921, 942, 946, 1002, 1021, 1058, 1067, 1078, 1093, 1095, 1100, 1134, 1136, 1150, 1210, 1218, 1242, 1250, 1260, 1276, 1316, 1380, 1392, 1420, 1432, 1485, 1517, 1536, 1539, 1610, 1644, 1663, 1683, 1699, 1700, 1705, 1712, 1731, 1733, 1761, 1767, 1832, 1862, 1874, 1890, 1915, 1923, 1940, 1979, 2002, 2013, 2014, 2016, 2047, 2099, 2110, 2123, 2146, 2153, 2184, 2211, 2239, 2283, 2352, 2383, 2388, 2398, 2407, 2419, 2428, 2429, 2441, 2449, 2458, 2460, 2462, 2499, 2502, 2510, 2520, 2526, 2572, 2573, 2579, 2605, 2643, 2648, 2666, 2688, 2692, 2699, 2700, 2712, 2736, 2756, 2757, 2768, 2796, 2797, 2799, 2812, 2824, 2845, 2872, 2873, 2874, 2893, 2917, 2932, 2935, 2979, 3020, 3069, 3078, 3079, 3107, 3126, 3127, 3140, 3142, 3157, 3162, 3198, 3204, 3216, 3246, 3288, 3304, 3309, 3328, 3336, 3362, 3433, 3454, 3469, 3473, 3495, 3502, 3509, 3515, 3519, 3528, 3544, 3558, 3572, 3573, 3590, 3593, 3608, 3652, 3671, 3672, 3681, 3682, 3700, 3718, 3726, 3731, 3756, 3764, 3765, 3794, 3800, 3832, 3833, 3915, 3923, 3971, 3977, 4014, 4027, 4083, 4094], 'layer_18': [13, 22, 36, 38, 44, 72, 98, 101, 104, 112, 136, 139, 153, 159, 160, 188, 189, 207, 208, 212, 234, 248, 252, 264, 286, 288, 304, 353, 358, 373, 385, 387, 388, 400, 425, 431, 438, 447, 494, 497, 505, 509, 515, 536, 556, 581, 586, 594, 601, 603, 621, 651, 653, 675, 687, 690, 691, 713, 716, 755, 785, 790, 793, 812, 817, 833, 902, 931, 979, 1015, 1040, 1053, 1055, 1066, 1083, 1094, 1113, 1123, 1130, 1139, 1145, 1159, 1162, 1169, 1176, 1197, 1220, 1222, 1231, 1238, 1278, 1284, 1291, 1312, 1313, 1327, 1333, 1383, 1391, 1398, 1403, 1408, 1419, 1442, 1454, 1456, 1460, 1469, 1470, 1471, 1502, 1506, 1509, 1514, 1553, 1578, 1579, 1630, 1635, 1666, 1669, 1685, 1687, 1702, 1705, 1743, 1779, 1785, 1787, 1793, 1794, 1810, 1819, 1868, 1885, 1906, 1909, 1915, 1917, 1919, 1930, 1932, 1943, 1949, 1953, 1954, 1956, 1970, 1990, 1996, 1999, 2003, 2015, 2024, 2034, 2036, 2071, 2078, 2087, 2088, 2117, 2121, 2155, 2175, 2212, 2218, 2240, 2243, 2257, 2262, 2272, 2300, 2320, 2339, 2355, 2414, 2423, 2427, 2429, 2433, 2434, 2445, 2451, 2461, 2492, 2506, 2534, 2539, 2545, 2554, 2568, 2569, 2659, 2668, 2672, 2688, 2706, 2709, 2732, 2739, 2752, 2763, 2768, 2772, 2781, 2784, 2793, 2803, 2810, 2817, 2824, 2841, 2850, 2876, 2911, 2913, 2951, 2981, 2999, 3002, 3004, 3012, 3026, 3048, 3085, 3094, 3112, 3131, 3139, 3155, 3169, 3173, 3184, 3188, 3204, 3232, 3240, 3244, 3269, 3286, 3293, 3308, 3309, 3311, 3312, 3370, 3377, 3396, 3407, 3445, 3456, 3461, 3490, 3499, 3511, 3527, 3554, 3556, 3572, 3574, 3605, 3634, 3637, 3649, 3657, 3664, 3671, 3676, 3682, 3693, 3714, 3715, 3751, 3758, 3773, 3800, 3829, 3840, 3854, 3860, 3888, 3900, 3902, 3930, 3939, 3950, 3965, 3981, 3995, 3996, 4001, 4022, 4025, 4028, 4033, 4055], 'layer_19': [0, 19, 23, 25, 45, 52, 69, 81, 115, 117, 121, 140, 158, 163, 183, 204, 209, 239, 250, 251, 252, 264, 339, 342, 345, 387, 405, 406, 409, 415, 417, 428, 446, 448, 453, 462, 463, 465, 492, 494, 539, 555, 585, 590, 591, 593, 599, 606, 607, 611, 615, 622, 638, 649, 665, 684, 692, 720, 721, 788, 793, 805, 818, 825, 828, 908, 914, 942, 948, 949, 970, 978, 993, 1010, 1017, 1061, 1072, 1083, 1108, 1122, 1135, 1158, 1178, 1194, 1195, 1201, 1208, 1226, 1228, 1233, 1247, 1249, 1266, 1283, 1298, 1318, 1326, 1339, 1354, 1372, 1378, 1380, 1382, 1383, 1388, 1397, 1404, 1416, 1421, 1423, 1428, 1484, 1535, 1542, 1551, 1556, 1559, 1570, 1574, 1576, 1583, 1591, 1602, 1611, 1612, 1640, 1643, 1648, 1652, 1663, 1676, 1677, 1684, 1727, 1745, 1752, 1760, 1761, 1778, 1787, 1788, 1798, 1800, 1802, 1806, 1822, 1829, 1845, 1850, 1895, 1897, 1899, 1912, 1940, 1941, 1956, 1966, 1976, 1985, 2012, 2023, 2068, 2073, 2080, 2086, 2089, 2127, 2145, 2154, 2156, 2179, 2181, 2219, 2233, 2246, 2247, 2261, 2274, 2280, 2283, 2309, 2318, 2338, 2346, 2351, 2357, 2377, 2408, 2419, 2423, 2431, 2463, 2472, 2483, 2491, 2554, 2565, 2578, 2585, 2598, 2609, 2611, 2613, 2615, 2623, 2628, 2636, 2657, 2660, 2690, 2692, 2695, 2698, 2700, 2736, 2750, 2756, 2765, 2784, 2791, 2799, 2817, 2823, 2834, 2854, 2867, 2872, 2876, 2877, 2880, 2882, 2883, 2901, 2909, 2914, 2922, 2928, 2931, 2946, 2947, 2991, 3023, 3028, 3038, 3039, 3050, 3054, 3060, 3067, 3072, 3079, 3094, 3099, 3142, 3144, 3185, 3198, 3223, 3237, 3240, 3253, 3254, 3258, 3285, 3287, 3306, 3307, 3311, 3314, 3326, 3336, 3356, 3373, 3397, 3413, 3438, 3442, 3443, 3481, 3482, 3498, 3500, 3506, 3509, 3526, 3554, 3558, 3591, 3592, 3595, 3597, 3605, 3635, 3664, 3702, 3747, 3751, 3769, 3785, 3807, 3849, 3871, 3883, 3919, 3925, 3928, 3930, 3973, 3974, 3997, 4005, 4054, 4061, 4062, 4076, 4094], 'layer_20': [3, 44, 67, 68, 139, 169, 213, 219, 232, 269, 272, 298, 303, 335, 375, 379, 408, 412, 432, 444, 488, 515, 521, 522, 529, 530, 556, 566, 568, 586, 599, 608, 610, 612, 656, 670, 736, 775, 780, 791, 792, 805, 830, 865, 877, 887, 954, 955, 962, 964, 1004, 1017, 1018, 1027, 1070, 1076, 1092, 1182, 1190, 1205, 1220, 1236, 1238, 1244, 1249, 1252, 1299, 1329, 1351, 1373, 1376, 1383, 1385, 1386, 1424, 1466, 1485, 1490, 1498, 1512, 1519, 1539, 1540, 1544, 1553, 1558, 1563, 1616, 1626, 1669, 1678, 1684, 1688, 1707, 1767, 1804, 1819, 1821, 1841, 1871, 1884, 1885, 1915, 1961, 1973, 1983, 2015, 2017, 2058, 2059, 2083, 2103, 2105, 2123, 2159, 2160, 2161, 2163, 2170, 2172, 2175, 2184, 2195, 2210, 2226, 2246, 2254, 2267, 2280, 2283, 2331, 2339, 2365, 2396, 2466, 2469, 2490, 2508, 2540, 2570, 2591, 2605, 2658, 2677, 2684, 2705, 2708, 2716, 2724, 2747, 2754, 2780, 2797, 2847, 2861, 2866, 2879, 2911, 2917, 2925, 2926, 2938, 2955, 2978, 2986, 2988, 2994, 2995, 3015, 3019, 3032, 3039, 3058, 3067, 3070, 3080, 3099, 3101, 3107, 3118, 3148, 3161, 3176, 3195, 3213, 3267, 3275, 3281, 3297, 3300, 3321, 3349, 3380, 3407, 3415, 3439, 3444, 3448, 3449, 3469, 3473, 3524, 3539, 3548, 3558, 3559, 3581, 3678, 3691, 3694, 3702, 3703, 3737, 3799, 3859, 3885, 3891, 3912, 3913, 3936, 3941, 3994, 3995, 3998, 4026, 4028, 4049, 4082], 'layer_21': [15, 53, 74, 76, 79, 100, 104, 105, 106, 150, 152, 175, 188, 190, 192, 194, 214, 232, 235, 245, 247, 264, 275, 277, 296, 299, 313, 359, 363, 374, 375, 377, 383, 397, 404, 410, 426, 428, 433, 442, 445, 446, 460, 468, 481, 483, 487, 501, 504, 520, 548, 569, 580, 583, 586, 611, 619, 622, 632, 653, 662, 685, 687, 694, 696, 707, 721, 722, 727, 735, 749, 756, 759, 762, 773, 790, 792, 800, 810, 822, 824, 835, 869, 874, 886, 908, 909, 922, 926, 970, 975, 987, 997, 1011, 1024, 1032, 1040, 1044, 1049, 1052, 1070, 1111, 1124, 1130, 1146, 1149, 1183, 1187, 1188, 1203, 1210, 1227, 1304, 1307, 1324, 1327, 1354, 1358, 1382, 1424, 1428, 1434, 1441, 1446, 1460, 1461, 1468, 1475, 1486, 1488, 1495, 1498, 1513, 1524, 1539, 1560, 1561, 1569, 1610, 1613, 1617, 1621, 1622, 1658, 1659, 1712, 1714, 1718, 1722, 1730, 1794, 1808, 1831, 1845, 1854, 1856, 1861, 1867, 1911, 1919, 1931, 1941, 1978, 1990, 1995, 2006, 2021, 2036, 2041, 2055, 2057, 2075, 2084, 2113, 2129, 2131, 2134, 2143, 2147, 2157, 2170, 2179, 2192, 2198, 2208, 2220, 2221, 2233, 2271, 2273, 2280, 2284, 2291, 2301, 2306, 2330, 2333, 2338, 2377, 2394, 2400, 2414, 2433, 2446, 2453, 2475, 2477, 2481, 2498, 2502, 2521, 2541, 2546, 2577, 2580, 2581, 2584, 2587, 2593, 2604, 2609, 2620, 2653, 2656, 2669, 2682, 2685, 2697, 2704, 2737, 2763, 2773, 2775, 2790, 2794, 2797, 2839, 2846, 2884, 2893, 2896, 2899, 2916, 2922, 2954, 3008, 3026, 3043, 3079, 3126, 3131, 3134, 3141, 3150, 3183, 3194, 3197, 3208, 3243, 3253, 3285, 3297, 3308, 3313, 3314, 3315, 3368, 3373, 3399, 3416, 3420, 3424, 3433, 3438, 3446, 3467, 3480, 3497, 3558, 3562, 3566, 3586, 3610, 3628, 3639, 3665, 3688, 3690, 3723, 3771, 3782, 3787, 3790, 3791, 3796, 3800, 3808, 3819, 3820, 3825, 3852, 3862, 3863, 3865, 3868, 3875, 3881, 3882, 3889, 3900, 3912, 3914, 3920, 3921, 3951, 3952, 3971, 3985, 4038, 4047, 4063, 4065, 4068, 4072, 4074, 4077, 4078, 4081, 4092], 'layer_22': [12, 23, 49, 92, 114, 124, 144, 199, 219, 221, 222, 223, 238, 248, 263, 272, 275, 276, 293, 318, 336, 341, 374, 378, 379, 385, 397, 441, 513, 517, 523, 538, 570, 597, 601, 602, 637, 641, 656, 661, 669, 681, 700, 705, 708, 714, 728, 733, 751, 761, 769, 771, 821, 842, 861, 881, 895, 896, 897, 917, 928, 930, 937, 975, 985, 987, 998, 1011, 1028, 1036, 1048, 1051, 1058, 1071, 1076, 1100, 1134, 1142, 1148, 1150, 1182, 1185, 1198, 1200, 1224, 1228, 1233, 1235, 1250, 1257, 1258, 1263, 1301, 1309, 1321, 1322, 1326, 1329, 1338, 1349, 1353, 1354, 1366, 1423, 1444, 1447, 1456, 1474, 1475, 1489, 1501, 1528, 1537, 1553, 1556, 1578, 1589, 1594, 1603, 1617, 1622, 1633, 1639, 1651, 1653, 1676, 1689, 1698, 1705, 1711, 1746, 1762, 1776, 1778, 1780, 1825, 1827, 1868, 1876, 1881, 1899, 1913, 1926, 1934, 1938, 1957, 1968, 1976, 1978, 1998, 1999, 2002, 2017, 2026, 2083, 2084, 2092, 2114, 2118, 2139, 2160, 2178, 2205, 2228, 2240, 2245, 2255, 2270, 2276, 2312, 2341, 2343, 2360, 2414, 2475, 2482, 2486, 2495, 2501, 2549, 2597, 2598, 2603, 2615, 2617, 2618, 2620, 2622, 2633, 2651, 2668, 2677, 2689, 2716, 2722, 2739, 2743, 2748, 2752, 2761, 2763, 2767, 2824, 2828, 2830, 2835, 2860, 2882, 2919, 2926, 2933, 2938, 2939, 2941, 2944, 2972, 2973, 2975, 2981, 2994, 3010, 3030, 3032, 3053, 3076, 3100, 3101, 3130, 3134, 3139, 3150, 3152, 3155, 3192, 3194, 3198, 3209, 3213, 3215, 3311, 3340, 3362, 3380, 3385, 3400, 3408, 3429, 3452, 3468, 3494, 3535, 3543, 3545, 3571, 3573, 3592, 3605, 3607, 3612, 3621, 3626, 3650, 3662, 3682, 3692, 3720, 3721, 3725, 3747, 3750, 3816, 3817, 3839, 3850, 3855, 3894, 3902, 3913, 3928, 3929, 3941, 3974, 3978, 3999, 4002, 4009, 4016, 4018, 4055, 4077, 4086], 'layer_23': [17, 79, 197, 260, 263, 274, 286, 289, 293, 330, 364, 396, 400, 401, 475, 495, 529, 561, 584, 609, 626, 670, 683, 706, 748, 766, 843, 878, 888, 933, 949, 974, 977, 978, 991, 992, 998, 1057, 1141, 1154, 1156, 1159, 1177, 1184, 1186, 1241, 1268, 1294, 1299, 1311, 1316, 1352, 1353, 1418, 1429, 1439, 1455, 1547, 1566, 1576, 1587, 1618, 1699, 1716, 1769, 1823, 1836, 1843, 1853, 1870, 1871, 1932, 1957, 1958, 2016, 2019, 2029, 2037, 2056, 2082, 2105, 2156, 2193, 2201, 2227, 2233, 2281, 2291, 2466, 2487, 2492, 2507, 2508, 2525, 2530, 2537, 2542, 2557, 2573, 2590, 2614, 2627, 2693, 2742, 2748, 2771, 2813, 2839, 2849, 2885, 2978, 2992, 3001, 3012, 3023, 3033, 3084, 3097, 3155, 3163, 3186, 3191, 3199, 3201, 3204, 3257, 3258, 3295, 3379, 3393, 3417, 3432, 3472, 3508, 3518, 3538, 3583, 3591, 3620, 3677, 3688, 3710, 3714, 3725, 3732, 3756, 3766, 3788, 3798, 3799, 3807, 3866, 3871, 3929, 3982, 3998, 4010, 4034, 4068, 4093, 4094]}, 'input': 'Asmara international airport is in which country?', 'correct_doc_id': 21871, 'relevant_docs': ['21871']}\n"
     ]
    }
   ],
   "source": [
    "print(result_dict[next(iter(result_dict.keys()))]['correct_doc_id'].item())\n",
    "import copy\n",
    "print(results_copy[next(iter(results_copy.keys()))])\n",
    "# results_copy\n",
    "with open(\"activated_neurons.json\", \"w\") as f:\n",
    "    json.dump(results_copy, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range(0, 10)\n",
      "{'layer_17': tensor([[   0,    0,    3],\n",
      "        [   0,    0,   22],\n",
      "        [   0,    0,   42],\n",
      "        [   0,    0,   49],\n",
      "        [   0,    0,   92],\n",
      "        [   0,    0,  103],\n",
      "        [   0,    0,  129],\n",
      "        [   0,    0,  133],\n",
      "        [   0,    0,  162],\n",
      "        [   0,    0,  215],\n",
      "        [   0,    0,  254],\n",
      "        [   0,    0,  280],\n",
      "        [   0,    0,  288],\n",
      "        [   0,    0,  321],\n",
      "        [   0,    0,  346],\n",
      "        [   0,    0,  377],\n",
      "        [   0,    0,  402],\n",
      "        [   0,    0,  459],\n",
      "        [   0,    0,  467],\n",
      "        [   0,    0,  478],\n",
      "        [   0,    0,  479],\n",
      "        [   0,    0,  481],\n",
      "        [   0,    0,  494],\n",
      "        [   0,    0,  501],\n",
      "        [   0,    0,  503],\n",
      "        [   0,    0,  509],\n",
      "        [   0,    0,  515],\n",
      "        [   0,    0,  516],\n",
      "        [   0,    0,  522],\n",
      "        [   0,    0,  527],\n",
      "        [   0,    0,  530],\n",
      "        [   0,    0,  536],\n",
      "        [   0,    0,  563],\n",
      "        [   0,    0,  570],\n",
      "        [   0,    0,  577],\n",
      "        [   0,    0,  589],\n",
      "        [   0,    0,  605],\n",
      "        [   0,    0,  622],\n",
      "        [   0,    0,  643],\n",
      "        [   0,    0,  647],\n",
      "        [   0,    0,  675],\n",
      "        [   0,    0,  708],\n",
      "        [   0,    0,  719],\n",
      "        [   0,    0,  811],\n",
      "        [   0,    0,  849],\n",
      "        [   0,    0,  864],\n",
      "        [   0,    0,  893],\n",
      "        [   0,    0,  900],\n",
      "        [   0,    0,  917],\n",
      "        [   0,    0,  957],\n",
      "        [   0,    0,  969],\n",
      "        [   0,    0, 1002],\n",
      "        [   0,    0, 1012],\n",
      "        [   0,    0, 1013],\n",
      "        [   0,    0, 1049],\n",
      "        [   0,    0, 1064],\n",
      "        [   0,    0, 1071],\n",
      "        [   0,    0, 1096],\n",
      "        [   0,    0, 1148],\n",
      "        [   0,    0, 1167],\n",
      "        [   0,    0, 1189],\n",
      "        [   0,    0, 1193],\n",
      "        [   0,    0, 1203],\n",
      "        [   0,    0, 1220],\n",
      "        [   0,    0, 1246],\n",
      "        [   0,    0, 1275],\n",
      "        [   0,    0, 1278],\n",
      "        [   0,    0, 1309],\n",
      "        [   0,    0, 1310],\n",
      "        [   0,    0, 1322],\n",
      "        [   0,    0, 1355],\n",
      "        [   0,    0, 1423],\n",
      "        [   0,    0, 1463],\n",
      "        [   0,    0, 1469],\n",
      "        [   0,    0, 1514],\n",
      "        [   0,    0, 1538],\n",
      "        [   0,    0, 1562],\n",
      "        [   0,    0, 1592],\n",
      "        [   0,    0, 1604],\n",
      "        [   0,    0, 1663],\n",
      "        [   0,    0, 1674],\n",
      "        [   0,    0, 1705],\n",
      "        [   0,    0, 1746],\n",
      "        [   0,    0, 1749],\n",
      "        [   0,    0, 1755],\n",
      "        [   0,    0, 1918],\n",
      "        [   0,    0, 1934],\n",
      "        [   0,    0, 1937],\n",
      "        [   0,    0, 1946],\n",
      "        [   0,    0, 1952],\n",
      "        [   0,    0, 1956],\n",
      "        [   0,    0, 2013],\n",
      "        [   0,    0, 2042],\n",
      "        [   0,    0, 2077],\n",
      "        [   0,    0, 2090],\n",
      "        [   0,    0, 2136],\n",
      "        [   0,    0, 2185],\n",
      "        [   0,    0, 2239],\n",
      "        [   0,    0, 2240],\n",
      "        [   0,    0, 2268],\n",
      "        [   0,    0, 2297],\n",
      "        [   0,    0, 2332],\n",
      "        [   0,    0, 2336],\n",
      "        [   0,    0, 2344],\n",
      "        [   0,    0, 2352],\n",
      "        [   0,    0, 2354],\n",
      "        [   0,    0, 2369],\n",
      "        [   0,    0, 2487],\n",
      "        [   0,    0, 2499],\n",
      "        [   0,    0, 2520],\n",
      "        [   0,    0, 2525],\n",
      "        [   0,    0, 2526],\n",
      "        [   0,    0, 2528],\n",
      "        [   0,    0, 2570],\n",
      "        [   0,    0, 2572],\n",
      "        [   0,    0, 2621],\n",
      "        [   0,    0, 2632],\n",
      "        [   0,    0, 2678],\n",
      "        [   0,    0, 2714],\n",
      "        [   0,    0, 2727],\n",
      "        [   0,    0, 2742],\n",
      "        [   0,    0, 2802],\n",
      "        [   0,    0, 2815],\n",
      "        [   0,    0, 2835],\n",
      "        [   0,    0, 2855],\n",
      "        [   0,    0, 2882],\n",
      "        [   0,    0, 2939],\n",
      "        [   0,    0, 2947],\n",
      "        [   0,    0, 2959],\n",
      "        [   0,    0, 2990],\n",
      "        [   0,    0, 3001],\n",
      "        [   0,    0, 3079],\n",
      "        [   0,    0, 3099],\n",
      "        [   0,    0, 3288],\n",
      "        [   0,    0, 3293],\n",
      "        [   0,    0, 3303],\n",
      "        [   0,    0, 3321],\n",
      "        [   0,    0, 3326],\n",
      "        [   0,    0, 3330],\n",
      "        [   0,    0, 3375],\n",
      "        [   0,    0, 3379],\n",
      "        [   0,    0, 3383],\n",
      "        [   0,    0, 3405],\n",
      "        [   0,    0, 3420],\n",
      "        [   0,    0, 3442],\n",
      "        [   0,    0, 3444],\n",
      "        [   0,    0, 3477],\n",
      "        [   0,    0, 3482],\n",
      "        [   0,    0, 3487],\n",
      "        [   0,    0, 3489],\n",
      "        [   0,    0, 3510],\n",
      "        [   0,    0, 3554],\n",
      "        [   0,    0, 3557],\n",
      "        [   0,    0, 3569],\n",
      "        [   0,    0, 3585],\n",
      "        [   0,    0, 3593],\n",
      "        [   0,    0, 3615],\n",
      "        [   0,    0, 3628],\n",
      "        [   0,    0, 3653],\n",
      "        [   0,    0, 3665],\n",
      "        [   0,    0, 3684],\n",
      "        [   0,    0, 3706],\n",
      "        [   0,    0, 3721],\n",
      "        [   0,    0, 3726],\n",
      "        [   0,    0, 3775],\n",
      "        [   0,    0, 3809],\n",
      "        [   0,    0, 3872],\n",
      "        [   0,    0, 3904],\n",
      "        [   0,    0, 3971],\n",
      "        [   0,    0, 3998],\n",
      "        [   0,    0, 4014],\n",
      "        [   0,    0, 4069],\n",
      "        [   0,    0, 4094]], device='cuda:0'), 'layer_18': tensor([[   0,    0,   31],\n",
      "        [   0,    0,   37],\n",
      "        [   0,    0,   46],\n",
      "        [   0,    0,   87],\n",
      "        [   0,    0,   90],\n",
      "        [   0,    0,  116],\n",
      "        [   0,    0,  118],\n",
      "        [   0,    0,  130],\n",
      "        [   0,    0,  132],\n",
      "        [   0,    0,  187],\n",
      "        [   0,    0,  194],\n",
      "        [   0,    0,  196],\n",
      "        [   0,    0,  218],\n",
      "        [   0,    0,  221],\n",
      "        [   0,    0,  231],\n",
      "        [   0,    0,  248],\n",
      "        [   0,    0,  267],\n",
      "        [   0,    0,  270],\n",
      "        [   0,    0,  272],\n",
      "        [   0,    0,  298],\n",
      "        [   0,    0,  335],\n",
      "        [   0,    0,  341],\n",
      "        [   0,    0,  348],\n",
      "        [   0,    0,  362],\n",
      "        [   0,    0,  364],\n",
      "        [   0,    0,  368],\n",
      "        [   0,    0,  385],\n",
      "        [   0,    0,  393],\n",
      "        [   0,    0,  456],\n",
      "        [   0,    0,  475],\n",
      "        [   0,    0,  518],\n",
      "        [   0,    0,  572],\n",
      "        [   0,    0,  593],\n",
      "        [   0,    0,  603],\n",
      "        [   0,    0,  607],\n",
      "        [   0,    0,  670],\n",
      "        [   0,    0,  672],\n",
      "        [   0,    0,  697],\n",
      "        [   0,    0,  703],\n",
      "        [   0,    0,  735],\n",
      "        [   0,    0,  743],\n",
      "        [   0,    0,  744],\n",
      "        [   0,    0,  759],\n",
      "        [   0,    0,  765],\n",
      "        [   0,    0,  767],\n",
      "        [   0,    0,  774],\n",
      "        [   0,    0,  807],\n",
      "        [   0,    0,  820],\n",
      "        [   0,    0,  827],\n",
      "        [   0,    0,  832],\n",
      "        [   0,    0,  851],\n",
      "        [   0,    0,  860],\n",
      "        [   0,    0,  868],\n",
      "        [   0,    0,  871],\n",
      "        [   0,    0,  877],\n",
      "        [   0,    0,  880],\n",
      "        [   0,    0,  915],\n",
      "        [   0,    0,  932],\n",
      "        [   0,    0,  936],\n",
      "        [   0,    0, 1029],\n",
      "        [   0,    0, 1053],\n",
      "        [   0,    0, 1086],\n",
      "        [   0,    0, 1101],\n",
      "        [   0,    0, 1108],\n",
      "        [   0,    0, 1121],\n",
      "        [   0,    0, 1135],\n",
      "        [   0,    0, 1141],\n",
      "        [   0,    0, 1179],\n",
      "        [   0,    0, 1203],\n",
      "        [   0,    0, 1214],\n",
      "        [   0,    0, 1220],\n",
      "        [   0,    0, 1248],\n",
      "        [   0,    0, 1261],\n",
      "        [   0,    0, 1270],\n",
      "        [   0,    0, 1284],\n",
      "        [   0,    0, 1287],\n",
      "        [   0,    0, 1312],\n",
      "        [   0,    0, 1317],\n",
      "        [   0,    0, 1336],\n",
      "        [   0,    0, 1338],\n",
      "        [   0,    0, 1340],\n",
      "        [   0,    0, 1373],\n",
      "        [   0,    0, 1385],\n",
      "        [   0,    0, 1420],\n",
      "        [   0,    0, 1441],\n",
      "        [   0,    0, 1452],\n",
      "        [   0,    0, 1473],\n",
      "        [   0,    0, 1487],\n",
      "        [   0,    0, 1488],\n",
      "        [   0,    0, 1494],\n",
      "        [   0,    0, 1497],\n",
      "        [   0,    0, 1521],\n",
      "        [   0,    0, 1560],\n",
      "        [   0,    0, 1576],\n",
      "        [   0,    0, 1582],\n",
      "        [   0,    0, 1626],\n",
      "        [   0,    0, 1630],\n",
      "        [   0,    0, 1644],\n",
      "        [   0,    0, 1672],\n",
      "        [   0,    0, 1693],\n",
      "        [   0,    0, 1716],\n",
      "        [   0,    0, 1756],\n",
      "        [   0,    0, 1759],\n",
      "        [   0,    0, 1783],\n",
      "        [   0,    0, 1792],\n",
      "        [   0,    0, 1800],\n",
      "        [   0,    0, 1835],\n",
      "        [   0,    0, 1842],\n",
      "        [   0,    0, 1858],\n",
      "        [   0,    0, 1880],\n",
      "        [   0,    0, 1905],\n",
      "        [   0,    0, 1906],\n",
      "        [   0,    0, 1927],\n",
      "        [   0,    0, 1945],\n",
      "        [   0,    0, 1997],\n",
      "        [   0,    0, 2030],\n",
      "        [   0,    0, 2069],\n",
      "        [   0,    0, 2079],\n",
      "        [   0,    0, 2080],\n",
      "        [   0,    0, 2084],\n",
      "        [   0,    0, 2090],\n",
      "        [   0,    0, 2121],\n",
      "        [   0,    0, 2128],\n",
      "        [   0,    0, 2130],\n",
      "        [   0,    0, 2142],\n",
      "        [   0,    0, 2155],\n",
      "        [   0,    0, 2159],\n",
      "        [   0,    0, 2171],\n",
      "        [   0,    0, 2176],\n",
      "        [   0,    0, 2199],\n",
      "        [   0,    0, 2206],\n",
      "        [   0,    0, 2242],\n",
      "        [   0,    0, 2245],\n",
      "        [   0,    0, 2269],\n",
      "        [   0,    0, 2319],\n",
      "        [   0,    0, 2332],\n",
      "        [   0,    0, 2356],\n",
      "        [   0,    0, 2365],\n",
      "        [   0,    0, 2449],\n",
      "        [   0,    0, 2460],\n",
      "        [   0,    0, 2470],\n",
      "        [   0,    0, 2521],\n",
      "        [   0,    0, 2529],\n",
      "        [   0,    0, 2569],\n",
      "        [   0,    0, 2573],\n",
      "        [   0,    0, 2594],\n",
      "        [   0,    0, 2630],\n",
      "        [   0,    0, 2633],\n",
      "        [   0,    0, 2650],\n",
      "        [   0,    0, 2658],\n",
      "        [   0,    0, 2668],\n",
      "        [   0,    0, 2681],\n",
      "        [   0,    0, 2716],\n",
      "        [   0,    0, 2719],\n",
      "        [   0,    0, 2739],\n",
      "        [   0,    0, 2748],\n",
      "        [   0,    0, 2761],\n",
      "        [   0,    0, 2781],\n",
      "        [   0,    0, 2785],\n",
      "        [   0,    0, 2791],\n",
      "        [   0,    0, 2802],\n",
      "        [   0,    0, 2824],\n",
      "        [   0,    0, 2836],\n",
      "        [   0,    0, 2859],\n",
      "        [   0,    0, 2862],\n",
      "        [   0,    0, 2876],\n",
      "        [   0,    0, 2885],\n",
      "        [   0,    0, 2888],\n",
      "        [   0,    0, 2893],\n",
      "        [   0,    0, 2900],\n",
      "        [   0,    0, 2927],\n",
      "        [   0,    0, 2974],\n",
      "        [   0,    0, 2978],\n",
      "        [   0,    0, 2979],\n",
      "        [   0,    0, 2993],\n",
      "        [   0,    0, 3028],\n",
      "        [   0,    0, 3043],\n",
      "        [   0,    0, 3044],\n",
      "        [   0,    0, 3058],\n",
      "        [   0,    0, 3066],\n",
      "        [   0,    0, 3068],\n",
      "        [   0,    0, 3070],\n",
      "        [   0,    0, 3085],\n",
      "        [   0,    0, 3086],\n",
      "        [   0,    0, 3132],\n",
      "        [   0,    0, 3150],\n",
      "        [   0,    0, 3195],\n",
      "        [   0,    0, 3209],\n",
      "        [   0,    0, 3299],\n",
      "        [   0,    0, 3301],\n",
      "        [   0,    0, 3303],\n",
      "        [   0,    0, 3332],\n",
      "        [   0,    0, 3373],\n",
      "        [   0,    0, 3377],\n",
      "        [   0,    0, 3381],\n",
      "        [   0,    0, 3407],\n",
      "        [   0,    0, 3413],\n",
      "        [   0,    0, 3420],\n",
      "        [   0,    0, 3453],\n",
      "        [   0,    0, 3459],\n",
      "        [   0,    0, 3490],\n",
      "        [   0,    0, 3500],\n",
      "        [   0,    0, 3523],\n",
      "        [   0,    0, 3542],\n",
      "        [   0,    0, 3551],\n",
      "        [   0,    0, 3554],\n",
      "        [   0,    0, 3556],\n",
      "        [   0,    0, 3561],\n",
      "        [   0,    0, 3590],\n",
      "        [   0,    0, 3593],\n",
      "        [   0,    0, 3595],\n",
      "        [   0,    0, 3602],\n",
      "        [   0,    0, 3605],\n",
      "        [   0,    0, 3615],\n",
      "        [   0,    0, 3631],\n",
      "        [   0,    0, 3644],\n",
      "        [   0,    0, 3646],\n",
      "        [   0,    0, 3667],\n",
      "        [   0,    0, 3669],\n",
      "        [   0,    0, 3682],\n",
      "        [   0,    0, 3685],\n",
      "        [   0,    0, 3705],\n",
      "        [   0,    0, 3715],\n",
      "        [   0,    0, 3759],\n",
      "        [   0,    0, 3775],\n",
      "        [   0,    0, 3791],\n",
      "        [   0,    0, 3795],\n",
      "        [   0,    0, 3822],\n",
      "        [   0,    0, 3826],\n",
      "        [   0,    0, 3843],\n",
      "        [   0,    0, 3852],\n",
      "        [   0,    0, 3884],\n",
      "        [   0,    0, 3904],\n",
      "        [   0,    0, 3918],\n",
      "        [   0,    0, 3924],\n",
      "        [   0,    0, 3969],\n",
      "        [   0,    0, 3984],\n",
      "        [   0,    0, 3999],\n",
      "        [   0,    0, 4003],\n",
      "        [   0,    0, 4011],\n",
      "        [   0,    0, 4014],\n",
      "        [   0,    0, 4042],\n",
      "        [   0,    0, 4085]], device='cuda:0'), 'layer_19': tensor([[   0,    0,    0],\n",
      "        [   0,    0,    4],\n",
      "        [   0,    0,   12],\n",
      "        [   0,    0,   36],\n",
      "        [   0,    0,   37],\n",
      "        [   0,    0,   52],\n",
      "        [   0,    0,   53],\n",
      "        [   0,    0,   66],\n",
      "        [   0,    0,   95],\n",
      "        [   0,    0,  104],\n",
      "        [   0,    0,  105],\n",
      "        [   0,    0,  113],\n",
      "        [   0,    0,  135],\n",
      "        [   0,    0,  147],\n",
      "        [   0,    0,  162],\n",
      "        [   0,    0,  177],\n",
      "        [   0,    0,  179],\n",
      "        [   0,    0,  193],\n",
      "        [   0,    0,  211],\n",
      "        [   0,    0,  221],\n",
      "        [   0,    0,  228],\n",
      "        [   0,    0,  233],\n",
      "        [   0,    0,  236],\n",
      "        [   0,    0,  251],\n",
      "        [   0,    0,  257],\n",
      "        [   0,    0,  285],\n",
      "        [   0,    0,  293],\n",
      "        [   0,    0,  305],\n",
      "        [   0,    0,  313],\n",
      "        [   0,    0,  345],\n",
      "        [   0,    0,  350],\n",
      "        [   0,    0,  351],\n",
      "        [   0,    0,  363],\n",
      "        [   0,    0,  365],\n",
      "        [   0,    0,  368],\n",
      "        [   0,    0,  373],\n",
      "        [   0,    0,  387],\n",
      "        [   0,    0,  420],\n",
      "        [   0,    0,  428],\n",
      "        [   0,    0,  466],\n",
      "        [   0,    0,  484],\n",
      "        [   0,    0,  496],\n",
      "        [   0,    0,  583],\n",
      "        [   0,    0,  586],\n",
      "        [   0,    0,  588],\n",
      "        [   0,    0,  595],\n",
      "        [   0,    0,  596],\n",
      "        [   0,    0,  601],\n",
      "        [   0,    0,  628],\n",
      "        [   0,    0,  635],\n",
      "        [   0,    0,  638],\n",
      "        [   0,    0,  648],\n",
      "        [   0,    0,  660],\n",
      "        [   0,    0,  692],\n",
      "        [   0,    0,  695],\n",
      "        [   0,    0,  696],\n",
      "        [   0,    0,  700],\n",
      "        [   0,    0,  702],\n",
      "        [   0,    0,  726],\n",
      "        [   0,    0,  732],\n",
      "        [   0,    0,  740],\n",
      "        [   0,    0,  768],\n",
      "        [   0,    0,  788],\n",
      "        [   0,    0,  794],\n",
      "        [   0,    0,  825],\n",
      "        [   0,    0,  831],\n",
      "        [   0,    0,  846],\n",
      "        [   0,    0,  855],\n",
      "        [   0,    0,  879],\n",
      "        [   0,    0,  906],\n",
      "        [   0,    0,  918],\n",
      "        [   0,    0,  921],\n",
      "        [   0,    0,  932],\n",
      "        [   0,    0,  934],\n",
      "        [   0,    0,  940],\n",
      "        [   0,    0,  947],\n",
      "        [   0,    0,  963],\n",
      "        [   0,    0,  970],\n",
      "        [   0,    0,  983],\n",
      "        [   0,    0, 1007],\n",
      "        [   0,    0, 1082],\n",
      "        [   0,    0, 1106],\n",
      "        [   0,    0, 1116],\n",
      "        [   0,    0, 1131],\n",
      "        [   0,    0, 1143],\n",
      "        [   0,    0, 1146],\n",
      "        [   0,    0, 1154],\n",
      "        [   0,    0, 1163],\n",
      "        [   0,    0, 1185],\n",
      "        [   0,    0, 1191],\n",
      "        [   0,    0, 1192],\n",
      "        [   0,    0, 1216],\n",
      "        [   0,    0, 1224],\n",
      "        [   0,    0, 1225],\n",
      "        [   0,    0, 1265],\n",
      "        [   0,    0, 1299],\n",
      "        [   0,    0, 1310],\n",
      "        [   0,    0, 1326],\n",
      "        [   0,    0, 1346],\n",
      "        [   0,    0, 1354],\n",
      "        [   0,    0, 1399],\n",
      "        [   0,    0, 1402],\n",
      "        [   0,    0, 1406],\n",
      "        [   0,    0, 1421],\n",
      "        [   0,    0, 1440],\n",
      "        [   0,    0, 1485],\n",
      "        [   0,    0, 1507],\n",
      "        [   0,    0, 1513],\n",
      "        [   0,    0, 1519],\n",
      "        [   0,    0, 1521],\n",
      "        [   0,    0, 1525],\n",
      "        [   0,    0, 1532],\n",
      "        [   0,    0, 1615],\n",
      "        [   0,    0, 1616],\n",
      "        [   0,    0, 1635],\n",
      "        [   0,    0, 1664],\n",
      "        [   0,    0, 1695],\n",
      "        [   0,    0, 1717],\n",
      "        [   0,    0, 1747],\n",
      "        [   0,    0, 1751],\n",
      "        [   0,    0, 1754],\n",
      "        [   0,    0, 1778],\n",
      "        [   0,    0, 1786],\n",
      "        [   0,    0, 1790],\n",
      "        [   0,    0, 1801],\n",
      "        [   0,    0, 1838],\n",
      "        [   0,    0, 1861],\n",
      "        [   0,    0, 1870],\n",
      "        [   0,    0, 1910],\n",
      "        [   0,    0, 1924],\n",
      "        [   0,    0, 1941],\n",
      "        [   0,    0, 1945],\n",
      "        [   0,    0, 1961],\n",
      "        [   0,    0, 1996],\n",
      "        [   0,    0, 2003],\n",
      "        [   0,    0, 2023],\n",
      "        [   0,    0, 2054],\n",
      "        [   0,    0, 2107],\n",
      "        [   0,    0, 2133],\n",
      "        [   0,    0, 2136],\n",
      "        [   0,    0, 2147],\n",
      "        [   0,    0, 2155],\n",
      "        [   0,    0, 2164],\n",
      "        [   0,    0, 2187],\n",
      "        [   0,    0, 2191],\n",
      "        [   0,    0, 2202],\n",
      "        [   0,    0, 2222],\n",
      "        [   0,    0, 2238],\n",
      "        [   0,    0, 2276],\n",
      "        [   0,    0, 2297],\n",
      "        [   0,    0, 2300],\n",
      "        [   0,    0, 2307],\n",
      "        [   0,    0, 2369],\n",
      "        [   0,    0, 2377],\n",
      "        [   0,    0, 2389],\n",
      "        [   0,    0, 2411],\n",
      "        [   0,    0, 2414],\n",
      "        [   0,    0, 2416],\n",
      "        [   0,    0, 2469],\n",
      "        [   0,    0, 2490],\n",
      "        [   0,    0, 2497],\n",
      "        [   0,    0, 2511],\n",
      "        [   0,    0, 2529],\n",
      "        [   0,    0, 2530],\n",
      "        [   0,    0, 2542],\n",
      "        [   0,    0, 2566],\n",
      "        [   0,    0, 2568],\n",
      "        [   0,    0, 2616],\n",
      "        [   0,    0, 2631],\n",
      "        [   0,    0, 2645],\n",
      "        [   0,    0, 2663],\n",
      "        [   0,    0, 2679],\n",
      "        [   0,    0, 2688],\n",
      "        [   0,    0, 2705],\n",
      "        [   0,    0, 2712],\n",
      "        [   0,    0, 2717],\n",
      "        [   0,    0, 2735],\n",
      "        [   0,    0, 2741],\n",
      "        [   0,    0, 2750],\n",
      "        [   0,    0, 2769],\n",
      "        [   0,    0, 2786],\n",
      "        [   0,    0, 2794],\n",
      "        [   0,    0, 2800],\n",
      "        [   0,    0, 2847],\n",
      "        [   0,    0, 2855],\n",
      "        [   0,    0, 2863],\n",
      "        [   0,    0, 2888],\n",
      "        [   0,    0, 2900],\n",
      "        [   0,    0, 2911],\n",
      "        [   0,    0, 2953],\n",
      "        [   0,    0, 2995],\n",
      "        [   0,    0, 3026],\n",
      "        [   0,    0, 3030],\n",
      "        [   0,    0, 3051],\n",
      "        [   0,    0, 3060],\n",
      "        [   0,    0, 3087],\n",
      "        [   0,    0, 3088],\n",
      "        [   0,    0, 3093],\n",
      "        [   0,    0, 3098],\n",
      "        [   0,    0, 3128],\n",
      "        [   0,    0, 3174],\n",
      "        [   0,    0, 3183],\n",
      "        [   0,    0, 3187],\n",
      "        [   0,    0, 3223],\n",
      "        [   0,    0, 3224],\n",
      "        [   0,    0, 3226],\n",
      "        [   0,    0, 3236],\n",
      "        [   0,    0, 3271],\n",
      "        [   0,    0, 3277],\n",
      "        [   0,    0, 3282],\n",
      "        [   0,    0, 3292],\n",
      "        [   0,    0, 3301],\n",
      "        [   0,    0, 3307],\n",
      "        [   0,    0, 3320],\n",
      "        [   0,    0, 3329],\n",
      "        [   0,    0, 3334],\n",
      "        [   0,    0, 3345],\n",
      "        [   0,    0, 3373],\n",
      "        [   0,    0, 3395],\n",
      "        [   0,    0, 3397],\n",
      "        [   0,    0, 3406],\n",
      "        [   0,    0, 3411],\n",
      "        [   0,    0, 3421],\n",
      "        [   0,    0, 3441],\n",
      "        [   0,    0, 3451],\n",
      "        [   0,    0, 3474],\n",
      "        [   0,    0, 3480],\n",
      "        [   0,    0, 3491],\n",
      "        [   0,    0, 3494],\n",
      "        [   0,    0, 3499],\n",
      "        [   0,    0, 3500],\n",
      "        [   0,    0, 3531],\n",
      "        [   0,    0, 3566],\n",
      "        [   0,    0, 3574],\n",
      "        [   0,    0, 3578],\n",
      "        [   0,    0, 3587],\n",
      "        [   0,    0, 3599],\n",
      "        [   0,    0, 3623],\n",
      "        [   0,    0, 3633],\n",
      "        [   0,    0, 3651],\n",
      "        [   0,    0, 3652],\n",
      "        [   0,    0, 3664],\n",
      "        [   0,    0, 3684],\n",
      "        [   0,    0, 3687],\n",
      "        [   0,    0, 3695],\n",
      "        [   0,    0, 3718],\n",
      "        [   0,    0, 3741],\n",
      "        [   0,    0, 3760],\n",
      "        [   0,    0, 3768],\n",
      "        [   0,    0, 3773],\n",
      "        [   0,    0, 3779],\n",
      "        [   0,    0, 3784],\n",
      "        [   0,    0, 3827],\n",
      "        [   0,    0, 3828],\n",
      "        [   0,    0, 3841],\n",
      "        [   0,    0, 3846],\n",
      "        [   0,    0, 3882],\n",
      "        [   0,    0, 3889],\n",
      "        [   0,    0, 3900],\n",
      "        [   0,    0, 3902],\n",
      "        [   0,    0, 3907],\n",
      "        [   0,    0, 3916],\n",
      "        [   0,    0, 3933],\n",
      "        [   0,    0, 3946],\n",
      "        [   0,    0, 3949],\n",
      "        [   0,    0, 3950],\n",
      "        [   0,    0, 3961],\n",
      "        [   0,    0, 3970],\n",
      "        [   0,    0, 3991],\n",
      "        [   0,    0, 4009],\n",
      "        [   0,    0, 4019],\n",
      "        [   0,    0, 4050],\n",
      "        [   0,    0, 4059],\n",
      "        [   0,    0, 4064],\n",
      "        [   0,    0, 4082],\n",
      "        [   0,    0, 4090],\n",
      "        [   0,    0, 4094]], device='cuda:0'), 'layer_20': tensor([[   0,    0,    2],\n",
      "        [   0,    0,   16],\n",
      "        [   0,    0,   18],\n",
      "        [   0,    0,   46],\n",
      "        [   0,    0,   49],\n",
      "        [   0,    0,   67],\n",
      "        [   0,    0,   94],\n",
      "        [   0,    0,  113],\n",
      "        [   0,    0,  129],\n",
      "        [   0,    0,  142],\n",
      "        [   0,    0,  200],\n",
      "        [   0,    0,  237],\n",
      "        [   0,    0,  320],\n",
      "        [   0,    0,  327],\n",
      "        [   0,    0,  352],\n",
      "        [   0,    0,  407],\n",
      "        [   0,    0,  412],\n",
      "        [   0,    0,  448],\n",
      "        [   0,    0,  464],\n",
      "        [   0,    0,  465],\n",
      "        [   0,    0,  471],\n",
      "        [   0,    0,  487],\n",
      "        [   0,    0,  489],\n",
      "        [   0,    0,  493],\n",
      "        [   0,    0,  510],\n",
      "        [   0,    0,  535],\n",
      "        [   0,    0,  554],\n",
      "        [   0,    0,  556],\n",
      "        [   0,    0,  559],\n",
      "        [   0,    0,  578],\n",
      "        [   0,    0,  589],\n",
      "        [   0,    0,  598],\n",
      "        [   0,    0,  599],\n",
      "        [   0,    0,  601],\n",
      "        [   0,    0,  612],\n",
      "        [   0,    0,  651],\n",
      "        [   0,    0,  667],\n",
      "        [   0,    0,  670],\n",
      "        [   0,    0,  676],\n",
      "        [   0,    0,  724],\n",
      "        [   0,    0,  745],\n",
      "        [   0,    0,  773],\n",
      "        [   0,    0,  832],\n",
      "        [   0,    0,  864],\n",
      "        [   0,    0,  865],\n",
      "        [   0,    0,  879],\n",
      "        [   0,    0,  884],\n",
      "        [   0,    0,  886],\n",
      "        [   0,    0,  891],\n",
      "        [   0,    0,  926],\n",
      "        [   0,    0,  971],\n",
      "        [   0,    0,  972],\n",
      "        [   0,    0,  989],\n",
      "        [   0,    0, 1063],\n",
      "        [   0,    0, 1084],\n",
      "        [   0,    0, 1090],\n",
      "        [   0,    0, 1108],\n",
      "        [   0,    0, 1128],\n",
      "        [   0,    0, 1133],\n",
      "        [   0,    0, 1217],\n",
      "        [   0,    0, 1230],\n",
      "        [   0,    0, 1232],\n",
      "        [   0,    0, 1236],\n",
      "        [   0,    0, 1244],\n",
      "        [   0,    0, 1254],\n",
      "        [   0,    0, 1260],\n",
      "        [   0,    0, 1320],\n",
      "        [   0,    0, 1384],\n",
      "        [   0,    0, 1386],\n",
      "        [   0,    0, 1422],\n",
      "        [   0,    0, 1427],\n",
      "        [   0,    0, 1429],\n",
      "        [   0,    0, 1466],\n",
      "        [   0,    0, 1485],\n",
      "        [   0,    0, 1526],\n",
      "        [   0,    0, 1531],\n",
      "        [   0,    0, 1542],\n",
      "        [   0,    0, 1553],\n",
      "        [   0,    0, 1567],\n",
      "        [   0,    0, 1594],\n",
      "        [   0,    0, 1625],\n",
      "        [   0,    0, 1629],\n",
      "        [   0,    0, 1646],\n",
      "        [   0,    0, 1649],\n",
      "        [   0,    0, 1650],\n",
      "        [   0,    0, 1672],\n",
      "        [   0,    0, 1696],\n",
      "        [   0,    0, 1707],\n",
      "        [   0,    0, 1735],\n",
      "        [   0,    0, 1742],\n",
      "        [   0,    0, 1763],\n",
      "        [   0,    0, 1785],\n",
      "        [   0,    0, 1801],\n",
      "        [   0,    0, 1848],\n",
      "        [   0,    0, 1877],\n",
      "        [   0,    0, 1900],\n",
      "        [   0,    0, 1901],\n",
      "        [   0,    0, 1906],\n",
      "        [   0,    0, 1935],\n",
      "        [   0,    0, 1949],\n",
      "        [   0,    0, 1956],\n",
      "        [   0,    0, 1965],\n",
      "        [   0,    0, 1977],\n",
      "        [   0,    0, 1991],\n",
      "        [   0,    0, 2000],\n",
      "        [   0,    0, 2007],\n",
      "        [   0,    0, 2058],\n",
      "        [   0,    0, 2060],\n",
      "        [   0,    0, 2074],\n",
      "        [   0,    0, 2090],\n",
      "        [   0,    0, 2106],\n",
      "        [   0,    0, 2123],\n",
      "        [   0,    0, 2136],\n",
      "        [   0,    0, 2140],\n",
      "        [   0,    0, 2144],\n",
      "        [   0,    0, 2148],\n",
      "        [   0,    0, 2156],\n",
      "        [   0,    0, 2165],\n",
      "        [   0,    0, 2168],\n",
      "        [   0,    0, 2203],\n",
      "        [   0,    0, 2206],\n",
      "        [   0,    0, 2220],\n",
      "        [   0,    0, 2221],\n",
      "        [   0,    0, 2264],\n",
      "        [   0,    0, 2267],\n",
      "        [   0,    0, 2283],\n",
      "        [   0,    0, 2303],\n",
      "        [   0,    0, 2356],\n",
      "        [   0,    0, 2365],\n",
      "        [   0,    0, 2398],\n",
      "        [   0,    0, 2402],\n",
      "        [   0,    0, 2408],\n",
      "        [   0,    0, 2409],\n",
      "        [   0,    0, 2410],\n",
      "        [   0,    0, 2435],\n",
      "        [   0,    0, 2469],\n",
      "        [   0,    0, 2480],\n",
      "        [   0,    0, 2489],\n",
      "        [   0,    0, 2519],\n",
      "        [   0,    0, 2543],\n",
      "        [   0,    0, 2553],\n",
      "        [   0,    0, 2585],\n",
      "        [   0,    0, 2588],\n",
      "        [   0,    0, 2610],\n",
      "        [   0,    0, 2635],\n",
      "        [   0,    0, 2649],\n",
      "        [   0,    0, 2668],\n",
      "        [   0,    0, 2708],\n",
      "        [   0,    0, 2728],\n",
      "        [   0,    0, 2744],\n",
      "        [   0,    0, 2747],\n",
      "        [   0,    0, 2749],\n",
      "        [   0,    0, 2760],\n",
      "        [   0,    0, 2776],\n",
      "        [   0,    0, 2795],\n",
      "        [   0,    0, 2861],\n",
      "        [   0,    0, 2867],\n",
      "        [   0,    0, 2876],\n",
      "        [   0,    0, 2879],\n",
      "        [   0,    0, 2893],\n",
      "        [   0,    0, 2905],\n",
      "        [   0,    0, 2958],\n",
      "        [   0,    0, 2983],\n",
      "        [   0,    0, 2992],\n",
      "        [   0,    0, 3015],\n",
      "        [   0,    0, 3023],\n",
      "        [   0,    0, 3046],\n",
      "        [   0,    0, 3063],\n",
      "        [   0,    0, 3173],\n",
      "        [   0,    0, 3208],\n",
      "        [   0,    0, 3240],\n",
      "        [   0,    0, 3242],\n",
      "        [   0,    0, 3257],\n",
      "        [   0,    0, 3261],\n",
      "        [   0,    0, 3307],\n",
      "        [   0,    0, 3321],\n",
      "        [   0,    0, 3329],\n",
      "        [   0,    0, 3357],\n",
      "        [   0,    0, 3358],\n",
      "        [   0,    0, 3367],\n",
      "        [   0,    0, 3407],\n",
      "        [   0,    0, 3440],\n",
      "        [   0,    0, 3444],\n",
      "        [   0,    0, 3464],\n",
      "        [   0,    0, 3498],\n",
      "        [   0,    0, 3503],\n",
      "        [   0,    0, 3514],\n",
      "        [   0,    0, 3532],\n",
      "        [   0,    0, 3540],\n",
      "        [   0,    0, 3558],\n",
      "        [   0,    0, 3573],\n",
      "        [   0,    0, 3596],\n",
      "        [   0,    0, 3603],\n",
      "        [   0,    0, 3612],\n",
      "        [   0,    0, 3620],\n",
      "        [   0,    0, 3624],\n",
      "        [   0,    0, 3655],\n",
      "        [   0,    0, 3665],\n",
      "        [   0,    0, 3713],\n",
      "        [   0,    0, 3720],\n",
      "        [   0,    0, 3735],\n",
      "        [   0,    0, 3754],\n",
      "        [   0,    0, 3762],\n",
      "        [   0,    0, 3765],\n",
      "        [   0,    0, 3766],\n",
      "        [   0,    0, 3771],\n",
      "        [   0,    0, 3779],\n",
      "        [   0,    0, 3827],\n",
      "        [   0,    0, 3855],\n",
      "        [   0,    0, 3886],\n",
      "        [   0,    0, 3899],\n",
      "        [   0,    0, 3934],\n",
      "        [   0,    0, 3949],\n",
      "        [   0,    0, 3973],\n",
      "        [   0,    0, 4000],\n",
      "        [   0,    0, 4001],\n",
      "        [   0,    0, 4020],\n",
      "        [   0,    0, 4033],\n",
      "        [   0,    0, 4036],\n",
      "        [   0,    0, 4062]], device='cuda:0'), 'layer_21': tensor([[   0,    0,   22],\n",
      "        [   0,    0,   36],\n",
      "        [   0,    0,   53],\n",
      "        [   0,    0,   69],\n",
      "        [   0,    0,   80],\n",
      "        [   0,    0,  102],\n",
      "        [   0,    0,  112],\n",
      "        [   0,    0,  116],\n",
      "        [   0,    0,  141],\n",
      "        [   0,    0,  169],\n",
      "        [   0,    0,  180],\n",
      "        [   0,    0,  220],\n",
      "        [   0,    0,  225],\n",
      "        [   0,    0,  233],\n",
      "        [   0,    0,  234],\n",
      "        [   0,    0,  235],\n",
      "        [   0,    0,  286],\n",
      "        [   0,    0,  292],\n",
      "        [   0,    0,  299],\n",
      "        [   0,    0,  308],\n",
      "        [   0,    0,  318],\n",
      "        [   0,    0,  353],\n",
      "        [   0,    0,  359],\n",
      "        [   0,    0,  364],\n",
      "        [   0,    0,  431],\n",
      "        [   0,    0,  436],\n",
      "        [   0,    0,  445],\n",
      "        [   0,    0,  460],\n",
      "        [   0,    0,  479],\n",
      "        [   0,    0,  496],\n",
      "        [   0,    0,  509],\n",
      "        [   0,    0,  525],\n",
      "        [   0,    0,  556],\n",
      "        [   0,    0,  559],\n",
      "        [   0,    0,  566],\n",
      "        [   0,    0,  568],\n",
      "        [   0,    0,  580],\n",
      "        [   0,    0,  621],\n",
      "        [   0,    0,  624],\n",
      "        [   0,    0,  641],\n",
      "        [   0,    0,  651],\n",
      "        [   0,    0,  656],\n",
      "        [   0,    0,  660],\n",
      "        [   0,    0,  682],\n",
      "        [   0,    0,  685],\n",
      "        [   0,    0,  699],\n",
      "        [   0,    0,  773],\n",
      "        [   0,    0,  784],\n",
      "        [   0,    0,  785],\n",
      "        [   0,    0,  790],\n",
      "        [   0,    0,  807],\n",
      "        [   0,    0,  813],\n",
      "        [   0,    0,  825],\n",
      "        [   0,    0,  845],\n",
      "        [   0,    0,  851],\n",
      "        [   0,    0,  865],\n",
      "        [   0,    0,  872],\n",
      "        [   0,    0,  883],\n",
      "        [   0,    0,  900],\n",
      "        [   0,    0,  907],\n",
      "        [   0,    0,  918],\n",
      "        [   0,    0,  979],\n",
      "        [   0,    0, 1026],\n",
      "        [   0,    0, 1032],\n",
      "        [   0,    0, 1045],\n",
      "        [   0,    0, 1049],\n",
      "        [   0,    0, 1054],\n",
      "        [   0,    0, 1065],\n",
      "        [   0,    0, 1115],\n",
      "        [   0,    0, 1133],\n",
      "        [   0,    0, 1138],\n",
      "        [   0,    0, 1223],\n",
      "        [   0,    0, 1237],\n",
      "        [   0,    0, 1251],\n",
      "        [   0,    0, 1252],\n",
      "        [   0,    0, 1255],\n",
      "        [   0,    0, 1277],\n",
      "        [   0,    0, 1282],\n",
      "        [   0,    0, 1339],\n",
      "        [   0,    0, 1341],\n",
      "        [   0,    0, 1353],\n",
      "        [   0,    0, 1381],\n",
      "        [   0,    0, 1387],\n",
      "        [   0,    0, 1428],\n",
      "        [   0,    0, 1460],\n",
      "        [   0,    0, 1465],\n",
      "        [   0,    0, 1476],\n",
      "        [   0,    0, 1482],\n",
      "        [   0,    0, 1510],\n",
      "        [   0,    0, 1518],\n",
      "        [   0,    0, 1530],\n",
      "        [   0,    0, 1533],\n",
      "        [   0,    0, 1544],\n",
      "        [   0,    0, 1560],\n",
      "        [   0,    0, 1561],\n",
      "        [   0,    0, 1593],\n",
      "        [   0,    0, 1614],\n",
      "        [   0,    0, 1702],\n",
      "        [   0,    0, 1711],\n",
      "        [   0,    0, 1746],\n",
      "        [   0,    0, 1749],\n",
      "        [   0,    0, 1786],\n",
      "        [   0,    0, 1792],\n",
      "        [   0,    0, 1795],\n",
      "        [   0,    0, 1832],\n",
      "        [   0,    0, 1838],\n",
      "        [   0,    0, 1858],\n",
      "        [   0,    0, 1860],\n",
      "        [   0,    0, 1873],\n",
      "        [   0,    0, 1875],\n",
      "        [   0,    0, 1880],\n",
      "        [   0,    0, 1889],\n",
      "        [   0,    0, 1925],\n",
      "        [   0,    0, 1948],\n",
      "        [   0,    0, 1975],\n",
      "        [   0,    0, 1984],\n",
      "        [   0,    0, 1996],\n",
      "        [   0,    0, 2002],\n",
      "        [   0,    0, 2009],\n",
      "        [   0,    0, 2024],\n",
      "        [   0,    0, 2040],\n",
      "        [   0,    0, 2041],\n",
      "        [   0,    0, 2045],\n",
      "        [   0,    0, 2053],\n",
      "        [   0,    0, 2055],\n",
      "        [   0,    0, 2057],\n",
      "        [   0,    0, 2081],\n",
      "        [   0,    0, 2131],\n",
      "        [   0,    0, 2142],\n",
      "        [   0,    0, 2147],\n",
      "        [   0,    0, 2159],\n",
      "        [   0,    0, 2165],\n",
      "        [   0,    0, 2178],\n",
      "        [   0,    0, 2191],\n",
      "        [   0,    0, 2220],\n",
      "        [   0,    0, 2234],\n",
      "        [   0,    0, 2248],\n",
      "        [   0,    0, 2250],\n",
      "        [   0,    0, 2253],\n",
      "        [   0,    0, 2260],\n",
      "        [   0,    0, 2267],\n",
      "        [   0,    0, 2271],\n",
      "        [   0,    0, 2295],\n",
      "        [   0,    0, 2297],\n",
      "        [   0,    0, 2336],\n",
      "        [   0,    0, 2338],\n",
      "        [   0,    0, 2354],\n",
      "        [   0,    0, 2370],\n",
      "        [   0,    0, 2388],\n",
      "        [   0,    0, 2389],\n",
      "        [   0,    0, 2405],\n",
      "        [   0,    0, 2435],\n",
      "        [   0,    0, 2446],\n",
      "        [   0,    0, 2461],\n",
      "        [   0,    0, 2463],\n",
      "        [   0,    0, 2476],\n",
      "        [   0,    0, 2486],\n",
      "        [   0,    0, 2521],\n",
      "        [   0,    0, 2532],\n",
      "        [   0,    0, 2534],\n",
      "        [   0,    0, 2536],\n",
      "        [   0,    0, 2541],\n",
      "        [   0,    0, 2549],\n",
      "        [   0,    0, 2578],\n",
      "        [   0,    0, 2597],\n",
      "        [   0,    0, 2604],\n",
      "        [   0,    0, 2607],\n",
      "        [   0,    0, 2615],\n",
      "        [   0,    0, 2634],\n",
      "        [   0,    0, 2655],\n",
      "        [   0,    0, 2668],\n",
      "        [   0,    0, 2710],\n",
      "        [   0,    0, 2733],\n",
      "        [   0,    0, 2749],\n",
      "        [   0,    0, 2763],\n",
      "        [   0,    0, 2814],\n",
      "        [   0,    0, 2847],\n",
      "        [   0,    0, 2850],\n",
      "        [   0,    0, 2854],\n",
      "        [   0,    0, 2875],\n",
      "        [   0,    0, 2884],\n",
      "        [   0,    0, 2893],\n",
      "        [   0,    0, 2916],\n",
      "        [   0,    0, 2919],\n",
      "        [   0,    0, 2956],\n",
      "        [   0,    0, 2965],\n",
      "        [   0,    0, 2966],\n",
      "        [   0,    0, 2994],\n",
      "        [   0,    0, 3007],\n",
      "        [   0,    0, 3018],\n",
      "        [   0,    0, 3041],\n",
      "        [   0,    0, 3059],\n",
      "        [   0,    0, 3069],\n",
      "        [   0,    0, 3147],\n",
      "        [   0,    0, 3170],\n",
      "        [   0,    0, 3186],\n",
      "        [   0,    0, 3189],\n",
      "        [   0,    0, 3194],\n",
      "        [   0,    0, 3219],\n",
      "        [   0,    0, 3228],\n",
      "        [   0,    0, 3264],\n",
      "        [   0,    0, 3281],\n",
      "        [   0,    0, 3307],\n",
      "        [   0,    0, 3313],\n",
      "        [   0,    0, 3332],\n",
      "        [   0,    0, 3335],\n",
      "        [   0,    0, 3341],\n",
      "        [   0,    0, 3351],\n",
      "        [   0,    0, 3354],\n",
      "        [   0,    0, 3361],\n",
      "        [   0,    0, 3367],\n",
      "        [   0,    0, 3373],\n",
      "        [   0,    0, 3397],\n",
      "        [   0,    0, 3423],\n",
      "        [   0,    0, 3425],\n",
      "        [   0,    0, 3458],\n",
      "        [   0,    0, 3463],\n",
      "        [   0,    0, 3480],\n",
      "        [   0,    0, 3487],\n",
      "        [   0,    0, 3517],\n",
      "        [   0,    0, 3570],\n",
      "        [   0,    0, 3579],\n",
      "        [   0,    0, 3587],\n",
      "        [   0,    0, 3591],\n",
      "        [   0,    0, 3592],\n",
      "        [   0,    0, 3593],\n",
      "        [   0,    0, 3595],\n",
      "        [   0,    0, 3650],\n",
      "        [   0,    0, 3685],\n",
      "        [   0,    0, 3694],\n",
      "        [   0,    0, 3697],\n",
      "        [   0,    0, 3698],\n",
      "        [   0,    0, 3712],\n",
      "        [   0,    0, 3721],\n",
      "        [   0,    0, 3724],\n",
      "        [   0,    0, 3744],\n",
      "        [   0,    0, 3779],\n",
      "        [   0,    0, 3790],\n",
      "        [   0,    0, 3800],\n",
      "        [   0,    0, 3804],\n",
      "        [   0,    0, 3814],\n",
      "        [   0,    0, 3818],\n",
      "        [   0,    0, 3821],\n",
      "        [   0,    0, 3834],\n",
      "        [   0,    0, 3842],\n",
      "        [   0,    0, 3919],\n",
      "        [   0,    0, 3925],\n",
      "        [   0,    0, 3941],\n",
      "        [   0,    0, 3946],\n",
      "        [   0,    0, 3977],\n",
      "        [   0,    0, 3994],\n",
      "        [   0,    0, 4011],\n",
      "        [   0,    0, 4030],\n",
      "        [   0,    0, 4056],\n",
      "        [   0,    0, 4060],\n",
      "        [   0,    0, 4074]], device='cuda:0'), 'layer_22': tensor([[   0,    0,   52],\n",
      "        [   0,    0,  106],\n",
      "        [   0,    0,  122],\n",
      "        [   0,    0,  133],\n",
      "        [   0,    0,  135],\n",
      "        [   0,    0,  138],\n",
      "        [   0,    0,  141],\n",
      "        [   0,    0,  144],\n",
      "        [   0,    0,  148],\n",
      "        [   0,    0,  159],\n",
      "        [   0,    0,  193],\n",
      "        [   0,    0,  200],\n",
      "        [   0,    0,  212],\n",
      "        [   0,    0,  216],\n",
      "        [   0,    0,  219],\n",
      "        [   0,    0,  222],\n",
      "        [   0,    0,  226],\n",
      "        [   0,    0,  238],\n",
      "        [   0,    0,  263],\n",
      "        [   0,    0,  277],\n",
      "        [   0,    0,  329],\n",
      "        [   0,    0,  333],\n",
      "        [   0,    0,  336],\n",
      "        [   0,    0,  353],\n",
      "        [   0,    0,  355],\n",
      "        [   0,    0,  356],\n",
      "        [   0,    0,  440],\n",
      "        [   0,    0,  457],\n",
      "        [   0,    0,  460],\n",
      "        [   0,    0,  490],\n",
      "        [   0,    0,  492],\n",
      "        [   0,    0,  500],\n",
      "        [   0,    0,  509],\n",
      "        [   0,    0,  514],\n",
      "        [   0,    0,  515],\n",
      "        [   0,    0,  520],\n",
      "        [   0,    0,  523],\n",
      "        [   0,    0,  534],\n",
      "        [   0,    0,  560],\n",
      "        [   0,    0,  561],\n",
      "        [   0,    0,  578],\n",
      "        [   0,    0,  585],\n",
      "        [   0,    0,  603],\n",
      "        [   0,    0,  610],\n",
      "        [   0,    0,  612],\n",
      "        [   0,    0,  623],\n",
      "        [   0,    0,  648],\n",
      "        [   0,    0,  657],\n",
      "        [   0,    0,  680],\n",
      "        [   0,    0,  716],\n",
      "        [   0,    0,  756],\n",
      "        [   0,    0,  775],\n",
      "        [   0,    0,  785],\n",
      "        [   0,    0,  792],\n",
      "        [   0,    0,  806],\n",
      "        [   0,    0,  808],\n",
      "        [   0,    0,  896],\n",
      "        [   0,    0,  897],\n",
      "        [   0,    0,  917],\n",
      "        [   0,    0,  930],\n",
      "        [   0,    0,  938],\n",
      "        [   0,    0,  939],\n",
      "        [   0,    0,  949],\n",
      "        [   0,    0,  988],\n",
      "        [   0,    0,  998],\n",
      "        [   0,    0, 1016],\n",
      "        [   0,    0, 1034],\n",
      "        [   0,    0, 1036],\n",
      "        [   0,    0, 1038],\n",
      "        [   0,    0, 1043],\n",
      "        [   0,    0, 1044],\n",
      "        [   0,    0, 1061],\n",
      "        [   0,    0, 1064],\n",
      "        [   0,    0, 1066],\n",
      "        [   0,    0, 1123],\n",
      "        [   0,    0, 1142],\n",
      "        [   0,    0, 1153],\n",
      "        [   0,    0, 1165],\n",
      "        [   0,    0, 1205],\n",
      "        [   0,    0, 1209],\n",
      "        [   0,    0, 1210],\n",
      "        [   0,    0, 1224],\n",
      "        [   0,    0, 1233],\n",
      "        [   0,    0, 1255],\n",
      "        [   0,    0, 1257],\n",
      "        [   0,    0, 1262],\n",
      "        [   0,    0, 1267],\n",
      "        [   0,    0, 1275],\n",
      "        [   0,    0, 1276],\n",
      "        [   0,    0, 1281],\n",
      "        [   0,    0, 1299],\n",
      "        [   0,    0, 1301],\n",
      "        [   0,    0, 1321],\n",
      "        [   0,    0, 1330],\n",
      "        [   0,    0, 1340],\n",
      "        [   0,    0, 1342],\n",
      "        [   0,    0, 1354],\n",
      "        [   0,    0, 1355],\n",
      "        [   0,    0, 1356],\n",
      "        [   0,    0, 1385],\n",
      "        [   0,    0, 1389],\n",
      "        [   0,    0, 1422],\n",
      "        [   0,    0, 1423],\n",
      "        [   0,    0, 1434],\n",
      "        [   0,    0, 1443],\n",
      "        [   0,    0, 1470],\n",
      "        [   0,    0, 1474],\n",
      "        [   0,    0, 1479],\n",
      "        [   0,    0, 1481],\n",
      "        [   0,    0, 1502],\n",
      "        [   0,    0, 1514],\n",
      "        [   0,    0, 1517],\n",
      "        [   0,    0, 1518],\n",
      "        [   0,    0, 1533],\n",
      "        [   0,    0, 1548],\n",
      "        [   0,    0, 1570],\n",
      "        [   0,    0, 1571],\n",
      "        [   0,    0, 1576],\n",
      "        [   0,    0, 1590],\n",
      "        [   0,    0, 1598],\n",
      "        [   0,    0, 1603],\n",
      "        [   0,    0, 1609],\n",
      "        [   0,    0, 1633],\n",
      "        [   0,    0, 1668],\n",
      "        [   0,    0, 1678],\n",
      "        [   0,    0, 1688],\n",
      "        [   0,    0, 1693],\n",
      "        [   0,    0, 1749],\n",
      "        [   0,    0, 1759],\n",
      "        [   0,    0, 1764],\n",
      "        [   0,    0, 1773],\n",
      "        [   0,    0, 1778],\n",
      "        [   0,    0, 1780],\n",
      "        [   0,    0, 1781],\n",
      "        [   0,    0, 1788],\n",
      "        [   0,    0, 1812],\n",
      "        [   0,    0, 1813],\n",
      "        [   0,    0, 1824],\n",
      "        [   0,    0, 1836],\n",
      "        [   0,    0, 1898],\n",
      "        [   0,    0, 1921],\n",
      "        [   0,    0, 1934],\n",
      "        [   0,    0, 1956],\n",
      "        [   0,    0, 1974],\n",
      "        [   0,    0, 1989],\n",
      "        [   0,    0, 1997],\n",
      "        [   0,    0, 2000],\n",
      "        [   0,    0, 2007],\n",
      "        [   0,    0, 2017],\n",
      "        [   0,    0, 2037],\n",
      "        [   0,    0, 2053],\n",
      "        [   0,    0, 2057],\n",
      "        [   0,    0, 2091],\n",
      "        [   0,    0, 2101],\n",
      "        [   0,    0, 2115],\n",
      "        [   0,    0, 2120],\n",
      "        [   0,    0, 2159],\n",
      "        [   0,    0, 2163],\n",
      "        [   0,    0, 2239],\n",
      "        [   0,    0, 2247],\n",
      "        [   0,    0, 2250],\n",
      "        [   0,    0, 2276],\n",
      "        [   0,    0, 2308],\n",
      "        [   0,    0, 2320],\n",
      "        [   0,    0, 2326],\n",
      "        [   0,    0, 2339],\n",
      "        [   0,    0, 2346],\n",
      "        [   0,    0, 2351],\n",
      "        [   0,    0, 2354],\n",
      "        [   0,    0, 2373],\n",
      "        [   0,    0, 2387],\n",
      "        [   0,    0, 2399],\n",
      "        [   0,    0, 2406],\n",
      "        [   0,    0, 2410],\n",
      "        [   0,    0, 2414],\n",
      "        [   0,    0, 2440],\n",
      "        [   0,    0, 2459],\n",
      "        [   0,    0, 2471],\n",
      "        [   0,    0, 2474],\n",
      "        [   0,    0, 2487],\n",
      "        [   0,    0, 2492],\n",
      "        [   0,    0, 2494],\n",
      "        [   0,    0, 2495],\n",
      "        [   0,    0, 2528],\n",
      "        [   0,    0, 2530],\n",
      "        [   0,    0, 2551],\n",
      "        [   0,    0, 2572],\n",
      "        [   0,    0, 2574],\n",
      "        [   0,    0, 2583],\n",
      "        [   0,    0, 2598],\n",
      "        [   0,    0, 2599],\n",
      "        [   0,    0, 2601],\n",
      "        [   0,    0, 2606],\n",
      "        [   0,    0, 2632],\n",
      "        [   0,    0, 2637],\n",
      "        [   0,    0, 2638],\n",
      "        [   0,    0, 2648],\n",
      "        [   0,    0, 2651],\n",
      "        [   0,    0, 2658],\n",
      "        [   0,    0, 2661],\n",
      "        [   0,    0, 2665],\n",
      "        [   0,    0, 2673],\n",
      "        [   0,    0, 2675],\n",
      "        [   0,    0, 2692],\n",
      "        [   0,    0, 2718],\n",
      "        [   0,    0, 2722],\n",
      "        [   0,    0, 2735],\n",
      "        [   0,    0, 2753],\n",
      "        [   0,    0, 2761],\n",
      "        [   0,    0, 2767],\n",
      "        [   0,    0, 2811],\n",
      "        [   0,    0, 2818],\n",
      "        [   0,    0, 2824],\n",
      "        [   0,    0, 2845],\n",
      "        [   0,    0, 2846],\n",
      "        [   0,    0, 2860],\n",
      "        [   0,    0, 2865],\n",
      "        [   0,    0, 2876],\n",
      "        [   0,    0, 2901],\n",
      "        [   0,    0, 2907],\n",
      "        [   0,    0, 2908],\n",
      "        [   0,    0, 2919],\n",
      "        [   0,    0, 2938],\n",
      "        [   0,    0, 2945],\n",
      "        [   0,    0, 2948],\n",
      "        [   0,    0, 2965],\n",
      "        [   0,    0, 2981],\n",
      "        [   0,    0, 3001],\n",
      "        [   0,    0, 3015],\n",
      "        [   0,    0, 3020],\n",
      "        [   0,    0, 3026],\n",
      "        [   0,    0, 3036],\n",
      "        [   0,    0, 3053],\n",
      "        [   0,    0, 3058],\n",
      "        [   0,    0, 3064],\n",
      "        [   0,    0, 3071],\n",
      "        [   0,    0, 3076],\n",
      "        [   0,    0, 3081],\n",
      "        [   0,    0, 3083],\n",
      "        [   0,    0, 3094],\n",
      "        [   0,    0, 3100],\n",
      "        [   0,    0, 3101],\n",
      "        [   0,    0, 3111],\n",
      "        [   0,    0, 3119],\n",
      "        [   0,    0, 3150],\n",
      "        [   0,    0, 3155],\n",
      "        [   0,    0, 3167],\n",
      "        [   0,    0, 3193],\n",
      "        [   0,    0, 3198],\n",
      "        [   0,    0, 3199],\n",
      "        [   0,    0, 3219],\n",
      "        [   0,    0, 3262],\n",
      "        [   0,    0, 3270],\n",
      "        [   0,    0, 3286],\n",
      "        [   0,    0, 3293],\n",
      "        [   0,    0, 3315],\n",
      "        [   0,    0, 3319],\n",
      "        [   0,    0, 3326],\n",
      "        [   0,    0, 3333],\n",
      "        [   0,    0, 3405],\n",
      "        [   0,    0, 3423],\n",
      "        [   0,    0, 3434],\n",
      "        [   0,    0, 3435],\n",
      "        [   0,    0, 3443],\n",
      "        [   0,    0, 3451],\n",
      "        [   0,    0, 3456],\n",
      "        [   0,    0, 3466],\n",
      "        [   0,    0, 3476],\n",
      "        [   0,    0, 3545],\n",
      "        [   0,    0, 3566],\n",
      "        [   0,    0, 3573],\n",
      "        [   0,    0, 3579],\n",
      "        [   0,    0, 3580],\n",
      "        [   0,    0, 3585],\n",
      "        [   0,    0, 3607],\n",
      "        [   0,    0, 3608],\n",
      "        [   0,    0, 3647],\n",
      "        [   0,    0, 3650],\n",
      "        [   0,    0, 3680],\n",
      "        [   0,    0, 3702],\n",
      "        [   0,    0, 3728],\n",
      "        [   0,    0, 3731],\n",
      "        [   0,    0, 3732],\n",
      "        [   0,    0, 3745],\n",
      "        [   0,    0, 3817],\n",
      "        [   0,    0, 3860],\n",
      "        [   0,    0, 3864],\n",
      "        [   0,    0, 3871],\n",
      "        [   0,    0, 3875],\n",
      "        [   0,    0, 3880],\n",
      "        [   0,    0, 3885],\n",
      "        [   0,    0, 3887],\n",
      "        [   0,    0, 3902],\n",
      "        [   0,    0, 3903],\n",
      "        [   0,    0, 3910],\n",
      "        [   0,    0, 3920],\n",
      "        [   0,    0, 3922],\n",
      "        [   0,    0, 3939],\n",
      "        [   0,    0, 3946],\n",
      "        [   0,    0, 3954],\n",
      "        [   0,    0, 3962],\n",
      "        [   0,    0, 3970],\n",
      "        [   0,    0, 3981],\n",
      "        [   0,    0, 3989],\n",
      "        [   0,    0, 3992],\n",
      "        [   0,    0, 4015],\n",
      "        [   0,    0, 4018],\n",
      "        [   0,    0, 4026],\n",
      "        [   0,    0, 4055]], device='cuda:0'), 'layer_23': tensor([[   0,    0,    8],\n",
      "        [   0,    0,   12],\n",
      "        [   0,    0,   56],\n",
      "        [   0,    0,  131],\n",
      "        [   0,    0,  212],\n",
      "        [   0,    0,  289],\n",
      "        [   0,    0,  294],\n",
      "        [   0,    0,  303],\n",
      "        [   0,    0,  312],\n",
      "        [   0,    0,  341],\n",
      "        [   0,    0,  356],\n",
      "        [   0,    0,  396],\n",
      "        [   0,    0,  401],\n",
      "        [   0,    0,  411],\n",
      "        [   0,    0,  509],\n",
      "        [   0,    0,  513],\n",
      "        [   0,    0,  517],\n",
      "        [   0,    0,  559],\n",
      "        [   0,    0,  577],\n",
      "        [   0,    0,  772],\n",
      "        [   0,    0,  839],\n",
      "        [   0,    0,  843],\n",
      "        [   0,    0,  869],\n",
      "        [   0,    0,  881],\n",
      "        [   0,    0,  887],\n",
      "        [   0,    0,  914],\n",
      "        [   0,    0,  937],\n",
      "        [   0,    0,  949],\n",
      "        [   0,    0,  955],\n",
      "        [   0,    0,  991],\n",
      "        [   0,    0, 1026],\n",
      "        [   0,    0, 1061],\n",
      "        [   0,    0, 1065],\n",
      "        [   0,    0, 1094],\n",
      "        [   0,    0, 1102],\n",
      "        [   0,    0, 1191],\n",
      "        [   0,    0, 1197],\n",
      "        [   0,    0, 1237],\n",
      "        [   0,    0, 1240],\n",
      "        [   0,    0, 1309],\n",
      "        [   0,    0, 1327],\n",
      "        [   0,    0, 1426],\n",
      "        [   0,    0, 1449],\n",
      "        [   0,    0, 1469],\n",
      "        [   0,    0, 1566],\n",
      "        [   0,    0, 1613],\n",
      "        [   0,    0, 1642],\n",
      "        [   0,    0, 1708],\n",
      "        [   0,    0, 1743],\n",
      "        [   0,    0, 1768],\n",
      "        [   0,    0, 1841],\n",
      "        [   0,    0, 1871],\n",
      "        [   0,    0, 1884],\n",
      "        [   0,    0, 1895],\n",
      "        [   0,    0, 1904],\n",
      "        [   0,    0, 1914],\n",
      "        [   0,    0, 1917],\n",
      "        [   0,    0, 1923],\n",
      "        [   0,    0, 1951],\n",
      "        [   0,    0, 1970],\n",
      "        [   0,    0, 1986],\n",
      "        [   0,    0, 1987],\n",
      "        [   0,    0, 2019],\n",
      "        [   0,    0, 2034],\n",
      "        [   0,    0, 2112],\n",
      "        [   0,    0, 2125],\n",
      "        [   0,    0, 2143],\n",
      "        [   0,    0, 2156],\n",
      "        [   0,    0, 2167],\n",
      "        [   0,    0, 2172],\n",
      "        [   0,    0, 2224],\n",
      "        [   0,    0, 2233],\n",
      "        [   0,    0, 2267],\n",
      "        [   0,    0, 2273],\n",
      "        [   0,    0, 2282],\n",
      "        [   0,    0, 2405],\n",
      "        [   0,    0, 2414],\n",
      "        [   0,    0, 2470],\n",
      "        [   0,    0, 2507],\n",
      "        [   0,    0, 2595],\n",
      "        [   0,    0, 2602],\n",
      "        [   0,    0, 2603],\n",
      "        [   0,    0, 2613],\n",
      "        [   0,    0, 2614],\n",
      "        [   0,    0, 2678],\n",
      "        [   0,    0, 2709],\n",
      "        [   0,    0, 2752],\n",
      "        [   0,    0, 2921],\n",
      "        [   0,    0, 2935],\n",
      "        [   0,    0, 2974],\n",
      "        [   0,    0, 2978],\n",
      "        [   0,    0, 3019],\n",
      "        [   0,    0, 3091],\n",
      "        [   0,    0, 3100],\n",
      "        [   0,    0, 3137],\n",
      "        [   0,    0, 3147],\n",
      "        [   0,    0, 3149],\n",
      "        [   0,    0, 3155],\n",
      "        [   0,    0, 3165],\n",
      "        [   0,    0, 3209],\n",
      "        [   0,    0, 3211],\n",
      "        [   0,    0, 3230],\n",
      "        [   0,    0, 3259],\n",
      "        [   0,    0, 3302],\n",
      "        [   0,    0, 3307],\n",
      "        [   0,    0, 3322],\n",
      "        [   0,    0, 3361],\n",
      "        [   0,    0, 3393],\n",
      "        [   0,    0, 3452],\n",
      "        [   0,    0, 3468],\n",
      "        [   0,    0, 3491],\n",
      "        [   0,    0, 3513],\n",
      "        [   0,    0, 3529],\n",
      "        [   0,    0, 3534],\n",
      "        [   0,    0, 3547],\n",
      "        [   0,    0, 3591],\n",
      "        [   0,    0, 3593],\n",
      "        [   0,    0, 3597],\n",
      "        [   0,    0, 3630],\n",
      "        [   0,    0, 3634],\n",
      "        [   0,    0, 3653],\n",
      "        [   0,    0, 3654],\n",
      "        [   0,    0, 3681],\n",
      "        [   0,    0, 3698],\n",
      "        [   0,    0, 3712],\n",
      "        [   0,    0, 3726],\n",
      "        [   0,    0, 3743],\n",
      "        [   0,    0, 3752],\n",
      "        [   0,    0, 3758],\n",
      "        [   0,    0, 3784],\n",
      "        [   0,    0, 3801],\n",
      "        [   0,    0, 3847],\n",
      "        [   0,    0, 3848],\n",
      "        [   0,    0, 3898],\n",
      "        [   0,    0, 3902],\n",
      "        [   0,    0, 3946],\n",
      "        [   0,    0, 3976],\n",
      "        [   0,    0, 3995],\n",
      "        [   0,    0, 3998],\n",
      "        [   0,    0, 4009],\n",
      "        [   0,    0, 4010],\n",
      "        [   0,    0, 4034],\n",
      "        [   0,    0, 4036],\n",
      "        [   0,    0, 4062],\n",
      "        [   0,    0, 4093]], device='cuda:0')}\n"
     ]
    }
   ],
   "source": [
    "# print(out[1]['decoder.22.mlp.hook_post'].squeeze(0, 1)[336])\n",
    "# print(out[1]['decoder.22.hook_mlp_out'].shape)\n",
    "# print((out[1]['decoder.22.mlp.hook_post'].squeeze(0,1) > 0).nonzero(as_tuple=True))\n",
    "print(range(10))\n",
    "print(extract_activated_neurons(out, range(17,24)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['hook_embed', 'encoder.0.hook_resid_pre', 'encoder.0.ln1.hook_scale', 'encoder.0.ln1.hook_normalized', 'encoder.0.attn.hook_q', 'encoder.0.attn.hook_k', 'encoder.0.attn.hook_v', 'encoder.0.attn.hook_attn_scores', 'encoder.0.attn.hook_pattern', 'encoder.0.attn.hook_z', 'encoder.0.hook_attn_out', 'encoder.0.hook_resid_mid', 'encoder.0.ln2.hook_scale', 'encoder.0.ln2.hook_normalized', 'encoder.0.mlp.hook_pre', 'encoder.0.mlp.hook_post', 'encoder.0.hook_mlp_out', 'encoder.0.hook_resid_post', 'encoder.1.hook_resid_pre', 'encoder.1.ln1.hook_scale', 'encoder.1.ln1.hook_normalized', 'encoder.1.attn.hook_q', 'encoder.1.attn.hook_k', 'encoder.1.attn.hook_v', 'encoder.1.attn.hook_attn_scores', 'encoder.1.attn.hook_pattern', 'encoder.1.attn.hook_z', 'encoder.1.hook_attn_out', 'encoder.1.hook_resid_mid', 'encoder.1.ln2.hook_scale', 'encoder.1.ln2.hook_normalized', 'encoder.1.mlp.hook_pre', 'encoder.1.mlp.hook_post', 'encoder.1.hook_mlp_out', 'encoder.1.hook_resid_post', 'encoder.2.hook_resid_pre', 'encoder.2.ln1.hook_scale', 'encoder.2.ln1.hook_normalized', 'encoder.2.attn.hook_q', 'encoder.2.attn.hook_k', 'encoder.2.attn.hook_v', 'encoder.2.attn.hook_attn_scores', 'encoder.2.attn.hook_pattern', 'encoder.2.attn.hook_z', 'encoder.2.hook_attn_out', 'encoder.2.hook_resid_mid', 'encoder.2.ln2.hook_scale', 'encoder.2.ln2.hook_normalized', 'encoder.2.mlp.hook_pre', 'encoder.2.mlp.hook_post', 'encoder.2.hook_mlp_out', 'encoder.2.hook_resid_post', 'encoder.3.hook_resid_pre', 'encoder.3.ln1.hook_scale', 'encoder.3.ln1.hook_normalized', 'encoder.3.attn.hook_q', 'encoder.3.attn.hook_k', 'encoder.3.attn.hook_v', 'encoder.3.attn.hook_attn_scores', 'encoder.3.attn.hook_pattern', 'encoder.3.attn.hook_z', 'encoder.3.hook_attn_out', 'encoder.3.hook_resid_mid', 'encoder.3.ln2.hook_scale', 'encoder.3.ln2.hook_normalized', 'encoder.3.mlp.hook_pre', 'encoder.3.mlp.hook_post', 'encoder.3.hook_mlp_out', 'encoder.3.hook_resid_post', 'encoder.4.hook_resid_pre', 'encoder.4.ln1.hook_scale', 'encoder.4.ln1.hook_normalized', 'encoder.4.attn.hook_q', 'encoder.4.attn.hook_k', 'encoder.4.attn.hook_v', 'encoder.4.attn.hook_attn_scores', 'encoder.4.attn.hook_pattern', 'encoder.4.attn.hook_z', 'encoder.4.hook_attn_out', 'encoder.4.hook_resid_mid', 'encoder.4.ln2.hook_scale', 'encoder.4.ln2.hook_normalized', 'encoder.4.mlp.hook_pre', 'encoder.4.mlp.hook_post', 'encoder.4.hook_mlp_out', 'encoder.4.hook_resid_post', 'encoder.5.hook_resid_pre', 'encoder.5.ln1.hook_scale', 'encoder.5.ln1.hook_normalized', 'encoder.5.attn.hook_q', 'encoder.5.attn.hook_k', 'encoder.5.attn.hook_v', 'encoder.5.attn.hook_attn_scores', 'encoder.5.attn.hook_pattern', 'encoder.5.attn.hook_z', 'encoder.5.hook_attn_out', 'encoder.5.hook_resid_mid', 'encoder.5.ln2.hook_scale', 'encoder.5.ln2.hook_normalized', 'encoder.5.mlp.hook_pre', 'encoder.5.mlp.hook_post', 'encoder.5.hook_mlp_out', 'encoder.5.hook_resid_post', 'encoder.6.hook_resid_pre', 'encoder.6.ln1.hook_scale', 'encoder.6.ln1.hook_normalized', 'encoder.6.attn.hook_q', 'encoder.6.attn.hook_k', 'encoder.6.attn.hook_v', 'encoder.6.attn.hook_attn_scores', 'encoder.6.attn.hook_pattern', 'encoder.6.attn.hook_z', 'encoder.6.hook_attn_out', 'encoder.6.hook_resid_mid', 'encoder.6.ln2.hook_scale', 'encoder.6.ln2.hook_normalized', 'encoder.6.mlp.hook_pre', 'encoder.6.mlp.hook_post', 'encoder.6.hook_mlp_out', 'encoder.6.hook_resid_post', 'encoder.7.hook_resid_pre', 'encoder.7.ln1.hook_scale', 'encoder.7.ln1.hook_normalized', 'encoder.7.attn.hook_q', 'encoder.7.attn.hook_k', 'encoder.7.attn.hook_v', 'encoder.7.attn.hook_attn_scores', 'encoder.7.attn.hook_pattern', 'encoder.7.attn.hook_z', 'encoder.7.hook_attn_out', 'encoder.7.hook_resid_mid', 'encoder.7.ln2.hook_scale', 'encoder.7.ln2.hook_normalized', 'encoder.7.mlp.hook_pre', 'encoder.7.mlp.hook_post', 'encoder.7.hook_mlp_out', 'encoder.7.hook_resid_post', 'encoder.8.hook_resid_pre', 'encoder.8.ln1.hook_scale', 'encoder.8.ln1.hook_normalized', 'encoder.8.attn.hook_q', 'encoder.8.attn.hook_k', 'encoder.8.attn.hook_v', 'encoder.8.attn.hook_attn_scores', 'encoder.8.attn.hook_pattern', 'encoder.8.attn.hook_z', 'encoder.8.hook_attn_out', 'encoder.8.hook_resid_mid', 'encoder.8.ln2.hook_scale', 'encoder.8.ln2.hook_normalized', 'encoder.8.mlp.hook_pre', 'encoder.8.mlp.hook_post', 'encoder.8.hook_mlp_out', 'encoder.8.hook_resid_post', 'encoder.9.hook_resid_pre', 'encoder.9.ln1.hook_scale', 'encoder.9.ln1.hook_normalized', 'encoder.9.attn.hook_q', 'encoder.9.attn.hook_k', 'encoder.9.attn.hook_v', 'encoder.9.attn.hook_attn_scores', 'encoder.9.attn.hook_pattern', 'encoder.9.attn.hook_z', 'encoder.9.hook_attn_out', 'encoder.9.hook_resid_mid', 'encoder.9.ln2.hook_scale', 'encoder.9.ln2.hook_normalized', 'encoder.9.mlp.hook_pre', 'encoder.9.mlp.hook_post', 'encoder.9.hook_mlp_out', 'encoder.9.hook_resid_post', 'encoder.10.hook_resid_pre', 'encoder.10.ln1.hook_scale', 'encoder.10.ln1.hook_normalized', 'encoder.10.attn.hook_q', 'encoder.10.attn.hook_k', 'encoder.10.attn.hook_v', 'encoder.10.attn.hook_attn_scores', 'encoder.10.attn.hook_pattern', 'encoder.10.attn.hook_z', 'encoder.10.hook_attn_out', 'encoder.10.hook_resid_mid', 'encoder.10.ln2.hook_scale', 'encoder.10.ln2.hook_normalized', 'encoder.10.mlp.hook_pre', 'encoder.10.mlp.hook_post', 'encoder.10.hook_mlp_out', 'encoder.10.hook_resid_post', 'encoder.11.hook_resid_pre', 'encoder.11.ln1.hook_scale', 'encoder.11.ln1.hook_normalized', 'encoder.11.attn.hook_q', 'encoder.11.attn.hook_k', 'encoder.11.attn.hook_v', 'encoder.11.attn.hook_attn_scores', 'encoder.11.attn.hook_pattern', 'encoder.11.attn.hook_z', 'encoder.11.hook_attn_out', 'encoder.11.hook_resid_mid', 'encoder.11.ln2.hook_scale', 'encoder.11.ln2.hook_normalized', 'encoder.11.mlp.hook_pre', 'encoder.11.mlp.hook_post', 'encoder.11.hook_mlp_out', 'encoder.11.hook_resid_post', 'encoder.12.hook_resid_pre', 'encoder.12.ln1.hook_scale', 'encoder.12.ln1.hook_normalized', 'encoder.12.attn.hook_q', 'encoder.12.attn.hook_k', 'encoder.12.attn.hook_v', 'encoder.12.attn.hook_attn_scores', 'encoder.12.attn.hook_pattern', 'encoder.12.attn.hook_z', 'encoder.12.hook_attn_out', 'encoder.12.hook_resid_mid', 'encoder.12.ln2.hook_scale', 'encoder.12.ln2.hook_normalized', 'encoder.12.mlp.hook_pre', 'encoder.12.mlp.hook_post', 'encoder.12.hook_mlp_out', 'encoder.12.hook_resid_post', 'encoder.13.hook_resid_pre', 'encoder.13.ln1.hook_scale', 'encoder.13.ln1.hook_normalized', 'encoder.13.attn.hook_q', 'encoder.13.attn.hook_k', 'encoder.13.attn.hook_v', 'encoder.13.attn.hook_attn_scores', 'encoder.13.attn.hook_pattern', 'encoder.13.attn.hook_z', 'encoder.13.hook_attn_out', 'encoder.13.hook_resid_mid', 'encoder.13.ln2.hook_scale', 'encoder.13.ln2.hook_normalized', 'encoder.13.mlp.hook_pre', 'encoder.13.mlp.hook_post', 'encoder.13.hook_mlp_out', 'encoder.13.hook_resid_post', 'encoder.14.hook_resid_pre', 'encoder.14.ln1.hook_scale', 'encoder.14.ln1.hook_normalized', 'encoder.14.attn.hook_q', 'encoder.14.attn.hook_k', 'encoder.14.attn.hook_v', 'encoder.14.attn.hook_attn_scores', 'encoder.14.attn.hook_pattern', 'encoder.14.attn.hook_z', 'encoder.14.hook_attn_out', 'encoder.14.hook_resid_mid', 'encoder.14.ln2.hook_scale', 'encoder.14.ln2.hook_normalized', 'encoder.14.mlp.hook_pre', 'encoder.14.mlp.hook_post', 'encoder.14.hook_mlp_out', 'encoder.14.hook_resid_post', 'encoder.15.hook_resid_pre', 'encoder.15.ln1.hook_scale', 'encoder.15.ln1.hook_normalized', 'encoder.15.attn.hook_q', 'encoder.15.attn.hook_k', 'encoder.15.attn.hook_v', 'encoder.15.attn.hook_attn_scores', 'encoder.15.attn.hook_pattern', 'encoder.15.attn.hook_z', 'encoder.15.hook_attn_out', 'encoder.15.hook_resid_mid', 'encoder.15.ln2.hook_scale', 'encoder.15.ln2.hook_normalized', 'encoder.15.mlp.hook_pre', 'encoder.15.mlp.hook_post', 'encoder.15.hook_mlp_out', 'encoder.15.hook_resid_post', 'encoder.16.hook_resid_pre', 'encoder.16.ln1.hook_scale', 'encoder.16.ln1.hook_normalized', 'encoder.16.attn.hook_q', 'encoder.16.attn.hook_k', 'encoder.16.attn.hook_v', 'encoder.16.attn.hook_attn_scores', 'encoder.16.attn.hook_pattern', 'encoder.16.attn.hook_z', 'encoder.16.hook_attn_out', 'encoder.16.hook_resid_mid', 'encoder.16.ln2.hook_scale', 'encoder.16.ln2.hook_normalized', 'encoder.16.mlp.hook_pre', 'encoder.16.mlp.hook_post', 'encoder.16.hook_mlp_out', 'encoder.16.hook_resid_post', 'encoder.17.hook_resid_pre', 'encoder.17.ln1.hook_scale', 'encoder.17.ln1.hook_normalized', 'encoder.17.attn.hook_q', 'encoder.17.attn.hook_k', 'encoder.17.attn.hook_v', 'encoder.17.attn.hook_attn_scores', 'encoder.17.attn.hook_pattern', 'encoder.17.attn.hook_z', 'encoder.17.hook_attn_out', 'encoder.17.hook_resid_mid', 'encoder.17.ln2.hook_scale', 'encoder.17.ln2.hook_normalized', 'encoder.17.mlp.hook_pre', 'encoder.17.mlp.hook_post', 'encoder.17.hook_mlp_out', 'encoder.17.hook_resid_post', 'encoder.18.hook_resid_pre', 'encoder.18.ln1.hook_scale', 'encoder.18.ln1.hook_normalized', 'encoder.18.attn.hook_q', 'encoder.18.attn.hook_k', 'encoder.18.attn.hook_v', 'encoder.18.attn.hook_attn_scores', 'encoder.18.attn.hook_pattern', 'encoder.18.attn.hook_z', 'encoder.18.hook_attn_out', 'encoder.18.hook_resid_mid', 'encoder.18.ln2.hook_scale', 'encoder.18.ln2.hook_normalized', 'encoder.18.mlp.hook_pre', 'encoder.18.mlp.hook_post', 'encoder.18.hook_mlp_out', 'encoder.18.hook_resid_post', 'encoder.19.hook_resid_pre', 'encoder.19.ln1.hook_scale', 'encoder.19.ln1.hook_normalized', 'encoder.19.attn.hook_q', 'encoder.19.attn.hook_k', 'encoder.19.attn.hook_v', 'encoder.19.attn.hook_attn_scores', 'encoder.19.attn.hook_pattern', 'encoder.19.attn.hook_z', 'encoder.19.hook_attn_out', 'encoder.19.hook_resid_mid', 'encoder.19.ln2.hook_scale', 'encoder.19.ln2.hook_normalized', 'encoder.19.mlp.hook_pre', 'encoder.19.mlp.hook_post', 'encoder.19.hook_mlp_out', 'encoder.19.hook_resid_post', 'encoder.20.hook_resid_pre', 'encoder.20.ln1.hook_scale', 'encoder.20.ln1.hook_normalized', 'encoder.20.attn.hook_q', 'encoder.20.attn.hook_k', 'encoder.20.attn.hook_v', 'encoder.20.attn.hook_attn_scores', 'encoder.20.attn.hook_pattern', 'encoder.20.attn.hook_z', 'encoder.20.hook_attn_out', 'encoder.20.hook_resid_mid', 'encoder.20.ln2.hook_scale', 'encoder.20.ln2.hook_normalized', 'encoder.20.mlp.hook_pre', 'encoder.20.mlp.hook_post', 'encoder.20.hook_mlp_out', 'encoder.20.hook_resid_post', 'encoder.21.hook_resid_pre', 'encoder.21.ln1.hook_scale', 'encoder.21.ln1.hook_normalized', 'encoder.21.attn.hook_q', 'encoder.21.attn.hook_k', 'encoder.21.attn.hook_v', 'encoder.21.attn.hook_attn_scores', 'encoder.21.attn.hook_pattern', 'encoder.21.attn.hook_z', 'encoder.21.hook_attn_out', 'encoder.21.hook_resid_mid', 'encoder.21.ln2.hook_scale', 'encoder.21.ln2.hook_normalized', 'encoder.21.mlp.hook_pre', 'encoder.21.mlp.hook_post', 'encoder.21.hook_mlp_out', 'encoder.21.hook_resid_post', 'encoder.22.hook_resid_pre', 'encoder.22.ln1.hook_scale', 'encoder.22.ln1.hook_normalized', 'encoder.22.attn.hook_q', 'encoder.22.attn.hook_k', 'encoder.22.attn.hook_v', 'encoder.22.attn.hook_attn_scores', 'encoder.22.attn.hook_pattern', 'encoder.22.attn.hook_z', 'encoder.22.hook_attn_out', 'encoder.22.hook_resid_mid', 'encoder.22.ln2.hook_scale', 'encoder.22.ln2.hook_normalized', 'encoder.22.mlp.hook_pre', 'encoder.22.mlp.hook_post', 'encoder.22.hook_mlp_out', 'encoder.22.hook_resid_post', 'encoder.23.hook_resid_pre', 'encoder.23.ln1.hook_scale', 'encoder.23.ln1.hook_normalized', 'encoder.23.attn.hook_q', 'encoder.23.attn.hook_k', 'encoder.23.attn.hook_v', 'encoder.23.attn.hook_attn_scores', 'encoder.23.attn.hook_pattern', 'encoder.23.attn.hook_z', 'encoder.23.hook_attn_out', 'encoder.23.hook_resid_mid', 'encoder.23.ln2.hook_scale', 'encoder.23.ln2.hook_normalized', 'encoder.23.mlp.hook_pre', 'encoder.23.mlp.hook_post', 'encoder.23.hook_mlp_out', 'encoder.23.hook_resid_post', 'encoder_final_ln.hook_scale', 'encoder_final_ln.hook_normalized', 'decoder.0.hook_resid_pre', 'decoder.0.ln1.hook_scale', 'decoder.0.ln1.hook_normalized', 'decoder.0.attn.hook_q', 'decoder.0.attn.hook_k', 'decoder.0.attn.hook_v', 'decoder.0.attn.hook_attn_scores', 'decoder.0.attn.hook_pattern', 'decoder.0.attn.hook_z', 'decoder.0.hook_attn_out', 'decoder.0.hook_resid_mid', 'decoder.0.ln2.hook_scale', 'decoder.0.ln2.hook_normalized', 'decoder.0.cross_attn.hook_q', 'decoder.0.cross_attn.hook_k', 'decoder.0.cross_attn.hook_v', 'decoder.0.cross_attn.hook_attn_scores', 'decoder.0.cross_attn.hook_pattern', 'decoder.0.cross_attn.hook_z', 'decoder.0.hook_cross_attn_out', 'decoder.0.hook_resid_mid_cross', 'decoder.0.ln3.hook_scale', 'decoder.0.ln3.hook_normalized', 'decoder.0.mlp.hook_pre', 'decoder.0.mlp.hook_post', 'decoder.0.hook_mlp_out', 'decoder.0.hook_resid_post', 'decoder.1.hook_resid_pre', 'decoder.1.ln1.hook_scale', 'decoder.1.ln1.hook_normalized', 'decoder.1.attn.hook_q', 'decoder.1.attn.hook_k', 'decoder.1.attn.hook_v', 'decoder.1.attn.hook_attn_scores', 'decoder.1.attn.hook_pattern', 'decoder.1.attn.hook_z', 'decoder.1.hook_attn_out', 'decoder.1.hook_resid_mid', 'decoder.1.ln2.hook_scale', 'decoder.1.ln2.hook_normalized', 'decoder.1.cross_attn.hook_q', 'decoder.1.cross_attn.hook_k', 'decoder.1.cross_attn.hook_v', 'decoder.1.cross_attn.hook_attn_scores', 'decoder.1.cross_attn.hook_pattern', 'decoder.1.cross_attn.hook_z', 'decoder.1.hook_cross_attn_out', 'decoder.1.hook_resid_mid_cross', 'decoder.1.ln3.hook_scale', 'decoder.1.ln3.hook_normalized', 'decoder.1.mlp.hook_pre', 'decoder.1.mlp.hook_post', 'decoder.1.hook_mlp_out', 'decoder.1.hook_resid_post', 'decoder.2.hook_resid_pre', 'decoder.2.ln1.hook_scale', 'decoder.2.ln1.hook_normalized', 'decoder.2.attn.hook_q', 'decoder.2.attn.hook_k', 'decoder.2.attn.hook_v', 'decoder.2.attn.hook_attn_scores', 'decoder.2.attn.hook_pattern', 'decoder.2.attn.hook_z', 'decoder.2.hook_attn_out', 'decoder.2.hook_resid_mid', 'decoder.2.ln2.hook_scale', 'decoder.2.ln2.hook_normalized', 'decoder.2.cross_attn.hook_q', 'decoder.2.cross_attn.hook_k', 'decoder.2.cross_attn.hook_v', 'decoder.2.cross_attn.hook_attn_scores', 'decoder.2.cross_attn.hook_pattern', 'decoder.2.cross_attn.hook_z', 'decoder.2.hook_cross_attn_out', 'decoder.2.hook_resid_mid_cross', 'decoder.2.ln3.hook_scale', 'decoder.2.ln3.hook_normalized', 'decoder.2.mlp.hook_pre', 'decoder.2.mlp.hook_post', 'decoder.2.hook_mlp_out', 'decoder.2.hook_resid_post', 'decoder.3.hook_resid_pre', 'decoder.3.ln1.hook_scale', 'decoder.3.ln1.hook_normalized', 'decoder.3.attn.hook_q', 'decoder.3.attn.hook_k', 'decoder.3.attn.hook_v', 'decoder.3.attn.hook_attn_scores', 'decoder.3.attn.hook_pattern', 'decoder.3.attn.hook_z', 'decoder.3.hook_attn_out', 'decoder.3.hook_resid_mid', 'decoder.3.ln2.hook_scale', 'decoder.3.ln2.hook_normalized', 'decoder.3.cross_attn.hook_q', 'decoder.3.cross_attn.hook_k', 'decoder.3.cross_attn.hook_v', 'decoder.3.cross_attn.hook_attn_scores', 'decoder.3.cross_attn.hook_pattern', 'decoder.3.cross_attn.hook_z', 'decoder.3.hook_cross_attn_out', 'decoder.3.hook_resid_mid_cross', 'decoder.3.ln3.hook_scale', 'decoder.3.ln3.hook_normalized', 'decoder.3.mlp.hook_pre', 'decoder.3.mlp.hook_post', 'decoder.3.hook_mlp_out', 'decoder.3.hook_resid_post', 'decoder.4.hook_resid_pre', 'decoder.4.ln1.hook_scale', 'decoder.4.ln1.hook_normalized', 'decoder.4.attn.hook_q', 'decoder.4.attn.hook_k', 'decoder.4.attn.hook_v', 'decoder.4.attn.hook_attn_scores', 'decoder.4.attn.hook_pattern', 'decoder.4.attn.hook_z', 'decoder.4.hook_attn_out', 'decoder.4.hook_resid_mid', 'decoder.4.ln2.hook_scale', 'decoder.4.ln2.hook_normalized', 'decoder.4.cross_attn.hook_q', 'decoder.4.cross_attn.hook_k', 'decoder.4.cross_attn.hook_v', 'decoder.4.cross_attn.hook_attn_scores', 'decoder.4.cross_attn.hook_pattern', 'decoder.4.cross_attn.hook_z', 'decoder.4.hook_cross_attn_out', 'decoder.4.hook_resid_mid_cross', 'decoder.4.ln3.hook_scale', 'decoder.4.ln3.hook_normalized', 'decoder.4.mlp.hook_pre', 'decoder.4.mlp.hook_post', 'decoder.4.hook_mlp_out', 'decoder.4.hook_resid_post', 'decoder.5.hook_resid_pre', 'decoder.5.ln1.hook_scale', 'decoder.5.ln1.hook_normalized', 'decoder.5.attn.hook_q', 'decoder.5.attn.hook_k', 'decoder.5.attn.hook_v', 'decoder.5.attn.hook_attn_scores', 'decoder.5.attn.hook_pattern', 'decoder.5.attn.hook_z', 'decoder.5.hook_attn_out', 'decoder.5.hook_resid_mid', 'decoder.5.ln2.hook_scale', 'decoder.5.ln2.hook_normalized', 'decoder.5.cross_attn.hook_q', 'decoder.5.cross_attn.hook_k', 'decoder.5.cross_attn.hook_v', 'decoder.5.cross_attn.hook_attn_scores', 'decoder.5.cross_attn.hook_pattern', 'decoder.5.cross_attn.hook_z', 'decoder.5.hook_cross_attn_out', 'decoder.5.hook_resid_mid_cross', 'decoder.5.ln3.hook_scale', 'decoder.5.ln3.hook_normalized', 'decoder.5.mlp.hook_pre', 'decoder.5.mlp.hook_post', 'decoder.5.hook_mlp_out', 'decoder.5.hook_resid_post', 'decoder.6.hook_resid_pre', 'decoder.6.ln1.hook_scale', 'decoder.6.ln1.hook_normalized', 'decoder.6.attn.hook_q', 'decoder.6.attn.hook_k', 'decoder.6.attn.hook_v', 'decoder.6.attn.hook_attn_scores', 'decoder.6.attn.hook_pattern', 'decoder.6.attn.hook_z', 'decoder.6.hook_attn_out', 'decoder.6.hook_resid_mid', 'decoder.6.ln2.hook_scale', 'decoder.6.ln2.hook_normalized', 'decoder.6.cross_attn.hook_q', 'decoder.6.cross_attn.hook_k', 'decoder.6.cross_attn.hook_v', 'decoder.6.cross_attn.hook_attn_scores', 'decoder.6.cross_attn.hook_pattern', 'decoder.6.cross_attn.hook_z', 'decoder.6.hook_cross_attn_out', 'decoder.6.hook_resid_mid_cross', 'decoder.6.ln3.hook_scale', 'decoder.6.ln3.hook_normalized', 'decoder.6.mlp.hook_pre', 'decoder.6.mlp.hook_post', 'decoder.6.hook_mlp_out', 'decoder.6.hook_resid_post', 'decoder.7.hook_resid_pre', 'decoder.7.ln1.hook_scale', 'decoder.7.ln1.hook_normalized', 'decoder.7.attn.hook_q', 'decoder.7.attn.hook_k', 'decoder.7.attn.hook_v', 'decoder.7.attn.hook_attn_scores', 'decoder.7.attn.hook_pattern', 'decoder.7.attn.hook_z', 'decoder.7.hook_attn_out', 'decoder.7.hook_resid_mid', 'decoder.7.ln2.hook_scale', 'decoder.7.ln2.hook_normalized', 'decoder.7.cross_attn.hook_q', 'decoder.7.cross_attn.hook_k', 'decoder.7.cross_attn.hook_v', 'decoder.7.cross_attn.hook_attn_scores', 'decoder.7.cross_attn.hook_pattern', 'decoder.7.cross_attn.hook_z', 'decoder.7.hook_cross_attn_out', 'decoder.7.hook_resid_mid_cross', 'decoder.7.ln3.hook_scale', 'decoder.7.ln3.hook_normalized', 'decoder.7.mlp.hook_pre', 'decoder.7.mlp.hook_post', 'decoder.7.hook_mlp_out', 'decoder.7.hook_resid_post', 'decoder.8.hook_resid_pre', 'decoder.8.ln1.hook_scale', 'decoder.8.ln1.hook_normalized', 'decoder.8.attn.hook_q', 'decoder.8.attn.hook_k', 'decoder.8.attn.hook_v', 'decoder.8.attn.hook_attn_scores', 'decoder.8.attn.hook_pattern', 'decoder.8.attn.hook_z', 'decoder.8.hook_attn_out', 'decoder.8.hook_resid_mid', 'decoder.8.ln2.hook_scale', 'decoder.8.ln2.hook_normalized', 'decoder.8.cross_attn.hook_q', 'decoder.8.cross_attn.hook_k', 'decoder.8.cross_attn.hook_v', 'decoder.8.cross_attn.hook_attn_scores', 'decoder.8.cross_attn.hook_pattern', 'decoder.8.cross_attn.hook_z', 'decoder.8.hook_cross_attn_out', 'decoder.8.hook_resid_mid_cross', 'decoder.8.ln3.hook_scale', 'decoder.8.ln3.hook_normalized', 'decoder.8.mlp.hook_pre', 'decoder.8.mlp.hook_post', 'decoder.8.hook_mlp_out', 'decoder.8.hook_resid_post', 'decoder.9.hook_resid_pre', 'decoder.9.ln1.hook_scale', 'decoder.9.ln1.hook_normalized', 'decoder.9.attn.hook_q', 'decoder.9.attn.hook_k', 'decoder.9.attn.hook_v', 'decoder.9.attn.hook_attn_scores', 'decoder.9.attn.hook_pattern', 'decoder.9.attn.hook_z', 'decoder.9.hook_attn_out', 'decoder.9.hook_resid_mid', 'decoder.9.ln2.hook_scale', 'decoder.9.ln2.hook_normalized', 'decoder.9.cross_attn.hook_q', 'decoder.9.cross_attn.hook_k', 'decoder.9.cross_attn.hook_v', 'decoder.9.cross_attn.hook_attn_scores', 'decoder.9.cross_attn.hook_pattern', 'decoder.9.cross_attn.hook_z', 'decoder.9.hook_cross_attn_out', 'decoder.9.hook_resid_mid_cross', 'decoder.9.ln3.hook_scale', 'decoder.9.ln3.hook_normalized', 'decoder.9.mlp.hook_pre', 'decoder.9.mlp.hook_post', 'decoder.9.hook_mlp_out', 'decoder.9.hook_resid_post', 'decoder.10.hook_resid_pre', 'decoder.10.ln1.hook_scale', 'decoder.10.ln1.hook_normalized', 'decoder.10.attn.hook_q', 'decoder.10.attn.hook_k', 'decoder.10.attn.hook_v', 'decoder.10.attn.hook_attn_scores', 'decoder.10.attn.hook_pattern', 'decoder.10.attn.hook_z', 'decoder.10.hook_attn_out', 'decoder.10.hook_resid_mid', 'decoder.10.ln2.hook_scale', 'decoder.10.ln2.hook_normalized', 'decoder.10.cross_attn.hook_q', 'decoder.10.cross_attn.hook_k', 'decoder.10.cross_attn.hook_v', 'decoder.10.cross_attn.hook_attn_scores', 'decoder.10.cross_attn.hook_pattern', 'decoder.10.cross_attn.hook_z', 'decoder.10.hook_cross_attn_out', 'decoder.10.hook_resid_mid_cross', 'decoder.10.ln3.hook_scale', 'decoder.10.ln3.hook_normalized', 'decoder.10.mlp.hook_pre', 'decoder.10.mlp.hook_post', 'decoder.10.hook_mlp_out', 'decoder.10.hook_resid_post', 'decoder.11.hook_resid_pre', 'decoder.11.ln1.hook_scale', 'decoder.11.ln1.hook_normalized', 'decoder.11.attn.hook_q', 'decoder.11.attn.hook_k', 'decoder.11.attn.hook_v', 'decoder.11.attn.hook_attn_scores', 'decoder.11.attn.hook_pattern', 'decoder.11.attn.hook_z', 'decoder.11.hook_attn_out', 'decoder.11.hook_resid_mid', 'decoder.11.ln2.hook_scale', 'decoder.11.ln2.hook_normalized', 'decoder.11.cross_attn.hook_q', 'decoder.11.cross_attn.hook_k', 'decoder.11.cross_attn.hook_v', 'decoder.11.cross_attn.hook_attn_scores', 'decoder.11.cross_attn.hook_pattern', 'decoder.11.cross_attn.hook_z', 'decoder.11.hook_cross_attn_out', 'decoder.11.hook_resid_mid_cross', 'decoder.11.ln3.hook_scale', 'decoder.11.ln3.hook_normalized', 'decoder.11.mlp.hook_pre', 'decoder.11.mlp.hook_post', 'decoder.11.hook_mlp_out', 'decoder.11.hook_resid_post', 'decoder.12.hook_resid_pre', 'decoder.12.ln1.hook_scale', 'decoder.12.ln1.hook_normalized', 'decoder.12.attn.hook_q', 'decoder.12.attn.hook_k', 'decoder.12.attn.hook_v', 'decoder.12.attn.hook_attn_scores', 'decoder.12.attn.hook_pattern', 'decoder.12.attn.hook_z', 'decoder.12.hook_attn_out', 'decoder.12.hook_resid_mid', 'decoder.12.ln2.hook_scale', 'decoder.12.ln2.hook_normalized', 'decoder.12.cross_attn.hook_q', 'decoder.12.cross_attn.hook_k', 'decoder.12.cross_attn.hook_v', 'decoder.12.cross_attn.hook_attn_scores', 'decoder.12.cross_attn.hook_pattern', 'decoder.12.cross_attn.hook_z', 'decoder.12.hook_cross_attn_out', 'decoder.12.hook_resid_mid_cross', 'decoder.12.ln3.hook_scale', 'decoder.12.ln3.hook_normalized', 'decoder.12.mlp.hook_pre', 'decoder.12.mlp.hook_post', 'decoder.12.hook_mlp_out', 'decoder.12.hook_resid_post', 'decoder.13.hook_resid_pre', 'decoder.13.ln1.hook_scale', 'decoder.13.ln1.hook_normalized', 'decoder.13.attn.hook_q', 'decoder.13.attn.hook_k', 'decoder.13.attn.hook_v', 'decoder.13.attn.hook_attn_scores', 'decoder.13.attn.hook_pattern', 'decoder.13.attn.hook_z', 'decoder.13.hook_attn_out', 'decoder.13.hook_resid_mid', 'decoder.13.ln2.hook_scale', 'decoder.13.ln2.hook_normalized', 'decoder.13.cross_attn.hook_q', 'decoder.13.cross_attn.hook_k', 'decoder.13.cross_attn.hook_v', 'decoder.13.cross_attn.hook_attn_scores', 'decoder.13.cross_attn.hook_pattern', 'decoder.13.cross_attn.hook_z', 'decoder.13.hook_cross_attn_out', 'decoder.13.hook_resid_mid_cross', 'decoder.13.ln3.hook_scale', 'decoder.13.ln3.hook_normalized', 'decoder.13.mlp.hook_pre', 'decoder.13.mlp.hook_post', 'decoder.13.hook_mlp_out', 'decoder.13.hook_resid_post', 'decoder.14.hook_resid_pre', 'decoder.14.ln1.hook_scale', 'decoder.14.ln1.hook_normalized', 'decoder.14.attn.hook_q', 'decoder.14.attn.hook_k', 'decoder.14.attn.hook_v', 'decoder.14.attn.hook_attn_scores', 'decoder.14.attn.hook_pattern', 'decoder.14.attn.hook_z', 'decoder.14.hook_attn_out', 'decoder.14.hook_resid_mid', 'decoder.14.ln2.hook_scale', 'decoder.14.ln2.hook_normalized', 'decoder.14.cross_attn.hook_q', 'decoder.14.cross_attn.hook_k', 'decoder.14.cross_attn.hook_v', 'decoder.14.cross_attn.hook_attn_scores', 'decoder.14.cross_attn.hook_pattern', 'decoder.14.cross_attn.hook_z', 'decoder.14.hook_cross_attn_out', 'decoder.14.hook_resid_mid_cross', 'decoder.14.ln3.hook_scale', 'decoder.14.ln3.hook_normalized', 'decoder.14.mlp.hook_pre', 'decoder.14.mlp.hook_post', 'decoder.14.hook_mlp_out', 'decoder.14.hook_resid_post', 'decoder.15.hook_resid_pre', 'decoder.15.ln1.hook_scale', 'decoder.15.ln1.hook_normalized', 'decoder.15.attn.hook_q', 'decoder.15.attn.hook_k', 'decoder.15.attn.hook_v', 'decoder.15.attn.hook_attn_scores', 'decoder.15.attn.hook_pattern', 'decoder.15.attn.hook_z', 'decoder.15.hook_attn_out', 'decoder.15.hook_resid_mid', 'decoder.15.ln2.hook_scale', 'decoder.15.ln2.hook_normalized', 'decoder.15.cross_attn.hook_q', 'decoder.15.cross_attn.hook_k', 'decoder.15.cross_attn.hook_v', 'decoder.15.cross_attn.hook_attn_scores', 'decoder.15.cross_attn.hook_pattern', 'decoder.15.cross_attn.hook_z', 'decoder.15.hook_cross_attn_out', 'decoder.15.hook_resid_mid_cross', 'decoder.15.ln3.hook_scale', 'decoder.15.ln3.hook_normalized', 'decoder.15.mlp.hook_pre', 'decoder.15.mlp.hook_post', 'decoder.15.hook_mlp_out', 'decoder.15.hook_resid_post', 'decoder.16.hook_resid_pre', 'decoder.16.ln1.hook_scale', 'decoder.16.ln1.hook_normalized', 'decoder.16.attn.hook_q', 'decoder.16.attn.hook_k', 'decoder.16.attn.hook_v', 'decoder.16.attn.hook_attn_scores', 'decoder.16.attn.hook_pattern', 'decoder.16.attn.hook_z', 'decoder.16.hook_attn_out', 'decoder.16.hook_resid_mid', 'decoder.16.ln2.hook_scale', 'decoder.16.ln2.hook_normalized', 'decoder.16.cross_attn.hook_q', 'decoder.16.cross_attn.hook_k', 'decoder.16.cross_attn.hook_v', 'decoder.16.cross_attn.hook_attn_scores', 'decoder.16.cross_attn.hook_pattern', 'decoder.16.cross_attn.hook_z', 'decoder.16.hook_cross_attn_out', 'decoder.16.hook_resid_mid_cross', 'decoder.16.ln3.hook_scale', 'decoder.16.ln3.hook_normalized', 'decoder.16.mlp.hook_pre', 'decoder.16.mlp.hook_post', 'decoder.16.hook_mlp_out', 'decoder.16.hook_resid_post', 'decoder.17.hook_resid_pre', 'decoder.17.ln1.hook_scale', 'decoder.17.ln1.hook_normalized', 'decoder.17.attn.hook_q', 'decoder.17.attn.hook_k', 'decoder.17.attn.hook_v', 'decoder.17.attn.hook_attn_scores', 'decoder.17.attn.hook_pattern', 'decoder.17.attn.hook_z', 'decoder.17.hook_attn_out', 'decoder.17.hook_resid_mid', 'decoder.17.ln2.hook_scale', 'decoder.17.ln2.hook_normalized', 'decoder.17.cross_attn.hook_q', 'decoder.17.cross_attn.hook_k', 'decoder.17.cross_attn.hook_v', 'decoder.17.cross_attn.hook_attn_scores', 'decoder.17.cross_attn.hook_pattern', 'decoder.17.cross_attn.hook_z', 'decoder.17.hook_cross_attn_out', 'decoder.17.hook_resid_mid_cross', 'decoder.17.ln3.hook_scale', 'decoder.17.ln3.hook_normalized', 'decoder.17.mlp.hook_pre', 'decoder.17.mlp.hook_post', 'decoder.17.hook_mlp_out', 'decoder.17.hook_resid_post', 'decoder.18.hook_resid_pre', 'decoder.18.ln1.hook_scale', 'decoder.18.ln1.hook_normalized', 'decoder.18.attn.hook_q', 'decoder.18.attn.hook_k', 'decoder.18.attn.hook_v', 'decoder.18.attn.hook_attn_scores', 'decoder.18.attn.hook_pattern', 'decoder.18.attn.hook_z', 'decoder.18.hook_attn_out', 'decoder.18.hook_resid_mid', 'decoder.18.ln2.hook_scale', 'decoder.18.ln2.hook_normalized', 'decoder.18.cross_attn.hook_q', 'decoder.18.cross_attn.hook_k', 'decoder.18.cross_attn.hook_v', 'decoder.18.cross_attn.hook_attn_scores', 'decoder.18.cross_attn.hook_pattern', 'decoder.18.cross_attn.hook_z', 'decoder.18.hook_cross_attn_out', 'decoder.18.hook_resid_mid_cross', 'decoder.18.ln3.hook_scale', 'decoder.18.ln3.hook_normalized', 'decoder.18.mlp.hook_pre', 'decoder.18.mlp.hook_post', 'decoder.18.hook_mlp_out', 'decoder.18.hook_resid_post', 'decoder.19.hook_resid_pre', 'decoder.19.ln1.hook_scale', 'decoder.19.ln1.hook_normalized', 'decoder.19.attn.hook_q', 'decoder.19.attn.hook_k', 'decoder.19.attn.hook_v', 'decoder.19.attn.hook_attn_scores', 'decoder.19.attn.hook_pattern', 'decoder.19.attn.hook_z', 'decoder.19.hook_attn_out', 'decoder.19.hook_resid_mid', 'decoder.19.ln2.hook_scale', 'decoder.19.ln2.hook_normalized', 'decoder.19.cross_attn.hook_q', 'decoder.19.cross_attn.hook_k', 'decoder.19.cross_attn.hook_v', 'decoder.19.cross_attn.hook_attn_scores', 'decoder.19.cross_attn.hook_pattern', 'decoder.19.cross_attn.hook_z', 'decoder.19.hook_cross_attn_out', 'decoder.19.hook_resid_mid_cross', 'decoder.19.ln3.hook_scale', 'decoder.19.ln3.hook_normalized', 'decoder.19.mlp.hook_pre', 'decoder.19.mlp.hook_post', 'decoder.19.hook_mlp_out', 'decoder.19.hook_resid_post', 'decoder.20.hook_resid_pre', 'decoder.20.ln1.hook_scale', 'decoder.20.ln1.hook_normalized', 'decoder.20.attn.hook_q', 'decoder.20.attn.hook_k', 'decoder.20.attn.hook_v', 'decoder.20.attn.hook_attn_scores', 'decoder.20.attn.hook_pattern', 'decoder.20.attn.hook_z', 'decoder.20.hook_attn_out', 'decoder.20.hook_resid_mid', 'decoder.20.ln2.hook_scale', 'decoder.20.ln2.hook_normalized', 'decoder.20.cross_attn.hook_q', 'decoder.20.cross_attn.hook_k', 'decoder.20.cross_attn.hook_v', 'decoder.20.cross_attn.hook_attn_scores', 'decoder.20.cross_attn.hook_pattern', 'decoder.20.cross_attn.hook_z', 'decoder.20.hook_cross_attn_out', 'decoder.20.hook_resid_mid_cross', 'decoder.20.ln3.hook_scale', 'decoder.20.ln3.hook_normalized', 'decoder.20.mlp.hook_pre', 'decoder.20.mlp.hook_post', 'decoder.20.hook_mlp_out', 'decoder.20.hook_resid_post', 'decoder.21.hook_resid_pre', 'decoder.21.ln1.hook_scale', 'decoder.21.ln1.hook_normalized', 'decoder.21.attn.hook_q', 'decoder.21.attn.hook_k', 'decoder.21.attn.hook_v', 'decoder.21.attn.hook_attn_scores', 'decoder.21.attn.hook_pattern', 'decoder.21.attn.hook_z', 'decoder.21.hook_attn_out', 'decoder.21.hook_resid_mid', 'decoder.21.ln2.hook_scale', 'decoder.21.ln2.hook_normalized', 'decoder.21.cross_attn.hook_q', 'decoder.21.cross_attn.hook_k', 'decoder.21.cross_attn.hook_v', 'decoder.21.cross_attn.hook_attn_scores', 'decoder.21.cross_attn.hook_pattern', 'decoder.21.cross_attn.hook_z', 'decoder.21.hook_cross_attn_out', 'decoder.21.hook_resid_mid_cross', 'decoder.21.ln3.hook_scale', 'decoder.21.ln3.hook_normalized', 'decoder.21.mlp.hook_pre', 'decoder.21.mlp.hook_post', 'decoder.21.hook_mlp_out', 'decoder.21.hook_resid_post', 'decoder.22.hook_resid_pre', 'decoder.22.ln1.hook_scale', 'decoder.22.ln1.hook_normalized', 'decoder.22.attn.hook_q', 'decoder.22.attn.hook_k', 'decoder.22.attn.hook_v', 'decoder.22.attn.hook_attn_scores', 'decoder.22.attn.hook_pattern', 'decoder.22.attn.hook_z', 'decoder.22.hook_attn_out', 'decoder.22.hook_resid_mid', 'decoder.22.ln2.hook_scale', 'decoder.22.ln2.hook_normalized', 'decoder.22.cross_attn.hook_q', 'decoder.22.cross_attn.hook_k', 'decoder.22.cross_attn.hook_v', 'decoder.22.cross_attn.hook_attn_scores', 'decoder.22.cross_attn.hook_pattern', 'decoder.22.cross_attn.hook_z', 'decoder.22.hook_cross_attn_out', 'decoder.22.hook_resid_mid_cross', 'decoder.22.ln3.hook_scale', 'decoder.22.ln3.hook_normalized', 'decoder.22.mlp.hook_pre', 'decoder.22.mlp.hook_post', 'decoder.22.hook_mlp_out', 'decoder.22.hook_resid_post', 'decoder.23.hook_resid_pre', 'decoder.23.ln1.hook_scale', 'decoder.23.ln1.hook_normalized', 'decoder.23.attn.hook_q', 'decoder.23.attn.hook_k', 'decoder.23.attn.hook_v', 'decoder.23.attn.hook_attn_scores', 'decoder.23.attn.hook_pattern', 'decoder.23.attn.hook_z', 'decoder.23.hook_attn_out', 'decoder.23.hook_resid_mid', 'decoder.23.ln2.hook_scale', 'decoder.23.ln2.hook_normalized', 'decoder.23.cross_attn.hook_q', 'decoder.23.cross_attn.hook_k', 'decoder.23.cross_attn.hook_v', 'decoder.23.cross_attn.hook_attn_scores', 'decoder.23.cross_attn.hook_pattern', 'decoder.23.cross_attn.hook_z', 'decoder.23.hook_cross_attn_out', 'decoder.23.hook_resid_mid_cross', 'decoder.23.ln3.hook_scale', 'decoder.23.ln3.hook_normalized', 'decoder.23.mlp.hook_pre', 'decoder.23.mlp.hook_post', 'decoder.23.hook_mlp_out', 'decoder.23.hook_resid_post', 'decoder_final_ln.hook_scale', 'decoder_final_ln.hook_normalized'])\n"
     ]
    }
   ],
   "source": [
    "print(out[1].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model predicted document:@DOC_ID_52288@, and right answer is:['67981', '52288']\n"
     ]
    }
   ],
   "source": [
    "idx = 4\n",
    "input_question = next(iter(data_loader))[0][idx]\n",
    "input_relevant_docs = next(iter(data_loader))[1][idx]\n",
    "logits = model(input_question)\n",
    "res = tokenizer.decode(torch.argmax(logits, dim=-1).squeeze(-1))\n",
    "print(f'Model predicted document:{res}, and right answer is:{input_relevant_docs}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T18:14:30.174407800Z",
     "start_time": "2025-06-23T17:45:12.074253800Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ze0zB9tkGAvn",
    "outputId": "b3901772-12d0-4d24-a4c8-cfb74134228d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n"
     ]
    }
   ],
   "source": [
    "correct_queries = []\n",
    "decoder_input = torch.tensor([[0]])\n",
    "for input_texts, target_texts, ids in data_loader:\n",
    "  input_tokens = tokenizer(input_texts, return_tensors='pt', padding=True)['input_ids']\n",
    "  logits = model.forward(input_tokens, decoder_input)\n",
    "  # print(torch.argmax(logits, dim=-1).squeeze(-1))\n",
    "  res = tokenizer.decode(torch.argmax(logits, dim=-1).squeeze(-1))\n",
    "  doc_ids = [s for s in res.replace('@','_').split(sep='_') if s.isdigit()]\n",
    "  correct_queries += [(id, query, predicted, truth) for id,query,predicted,truth in zip(ids, input_texts, doc_ids, target_texts) if predicted in truth]\n",
    "\n",
    "\n",
    "# for entry in training_data:\n",
    "#   id, query, relevant_docs = entry\n",
    "#   input_tokens = tokenizer(query, return_tensors='pt')['input_ids']\n",
    "#   decoder_input = torch.tensor([[0]])\n",
    "\n",
    "#   logits, cache = model.run_with_cache(input_tokens, decoder_input, remove_batch_dim=True)\n",
    "#   res = tokenizer.decode(torch.argmax(logits, dim=-1)[0][0])\n",
    "#   if res in relevant_docs:\n",
    "#     correct_queries.append(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T19:23:47.553301500Z",
     "start_time": "2025-06-23T19:07:36.208957500Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "nOnWE-FFsA4d",
    "outputId": "77c4dc1b-19be-4d15-b7fc-4a19cd290d14"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n",
      "WARNING:root:No attention mask provided. Assuming all tokens should be attended to.\n"
     ]
    }
   ],
   "source": [
    "ids = [entry[0] for entry in correct_queries]\n",
    "queries = [entry[1] for entry in correct_queries]\n",
    "truths = [entry[3] for entry in correct_queries]\n",
    "\n",
    "correct_dataset = QuestionsDataset(queries, truths, ids)\n",
    "dl = DataLoader(correct_dataset, batch_size=16, shuffle = False, collate_fn=dataset.collate_fn)\n",
    "\n",
    "decoder_input = torch.tensor([[0]])\n",
    "cached_mlps = {}\n",
    "for input_texts, target_texts, ids in dl:\n",
    "  input_tokens = tokenizer(input_texts, return_tensors='pt', padding=True)['input_ids']\n",
    "  _, cache = model.run_with_cache(input_tokens, decoder_input)\n",
    "  for layer in range(18, 24):\n",
    "      cached_mlps[f\"layer_{layer}\"] = torch.cat((cached_mlps.setdefault(f\"layer_{layer}\", torch.Tensor()), cache[f\"decoder.{layer}.mlp.hook_post\"]), dim = 0)\n",
    "\n",
    "torch.save(cached_mlps,\"cached_mlp_from_correct_queries.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-24T17:31:41.290295500Z",
     "start_time": "2025-06-24T17:31:41.273303100Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'QTest0': '21871',\n",
       " 'QTest2': '70062',\n",
       " 'QTest4': '52288',\n",
       " 'QTest5': '38019',\n",
       " 'QTest6': '8466',\n",
       " 'QTest7': '73330',\n",
       " 'QTest8': '9181',\n",
       " 'QTest9': '70053',\n",
       " 'QTest10': '52421',\n",
       " 'QTest11': '13600',\n",
       " 'QTest12': '60198',\n",
       " 'QTest14': '46780',\n",
       " 'QTest15': '50855',\n",
       " 'QTest17': '68189',\n",
       " 'QTest18': '11358',\n",
       " 'QTest20': '42034',\n",
       " 'QTest21': '66108',\n",
       " 'QTest22': '4579',\n",
       " 'QTest23': '38155',\n",
       " 'QTest24': '59353',\n",
       " 'QTest25': '36612',\n",
       " 'QTest27': '52886',\n",
       " 'QTest28': '52588',\n",
       " 'QTest29': '34722',\n",
       " 'QTest30': '23458',\n",
       " 'QTest31': '7944',\n",
       " 'QTest32': '23003',\n",
       " 'QTest34': '11482',\n",
       " 'QTest35': '56292',\n",
       " 'QTest38': '45635',\n",
       " 'QTest39': '29055',\n",
       " 'QTest41': '57923',\n",
       " 'QTest42': '59708',\n",
       " 'QTest45': '39093',\n",
       " 'QTest46': '45883',\n",
       " 'QTest47': '59708',\n",
       " 'QTest48': '51159',\n",
       " 'QTest49': '21815',\n",
       " 'QTest50': '56658',\n",
       " 'QTest51': '55325',\n",
       " 'QTest52': '3839',\n",
       " 'QTest55': '4181',\n",
       " 'QTest56': '29055',\n",
       " 'QTest57': '34767',\n",
       " 'QTest58': '67193',\n",
       " 'QTest59': '59878',\n",
       " 'QTest60': '18097',\n",
       " 'QTest61': '68811',\n",
       " 'QTest62': '54638',\n",
       " 'QTest64': '11569',\n",
       " 'QTest66': '7835',\n",
       " 'QTest69': '25156',\n",
       " 'QTest70': '62475',\n",
       " 'QTest73': '39953',\n",
       " 'QTest76': '55838',\n",
       " 'QTest77': '51159',\n",
       " 'QTest78': '12940',\n",
       " 'QTest79': '26439',\n",
       " 'QTest80': '22590',\n",
       " 'QTest82': '23003',\n",
       " 'QTest83': '55580',\n",
       " 'QTest85': '29694',\n",
       " 'QTest87': '36924',\n",
       " 'QTest88': '4811',\n",
       " 'QTest90': '13055',\n",
       " 'QTest94': '19074',\n",
       " 'QTest95': '18909',\n",
       " 'QTest97': '9768',\n",
       " 'QTest98': '46835',\n",
       " 'QTest103': '29240',\n",
       " 'QTest104': '60667',\n",
       " 'QTest105': '36924',\n",
       " 'QTest108': '59394',\n",
       " 'QTest109': '29055',\n",
       " 'QTest110': '5115',\n",
       " 'QTest113': '13545',\n",
       " 'QTest115': '51257',\n",
       " 'QTest116': '60087',\n",
       " 'QTest117': '68438',\n",
       " 'QTest118': '57454',\n",
       " 'QTest119': '65166',\n",
       " 'QTest122': '11282',\n",
       " 'QTest124': '25361',\n",
       " 'QTest126': '7968',\n",
       " 'QTest127': '596',\n",
       " 'QTest128': '49862',\n",
       " 'QTest129': '64921',\n",
       " 'QTest130': '26750',\n",
       " 'QTest131': '3920',\n",
       " 'QTest132': '50579',\n",
       " 'QTest138': '48636',\n",
       " 'QTest142': '59472',\n",
       " 'QTest143': '27132',\n",
       " 'QTest144': '55666',\n",
       " 'QTest146': '2206',\n",
       " 'QTest147': '38510',\n",
       " 'QTest148': '26122',\n",
       " 'QTest149': '24737',\n",
       " 'QTest150': '32673',\n",
       " 'QTest151': '2182',\n",
       " 'QTest152': '59215',\n",
       " 'QTest155': '24271',\n",
       " 'QTest156': '3839',\n",
       " 'QTest157': '53078',\n",
       " 'QTest159': '22598',\n",
       " 'QTest160': '39953',\n",
       " 'QTest161': '21059',\n",
       " 'QTest162': '72650',\n",
       " 'QTest163': '34445',\n",
       " 'QTest165': '16178',\n",
       " 'QTest166': '23135',\n",
       " 'QTest168': '71767',\n",
       " 'QTest169': '25199',\n",
       " 'QTest172': '4246',\n",
       " 'QTest173': '30506',\n",
       " 'QTest174': '3233',\n",
       " 'QTest175': '14554',\n",
       " 'QTest176': '48158',\n",
       " 'QTest178': '69056',\n",
       " 'QTest179': '71591',\n",
       " 'QTest183': '38505',\n",
       " 'QTest184': '17296',\n",
       " 'QTest185': '25107',\n",
       " 'QTest186': '28540',\n",
       " 'QTest187': '41135',\n",
       " 'QTest188': '62223',\n",
       " 'QTest189': '10380',\n",
       " 'QTest190': '49298',\n",
       " 'QTest197': '46412',\n",
       " 'QTest198': '11086',\n",
       " 'QTest199': '9615',\n",
       " 'QTest201': '45706',\n",
       " 'QTest208': '44277',\n",
       " 'QTest209': '69384',\n",
       " 'QTest213': '38044',\n",
       " 'QTest214': '53537',\n",
       " 'QTest216': '22071',\n",
       " 'QTest219': '19965',\n",
       " 'QTest221': '46476',\n",
       " 'QTest223': '39913',\n",
       " 'QTest224': '69024',\n",
       " 'QTest225': '16797',\n",
       " 'QTest226': '63901',\n",
       " 'QTest227': '6939',\n",
       " 'QTest229': '39160',\n",
       " 'QTest230': '42761',\n",
       " 'QTest232': '41888',\n",
       " 'QTest233': '27994',\n",
       " 'QTest234': '4680',\n",
       " 'QTest235': '56658',\n",
       " 'QTest236': '59118',\n",
       " 'QTest237': '25091',\n",
       " 'QTest239': '17838',\n",
       " 'QTest240': '63147',\n",
       " 'QTest241': '9701',\n",
       " 'QTest245': '13055',\n",
       " 'QTest246': '11810',\n",
       " 'QTest247': '30079',\n",
       " 'QTest248': '67427',\n",
       " 'QTest249': '37406',\n",
       " 'QTest251': '57939',\n",
       " 'QTest252': '13459',\n",
       " 'QTest253': '42702',\n",
       " 'QTest256': '28908',\n",
       " 'QTest257': '64053',\n",
       " 'QTest259': '43731',\n",
       " 'QTest262': '49298',\n",
       " 'QTest263': '65274',\n",
       " 'QTest265': '9479',\n",
       " 'QTest266': '64788',\n",
       " 'QTest268': '53935',\n",
       " 'QTest270': '41840',\n",
       " 'QTest271': '71418',\n",
       " 'QTest272': '55129',\n",
       " 'QTest275': '37786',\n",
       " 'QTest279': '17799',\n",
       " 'QTest280': '52195',\n",
       " 'QTest281': '45656',\n",
       " 'QTest282': '62124',\n",
       " 'QTest284': '1916',\n",
       " 'QTest285': '39709',\n",
       " 'QTest286': '44633',\n",
       " 'QTest289': '68679',\n",
       " 'QTest292': '37665',\n",
       " 'QTest293': '63868',\n",
       " 'QTest295': '64788',\n",
       " 'QTest298': '4456',\n",
       " 'QTest299': '13860',\n",
       " 'QTest300': '39208',\n",
       " 'QTest301': '5007',\n",
       " 'QTest302': '59339',\n",
       " 'QTest303': '71095',\n",
       " 'QTest304': '14178',\n",
       " 'QTest305': '73024',\n",
       " 'QTest306': '559',\n",
       " 'QTest307': '20454',\n",
       " 'QTest308': '13055',\n",
       " 'QTest310': '14209',\n",
       " 'QTest311': '67503',\n",
       " 'QTest314': '73713',\n",
       " 'QTest315': '14224',\n",
       " 'QTest316': '4588',\n",
       " 'QTest318': '68926',\n",
       " 'QTest319': '46220',\n",
       " 'QTest322': '1734',\n",
       " 'QTest325': '24259',\n",
       " 'QTest326': '59066',\n",
       " 'QTest330': '33401',\n",
       " 'QTest331': '874',\n",
       " 'QTest332': '49240',\n",
       " 'QTest335': '69231',\n",
       " 'QTest336': '71708',\n",
       " 'QTest337': '57868',\n",
       " 'QTest338': '6356',\n",
       " 'QTest339': '45485',\n",
       " 'QTest344': '55438',\n",
       " 'QTest347': '6265',\n",
       " 'QTest348': '38305',\n",
       " 'QTest349': '69217',\n",
       " 'QTest350': '66098',\n",
       " 'QTest351': '67568',\n",
       " 'QTest352': '27609',\n",
       " 'QTest353': '29487',\n",
       " 'QTest354': '6372',\n",
       " 'QTest356': '41165',\n",
       " 'QTest358': '1258',\n",
       " 'QTest359': '26326',\n",
       " 'QTest360': '51213',\n",
       " 'QTest361': '45181',\n",
       " 'QTest362': '41941',\n",
       " 'QTest365': '53518',\n",
       " 'QTest366': '66611',\n",
       " 'QTest367': '30036',\n",
       " 'QTest368': '19715',\n",
       " 'QTest370': '3617',\n",
       " 'QTest371': '48426',\n",
       " 'QTest372': '6405',\n",
       " 'QTest373': '67465',\n",
       " 'QTest374': '18581',\n",
       " 'QTest375': '38986',\n",
       " 'QTest376': '1300',\n",
       " 'QTest379': '33044',\n",
       " 'QTest380': '12397',\n",
       " 'QTest381': '40405',\n",
       " 'QTest382': '17520',\n",
       " 'QTest384': '3146',\n",
       " 'QTest385': '36660',\n",
       " 'QTest386': '42477',\n",
       " 'QTest387': '5654',\n",
       " 'QTest388': '45181',\n",
       " 'QTest390': '7919',\n",
       " 'QTest392': '13623',\n",
       " 'QTest394': '22237',\n",
       " 'QTest395': '66703',\n",
       " 'QTest398': '36098',\n",
       " 'QTest399': '62764',\n",
       " 'QTest400': '46476',\n",
       " 'QTest401': '13613',\n",
       " 'QTest402': '16149',\n",
       " 'QTest403': '16315',\n",
       " 'QTest405': '16315',\n",
       " 'QTest406': '63002',\n",
       " 'QTest407': '31436',\n",
       " 'QTest409': '1782',\n",
       " 'QTest410': '53721',\n",
       " 'QTest411': '35875',\n",
       " 'QTest412': '50315',\n",
       " 'QTest413': '26823',\n",
       " 'QTest414': '71660',\n",
       " 'QTest415': '37589',\n",
       " 'QTest417': '39224',\n",
       " 'QTest418': '29828',\n",
       " 'QTest420': '6952',\n",
       " 'QTest421': '49716',\n",
       " 'QTest423': '8414',\n",
       " 'QTest425': '57127',\n",
       " 'QTest426': '1303',\n",
       " 'QTest428': '50595',\n",
       " 'QTest429': '71743',\n",
       " 'QTest430': '35875',\n",
       " 'QTest433': '34878',\n",
       " 'QTest434': '61348',\n",
       " 'QTest435': '35843',\n",
       " 'QTest436': '6597',\n",
       " 'QTest437': '50412',\n",
       " 'QTest438': '34082',\n",
       " 'QTest439': '2652',\n",
       " 'QTest440': '53716',\n",
       " 'QTest441': '49060',\n",
       " 'QTest443': '23163',\n",
       " 'QTest446': '60098',\n",
       " 'QTest448': '35918',\n",
       " 'QTest449': '14923',\n",
       " 'QTest450': '49367',\n",
       " 'QTest451': '9030',\n",
       " 'QTest453': '5353',\n",
       " 'QTest455': '60101',\n",
       " 'QTest458': '35260',\n",
       " 'QTest459': '59708',\n",
       " 'QTest462': '67696',\n",
       " 'QTest463': '8026',\n",
       " 'QTest464': '28212',\n",
       " 'QTest465': '53933',\n",
       " 'QTest466': '10596',\n",
       " 'QTest468': '61073',\n",
       " 'QTest469': '7175',\n",
       " 'QTest471': '60278',\n",
       " 'QTest472': '69122',\n",
       " 'QTest473': '49007',\n",
       " 'QTest474': '11822',\n",
       " 'QTest475': '64853',\n",
       " 'QTest476': '52933',\n",
       " 'QTest477': '4367',\n",
       " 'QTest478': '5758',\n",
       " 'QTest479': '15709',\n",
       " 'QTest480': '46421',\n",
       " 'QTest482': '59903',\n",
       " 'QTest483': '37776',\n",
       " 'QTest484': '2836',\n",
       " 'QTest485': '9478',\n",
       " 'QTest486': '7154',\n",
       " 'QTest487': '31637',\n",
       " 'QTest488': '32719',\n",
       " 'QTest489': '45354',\n",
       " 'QTest490': '71386',\n",
       " 'QTest491': '61603',\n",
       " 'QTest493': '67806',\n",
       " 'QTest494': '6479',\n",
       " 'QTest495': '928',\n",
       " 'QTest497': '39508',\n",
       " 'QTest498': '70398',\n",
       " 'QTest499': '35861',\n",
       " 'QTest500': '49298',\n",
       " 'QTest503': '60581',\n",
       " 'QTest505': '25886',\n",
       " 'QTest509': '36917',\n",
       " 'QTest513': '67427',\n",
       " 'QTest516': '54317',\n",
       " 'QTest517': '26550',\n",
       " 'QTest518': '11991',\n",
       " 'QTest520': '55883',\n",
       " 'QTest521': '58628',\n",
       " 'QTest522': '38452',\n",
       " 'QTest523': '6952',\n",
       " 'QTest524': '61040',\n",
       " 'QTest525': '5046',\n",
       " 'QTest526': '2182',\n",
       " 'QTest528': '17880',\n",
       " 'QTest530': '13706',\n",
       " 'QTest531': '53716',\n",
       " 'QTest532': '61753',\n",
       " 'QTest533': '8881',\n",
       " 'QTest535': '567',\n",
       " 'QTest536': '53894',\n",
       " 'QTest537': '65740',\n",
       " 'QTest538': '2182',\n",
       " 'QTest539': '17551',\n",
       " 'QTest540': '14054',\n",
       " 'QTest542': '52468',\n",
       " 'QTest543': '17892',\n",
       " 'QTest544': '69056',\n",
       " 'QTest545': '59362',\n",
       " 'QTest546': '24442',\n",
       " 'QTest547': '25084',\n",
       " 'QTest548': '59708',\n",
       " 'QTest550': '67853',\n",
       " 'QTest552': '50760',\n",
       " 'QTest555': '38683',\n",
       " 'QTest557': '10526',\n",
       " 'QTest559': '17593',\n",
       " 'QTest563': '53670',\n",
       " 'QTest564': '57375',\n",
       " 'QTest565': '18081',\n",
       " 'QTest566': '54564',\n",
       " 'QTest567': '17880',\n",
       " 'QTest571': '6780',\n",
       " 'QTest572': '54233',\n",
       " 'QTest575': '16516',\n",
       " 'QTest576': '42017',\n",
       " 'QTest577': '27077',\n",
       " 'QTest578': '41257',\n",
       " 'QTest579': '34399',\n",
       " 'QTest581': '49318',\n",
       " 'QTest582': '4844',\n",
       " 'QTest585': '8752',\n",
       " 'QTest587': '28216',\n",
       " 'QTest589': '29041',\n",
       " 'QTest591': '13592',\n",
       " 'QTest592': '68679',\n",
       " 'QTest593': '16957',\n",
       " 'QTest594': '51307',\n",
       " 'QTest595': '10900',\n",
       " 'QTest596': '64779',\n",
       " 'QTest597': '7897',\n",
       " 'QTest599': '49407',\n",
       " 'QTest600': '33142',\n",
       " 'QTest601': '13600',\n",
       " 'QTest603': '3365',\n",
       " 'QTest607': '34275',\n",
       " 'QTest608': '14546',\n",
       " 'QTest609': '52980',\n",
       " 'QTest610': '9701',\n",
       " 'QTest611': '10108',\n",
       " 'QTest612': '57888',\n",
       " 'QTest613': '21031',\n",
       " 'QTest614': '11819',\n",
       " 'QTest618': '7892',\n",
       " 'QTest620': '40789',\n",
       " 'QTest623': '67238',\n",
       " 'QTest624': '2534',\n",
       " 'QTest625': '28436',\n",
       " 'QTest630': '44017',\n",
       " 'QTest633': '67427',\n",
       " 'QTest635': '14528',\n",
       " 'QTest637': '2947',\n",
       " 'QTest638': '1760',\n",
       " 'QTest639': '27198',\n",
       " 'QTest640': '68116',\n",
       " 'QTest641': '68317',\n",
       " 'QTest643': '19532',\n",
       " 'QTest644': '29290',\n",
       " 'QTest645': '22825',\n",
       " 'QTest646': '26282',\n",
       " 'QTest648': '51815',\n",
       " 'QTest649': '70343',\n",
       " 'QTest651': '9325',\n",
       " 'QTest654': '36553',\n",
       " 'QTest656': '15984',\n",
       " 'QTest660': '16466',\n",
       " 'QTest661': '47511',\n",
       " 'QTest662': '36242',\n",
       " 'QTest663': '9615',\n",
       " 'QTest664': '6405',\n",
       " 'QTest665': '28327',\n",
       " 'QTest666': '22908',\n",
       " 'QTest667': '55539',\n",
       " 'QTest668': '7968',\n",
       " 'QTest670': '69056',\n",
       " 'QTest671': '63324',\n",
       " 'QTest673': '61270',\n",
       " 'QTest675': '16306',\n",
       " 'QTest676': '55688',\n",
       " 'QTest679': '24931',\n",
       " 'QTest680': '13719',\n",
       " 'QTest682': '38080',\n",
       " 'QTest684': '2171',\n",
       " 'QTest685': '70083',\n",
       " 'QTest686': '2182',\n",
       " 'QTest688': '63779',\n",
       " 'QTest690': '71386',\n",
       " 'QTest693': '51299',\n",
       " 'QTest695': '40314',\n",
       " 'QTest696': '11180',\n",
       " 'QTest697': '49298',\n",
       " 'QTest699': '52173',\n",
       " 'QTest701': '22055',\n",
       " 'QTest703': '36553',\n",
       " 'QTest707': '55723',\n",
       " 'QTest708': '67350',\n",
       " 'QTest709': '49731',\n",
       " 'QTest710': '42335',\n",
       " 'QTest711': '73347',\n",
       " 'QTest712': '65676',\n",
       " 'QTest713': '59255',\n",
       " 'QTest716': '13335',\n",
       " 'QTest718': '29599',\n",
       " 'QTest719': '67852',\n",
       " 'QTest720': '12673',\n",
       " 'QTest721': '35768',\n",
       " 'QTest722': '5358',\n",
       " 'QTest723': '6285',\n",
       " 'QTest725': '63220',\n",
       " 'QTest727': '62179',\n",
       " 'QTest731': '39209',\n",
       " 'QTest732': '33451',\n",
       " 'QTest734': '71386',\n",
       " 'QTest735': '35103',\n",
       " 'QTest736': '10034',\n",
       " 'QTest737': '3727',\n",
       " 'QTest738': '46280',\n",
       " 'QTest739': '53696',\n",
       " 'QTest741': '4936',\n",
       " 'QTest742': '68679',\n",
       " 'QTest743': '68679',\n",
       " 'QTest744': '43030',\n",
       " 'QTest747': '24001',\n",
       " 'QTest748': '17597',\n",
       " 'QTest749': '10377',\n",
       " 'QTest751': '66636',\n",
       " 'QTest752': '16315',\n",
       " 'QTest753': '22661',\n",
       " 'QTest756': '49890',\n",
       " 'QTest759': '42778',\n",
       " 'QTest760': '42442',\n",
       " 'QTest761': '73002',\n",
       " 'QTest762': '6078',\n",
       " 'QTest764': '46175',\n",
       " 'QTest767': '48941',\n",
       " 'QTest768': '53461',\n",
       " 'QTest769': '73628',\n",
       " 'QTest771': '46799',\n",
       " 'QTest772': '4887',\n",
       " 'QTest773': '49994',\n",
       " 'QTest775': '3704',\n",
       " 'QTest779': '10872',\n",
       " 'QTest780': '4172',\n",
       " 'QTest781': '38319',\n",
       " 'QTest783': '71386',\n",
       " 'QTest786': '45437',\n",
       " 'QTest788': '67427',\n",
       " 'QTest789': '2182',\n",
       " 'QTest790': '26530',\n",
       " 'QTest792': '26530',\n",
       " 'QTest794': '24086',\n",
       " 'QTest795': '48864',\n",
       " 'QTest796': '6072',\n",
       " 'QTest797': '61042',\n",
       " 'QTest798': '45485',\n",
       " 'QTest800': '65795',\n",
       " 'QTest801': '69835',\n",
       " 'QTest806': '37613',\n",
       " 'QTest807': '26948',\n",
       " 'QTest809': '49659',\n",
       " 'QTest812': '71481',\n",
       " 'QTest813': '66459',\n",
       " 'QTest814': '52777',\n",
       " 'QTest815': '67427',\n",
       " 'QTest816': '64878',\n",
       " 'QTest818': '37589',\n",
       " 'QTest819': '69231',\n",
       " 'QTest820': '18686',\n",
       " 'QTest821': '20310',\n",
       " 'QTest822': '50826',\n",
       " 'QTest823': '12731',\n",
       " 'QTest825': '65795',\n",
       " 'QTest827': '5762',\n",
       " 'QTest828': '23511',\n",
       " 'QTest829': '2901',\n",
       " 'QTest830': '51387',\n",
       " 'QTest831': '10205',\n",
       " 'QTest832': '7738',\n",
       " 'QTest833': '2602',\n",
       " 'QTest834': '66624',\n",
       " 'QTest836': '19318',\n",
       " 'QTest837': '4389',\n",
       " 'QTest839': '4883',\n",
       " 'QTest840': '46092',\n",
       " 'QTest841': '14698',\n",
       " 'QTest842': '17025',\n",
       " 'QTest846': '29022',\n",
       " 'QTest847': '70331',\n",
       " 'QTest848': '38323',\n",
       " 'QTest849': '22375',\n",
       " 'QTest851': '23278',\n",
       " 'QTest852': '33283',\n",
       " 'QTest854': '28692',\n",
       " 'QTest855': '51372',\n",
       " 'QTest858': '59708',\n",
       " 'QTest859': '62247',\n",
       " 'QTest860': '33628',\n",
       " 'QTest861': '51010',\n",
       " 'QTest862': '68111',\n",
       " 'QTest863': '3685',\n",
       " 'QTest864': '32289',\n",
       " 'QTest865': '71441',\n",
       " 'QTest866': '11819',\n",
       " 'QTest868': '57812',\n",
       " 'QTest869': '4001',\n",
       " 'QTest870': '44030',\n",
       " 'QTest872': '15944',\n",
       " 'QTest873': '60998',\n",
       " 'QTest874': '32900',\n",
       " 'QTest875': '26530',\n",
       " 'QTest876': '58205',\n",
       " 'QTest877': '12105',\n",
       " 'QTest878': '60184',\n",
       " 'QTest879': '53894',\n",
       " 'QTest880': '17143',\n",
       " 'QTest881': '46769',\n",
       " 'QTest882': '57660',\n",
       " 'QTest883': '64433',\n",
       " 'QTest884': '42477',\n",
       " 'QTest885': '73868',\n",
       " 'QTest886': '40053',\n",
       " 'QTest888': '44325',\n",
       " 'QTest889': '42730',\n",
       " 'QTest891': '69817',\n",
       " 'QTest892': '44539',\n",
       " 'QTest895': '32476',\n",
       " 'QTest896': '73958',\n",
       " 'QTest897': '475',\n",
       " 'QTest898': '20168',\n",
       " 'QTest900': '61844',\n",
       " 'QTest901': '5952',\n",
       " 'QTest903': '1258',\n",
       " 'QTest904': '1258',\n",
       " 'QTest906': '1258',\n",
       " 'QTest907': '1258',\n",
       " 'QTest908': '61906',\n",
       " 'QTest909': '36098',\n",
       " 'QTest910': '69764',\n",
       " 'QTest911': '5010',\n",
       " 'QTest914': '3810',\n",
       " 'QTest915': '18844',\n",
       " 'QTest916': '48443',\n",
       " 'QTest917': '50491',\n",
       " 'QTest918': '69056',\n",
       " 'QTest920': '67015',\n",
       " 'QTest922': '1688',\n",
       " 'QTest923': '769',\n",
       " 'QTest925': '2182',\n",
       " 'QTest926': '70663',\n",
       " 'QTest928': '60098',\n",
       " 'QTest929': '47298',\n",
       " 'QTest930': '34878',\n",
       " 'QTest933': '48955',\n",
       " 'QTest934': '35151',\n",
       " 'QTest939': '67925',\n",
       " 'QTest940': '33262',\n",
       " 'QTest942': '57888',\n",
       " 'QTest943': '57888',\n",
       " 'QTest945': '72145',\n",
       " 'QTest947': '5462',\n",
       " 'QTest948': '10722',\n",
       " 'QTest950': '9493',\n",
       " 'QTest951': '14237',\n",
       " 'QTest952': '29351',\n",
       " 'QTest954': '38872',\n",
       " 'QTest956': '67238',\n",
       " 'QTest957': '7068',\n",
       " 'QTest958': '49885',\n",
       " 'QTest960': '38314',\n",
       " 'QTest961': '44170',\n",
       " 'QTest962': '35625',\n",
       " 'QTest963': '62475',\n",
       " 'QTest965': '21600',\n",
       " 'QTest966': '47836',\n",
       " 'QTest968': '2347',\n",
       " 'QTest971': '9478',\n",
       " 'QTest973': '61760',\n",
       " 'QTest974': '68679',\n",
       " 'QTest978': '30957',\n",
       " 'QTest979': '62858',\n",
       " 'QTest980': '29152',\n",
       " 'QTest981': '61543',\n",
       " 'QTest982': '59468',\n",
       " 'QTest987': '58607',\n",
       " 'QTest988': '57375',\n",
       " 'QTest990': '51946',\n",
       " 'QTest992': '71526',\n",
       " 'QTest993': '21265',\n",
       " 'QTest994': '50644',\n",
       " 'QTest995': '34747',\n",
       " 'QTest996': '10022',\n",
       " 'QTest997': '14361',\n",
       " 'QTest999': '46214',\n",
       " 'QTest1001': '36117',\n",
       " 'QTest1002': '69835',\n",
       " 'QTest1003': '8382',\n",
       " 'QTest1004': '46479',\n",
       " 'QTest1005': '47570',\n",
       " 'QTest1006': '69292',\n",
       " 'QTest1007': '44551',\n",
       " 'QTest1008': '28134',\n",
       " 'QTest1009': '9190',\n",
       " 'QTest1012': '70343',\n",
       " 'QTest1013': '42449',\n",
       " 'QTest1014': '36242',\n",
       " 'QTest1017': '52804',\n",
       " 'QTest1018': '53554',\n",
       " 'QTest1019': '57645',\n",
       " 'QTest1020': '57821',\n",
       " 'QTest1023': '71135',\n",
       " 'QTest1024': '61291',\n",
       " 'QTest1028': '13304',\n",
       " 'QTest1029': '49367',\n",
       " 'QTest1030': '55778',\n",
       " 'QTest1035': '29802',\n",
       " 'QTest1036': '69384',\n",
       " 'QTest1037': '34736',\n",
       " 'QTest1038': '55819',\n",
       " 'QTest1041': '32318',\n",
       " 'QTest1042': '24355',\n",
       " 'QTest1043': '65878',\n",
       " 'QTest1044': '55008',\n",
       " 'QTest1045': '42435',\n",
       " 'QTest1046': '8631',\n",
       " 'QTest1048': '26530',\n",
       " 'QTest1049': '51618',\n",
       " 'QTest1050': '57866',\n",
       " 'QTest1051': '72896',\n",
       " 'QTest1052': '22902',\n",
       " 'QTest1055': '51096',\n",
       " 'QTest1060': '42618',\n",
       " 'QTest1062': '50502',\n",
       " 'QTest1063': '65816',\n",
       " 'QTest1067': '72085',\n",
       " 'QTest1068': '26530',\n",
       " 'QTest1069': '17235',\n",
       " 'QTest1070': '58069',\n",
       " 'QTest1072': '65456',\n",
       " 'QTest1073': '43717',\n",
       " 'QTest1074': '16878',\n",
       " 'QTest1076': '6406',\n",
       " 'QTest1080': '30082',\n",
       " 'QTest1083': '49298',\n",
       " 'QTest1085': '68298',\n",
       " 'QTest1086': '38505',\n",
       " 'QTest1088': '39709',\n",
       " 'QTest1089': '23116',\n",
       " 'QTest1091': '51772',\n",
       " 'QTest1093': '12347',\n",
       " 'QTest1096': '12623',\n",
       " 'QTest1097': '27782',\n",
       " 'QTest1100': '71386',\n",
       " 'QTest1101': '7782',\n",
       " 'QTest1102': '71386',\n",
       " 'QTest1103': '47181',\n",
       " 'QTest1106': '2398',\n",
       " 'QTest1107': '73809',\n",
       " 'QTest1109': '23594',\n",
       " 'QTest1112': '56920',\n",
       " 'QTest1113': '24457',\n",
       " 'QTest1114': '71386',\n",
       " 'QTest1116': '2567',\n",
       " 'QTest1117': '66455',\n",
       " 'QTest1118': '65490',\n",
       " 'QTest1120': '69516',\n",
       " 'QTest1121': '16856',\n",
       " 'QTest1122': '60223',\n",
       " 'QTest1123': '1622',\n",
       " 'QTest1124': '57812',\n",
       " 'QTest1129': '62119',\n",
       " 'QTest1130': '6980',\n",
       " 'QTest1132': '47031',\n",
       " 'QTest1135': '50116',\n",
       " 'QTest1136': '66071',\n",
       " 'QTest1137': '13389',\n",
       " 'QTest1138': '63141',\n",
       " 'QTest1139': '26530',\n",
       " 'QTest1140': '34275',\n",
       " 'QTest1143': '1331',\n",
       " 'QTest1144': '55206',\n",
       " 'QTest1145': '17691',\n",
       " 'QTest1146': '67351',\n",
       " 'QTest1148': '64142',\n",
       " 'QTest1149': '56097',\n",
       " 'QTest1151': '38989',\n",
       " 'QTest1152': '52577',\n",
       " 'QTest1153': '22666',\n",
       " 'QTest1154': '24086',\n",
       " 'QTest1155': '2628',\n",
       " 'QTest1158': '35060',\n",
       " 'QTest1159': '10995',\n",
       " 'QTest1162': '72601',\n",
       " 'QTest1164': '23537',\n",
       " 'QTest1165': '54653',\n",
       " 'QTest1166': '51862',\n",
       " 'QTest1171': '46390',\n",
       " 'QTest1172': '14342',\n",
       " 'QTest1173': '29055',\n",
       " 'QTest1174': '48365',\n",
       " 'QTest1176': '49298',\n",
       " 'QTest1178': '73660',\n",
       " 'QTest1180': '50086',\n",
       " 'QTest1181': '49751',\n",
       " 'QTest1182': '46274',\n",
       " 'QTest1183': '44575',\n",
       " 'QTest1187': '18350',\n",
       " 'QTest1188': '48038',\n",
       " 'QTest1191': '70058',\n",
       " 'QTest1195': '64616',\n",
       " 'QTest1196': '32869',\n",
       " 'QTest1198': '36098',\n",
       " 'QTest1199': '60611',\n",
       " 'QTest1201': '37076',\n",
       " 'QTest1203': '61796',\n",
       " 'QTest1204': '71386',\n",
       " 'QTest1205': '73139',\n",
       " 'QTest1207': '10378',\n",
       " 'QTest1208': '30784',\n",
       " 'QTest1210': '17186',\n",
       " 'QTest1212': '68739',\n",
       " 'QTest1216': '11704',\n",
       " 'QTest1223': '68754',\n",
       " 'QTest1225': '71767',\n",
       " 'QTest1228': '25657',\n",
       " 'QTest1231': '2182',\n",
       " 'QTest1232': '15335',\n",
       " 'QTest1233': '54854',\n",
       " 'QTest1234': '50431',\n",
       " 'QTest1236': '32289',\n",
       " 'QTest1241': '69231',\n",
       " 'QTest1242': '62233',\n",
       " 'QTest1243': '66636',\n",
       " 'QTest1244': '26434',\n",
       " 'QTest1246': '24581',\n",
       " 'QTest1247': '64110',\n",
       " 'QTest1248': '58362',\n",
       " 'QTest1253': '62233',\n",
       " 'QTest1255': '5685',\n",
       " 'QTest1256': '26733',\n",
       " 'QTest1257': '37981',\n",
       " 'QTest1259': '26530',\n",
       " 'QTest1260': '54330',\n",
       " 'QTest1261': '37852',\n",
       " 'QTest1262': '40978',\n",
       " 'QTest1266': '7661',\n",
       " 'QTest1267': '36642',\n",
       " 'QTest1273': '73675',\n",
       " 'QTest1274': '61388',\n",
       " 'QTest1276': '50987',\n",
       " 'QTest1277': '36503',\n",
       " 'QTest1278': '63090',\n",
       " 'QTest1279': '29223',\n",
       " 'QTest1282': '2064',\n",
       " 'QTest1283': '7798',\n",
       " 'QTest1284': '24361',\n",
       " 'QTest1285': '44321',\n",
       " 'QTest1286': '26530',\n",
       " 'QTest1287': '30136',\n",
       " 'QTest1289': '71397',\n",
       " 'QTest1290': '39441',\n",
       " 'QTest1291': '18239',\n",
       " 'QTest1292': '22177',\n",
       " 'QTest1294': '4107',\n",
       " 'QTest1296': '2602',\n",
       " 'QTest1298': '49706',\n",
       " 'QTest1301': '15741',\n",
       " 'QTest1302': '53174',\n",
       " 'QTest1303': '22702',\n",
       " 'QTest1304': '47499',\n",
       " 'QTest1306': '73137',\n",
       " 'QTest1307': '55921',\n",
       " 'QTest1311': '50237',\n",
       " 'QTest1315': '47523',\n",
       " 'QTest1316': '65590',\n",
       " 'QTest1317': '16315',\n",
       " 'QTest1320': '8475',\n",
       " 'QTest1323': '4661',\n",
       " 'QTest1325': '40367',\n",
       " 'QTest1326': '16114',\n",
       " 'QTest1327': '68367',\n",
       " 'QTest1328': '56892',\n",
       " 'QTest1329': '43158',\n",
       " 'QTest1331': '40125',\n",
       " 'QTest1332': '53155',\n",
       " 'QTest1334': '31343',\n",
       " 'QTest1337': '57014',\n",
       " 'QTest1338': '22955',\n",
       " 'QTest1339': '32673',\n",
       " 'QTest1340': '14330',\n",
       " 'QTest1342': '71470',\n",
       " 'QTest1343': '51480',\n",
       " 'QTest1348': '51815',\n",
       " 'QTest1355': '54125',\n",
       " 'QTest1356': '23335',\n",
       " 'QTest1358': '65705',\n",
       " 'QTest1359': '15998',\n",
       " 'QTest1360': '27748',\n",
       " 'QTest1365': '70298',\n",
       " 'QTest1366': '35543',\n",
       " 'QTest1371': '70823',\n",
       " 'QTest1372': '20531',\n",
       " 'QTest1374': '49030',\n",
       " 'QTest1376': '20145',\n",
       " 'QTest1377': '9477',\n",
       " 'QTest1378': '69086',\n",
       " 'QTest1379': '20981',\n",
       " 'QTest1381': '49848',\n",
       " 'QTest1383': '6140',\n",
       " 'QTest1384': '2943',\n",
       " 'QTest1386': '59708',\n",
       " 'QTest1389': '36904',\n",
       " 'QTest1391': '33080',\n",
       " 'QTest1393': '68688',\n",
       " 'QTest1395': '11595',\n",
       " 'QTest1396': '14209',\n",
       " 'QTest1401': '39841',\n",
       " 'QTest1402': '24911',\n",
       " 'QTest1406': '52228',\n",
       " 'QTest1407': '24623',\n",
       " 'QTest1411': '23537',\n",
       " 'QTest1413': '11770',\n",
       " 'QTest1415': '60550',\n",
       " 'QTest1417': '37728',\n",
       " 'QTest1418': '30080',\n",
       " 'QTest1419': '47615',\n",
       " 'QTest1422': '26456',\n",
       " 'QTest1423': '25122',\n",
       " 'QTest1425': '9733',\n",
       " 'QTest1426': '73592',\n",
       " 'QTest1427': '49338',\n",
       " 'QTest1428': '67038',\n",
       " 'QTest1429': '8264',\n",
       " 'QTest1432': '48583',\n",
       " 'QTest1433': '616',\n",
       " 'QTest1434': '35931',\n",
       " 'QTest1436': '68928',\n",
       " 'QTest1438': '37392',\n",
       " 'QTest1439': '33385',\n",
       " 'QTest1440': '9133',\n",
       " 'QTest1442': '35069',\n",
       " 'QTest1443': '59622',\n",
       " 'QTest1446': '11693',\n",
       " 'QTest1447': '54299',\n",
       " 'QTest1448': '61421',\n",
       " 'QTest1450': '9133',\n",
       " 'QTest1451': '65907',\n",
       " 'QTest1452': '64276',\n",
       " 'QTest1454': '47570',\n",
       " 'QTest1455': '10108',\n",
       " 'QTest1456': '69756',\n",
       " 'QTest1457': '22683',\n",
       " 'QTest1458': '46268',\n",
       " 'QTest1459': '65216',\n",
       " 'QTest1461': '58218',\n",
       " 'QTest1462': '70342',\n",
       " 'QTest1463': '69941',\n",
       " 'QTest1464': '6506',\n",
       " 'QTest1465': '63348',\n",
       " 'QTest1466': '34096',\n",
       " 'QTest1468': '41871',\n",
       " 'QTest1469': '30778',\n",
       " 'QTest1470': '26122',\n",
       " 'QTest1471': '69516',\n",
       " 'QTest1472': '49470',\n",
       " 'QTest1473': '31136',\n",
       " 'QTest1479': '3431',\n",
       " 'QTest1481': '34082',\n",
       " 'QTest1484': '6129',\n",
       " 'QTest1485': '45810',\n",
       " 'QTest1486': '50707',\n",
       " 'QTest1488': '51238',\n",
       " 'QTest1489': '58922',\n",
       " 'QTest1490': '11819',\n",
       " 'QTest1491': '54298',\n",
       " 'QTest1492': '30674',\n",
       " 'QTest1493': '15837',\n",
       " 'QTest1496': '8333',\n",
       " 'QTest1497': '10172',\n",
       " 'QTest1501': '57300',\n",
       " 'QTest1502': '73428',\n",
       " 'QTest1504': '64016',\n",
       " 'QTest1505': '16157',\n",
       " 'QTest1507': '43688',\n",
       " 'QTest1508': '17880',\n",
       " 'QTest1509': '53980',\n",
       " 'QTest1511': '64309',\n",
       " 'QTest1513': '20933',\n",
       " 'QTest1514': '39498',\n",
       " 'QTest1515': '56251',\n",
       " 'QTest1516': '11180',\n",
       " 'QTest1517': '28824',\n",
       " 'QTest1520': '42968',\n",
       " 'QTest1521': '63873',\n",
       " 'QTest1522': '44711',\n",
       " 'QTest1523': '68679',\n",
       " 'QTest1524': '70215',\n",
       " 'QTest1525': '39160',\n",
       " 'QTest1526': '56014',\n",
       " 'QTest1527': '43951',\n",
       " 'QTest1528': '70834',\n",
       " 'QTest1529': '47773',\n",
       " 'QTest1531': '59082',\n",
       " 'QTest1533': '64008',\n",
       " 'QTest1534': '29888',\n",
       " 'QTest1537': '47539',\n",
       " 'QTest1538': '11819',\n",
       " 'QTest1540': '33823',\n",
       " 'QTest1541': '9478',\n",
       " 'QTest1542': '66643',\n",
       " 'QTest1545': '57821',\n",
       " 'QTest1548': '36242',\n",
       " 'QTest1549': '53397',\n",
       " 'QTest1550': '33602',\n",
       " 'QTest1551': '64817',\n",
       " 'QTest1553': '68812',\n",
       " 'QTest1555': '38912',\n",
       " 'QTest1559': '73809',\n",
       " 'QTest1561': '14619',\n",
       " 'QTest1562': '34082',\n",
       " 'QTest1563': '24845',\n",
       " 'QTest1567': '64124',\n",
       " 'QTest1568': '33498',\n",
       " 'QTest1570': '27413',\n",
       " 'QTest1572': '59682',\n",
       " 'QTest1574': '14873',\n",
       " 'QTest1576': '21492',\n",
       " 'QTest1577': '51305',\n",
       " 'QTest1579': '35275',\n",
       " 'QTest1580': '17553',\n",
       " 'QTest1581': '51272',\n",
       " 'QTest1583': '66158',\n",
       " 'QTest1584': '42275',\n",
       " 'QTest1585': '51338',\n",
       " 'QTest1586': '69302',\n",
       " 'QTest1592': '37434',\n",
       " 'QTest1594': '1115',\n",
       " 'QTest1595': '67897',\n",
       " ...}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cached = torch.load(\"cached_mlp_from_correct_queries.json\")\n",
    "# cached_with_query_id = {q_id : {key : cached_layer.squeeze(1)[index] for key,cached_layer in cached.items()} for index, q_id in enumerate(ids)} \n",
    "# queries_predicted = {l[0] : l[2] for l in correct_queries}\n",
    "queries_predicted\n",
    "# TODO: what do we need to save for each doc id and query\n",
    "# dict: doc-id -> num-of-valid-queries\n",
    "# doc-id -> (activations, valid-queries)\n",
    "# TODO: make sure all docs were indexed\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T18:14:33.531032400Z",
     "start_time": "2025-06-23T18:14:33.522869100Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mZ7MIWz9tx7C",
    "outputId": "68c2d32a-530b-4e8a-94a9-44140a376a06"
   },
   "outputs": [],
   "source": [
    "query = \"test query\"\n",
    "\n",
    "input_tokens = tokenizer(query, return_tensors='pt')['input_ids']\n",
    "decoder_input = torch.tensor([[0]])\n",
    "\n",
    "logits, cache = model.run_with_cache(input_tokens, decoder_input, remove_batch_dim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-06-23T18:14:33.523879900Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cauO_zdtvAGk",
    "outputId": "2174c101-63d3-4b32-db9f-f08b46c10526"
   },
   "outputs": [],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-06-23T18:14:33.526799Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u-ROJ8xEvGho",
    "outputId": "8a17d7b5-21b3-4142-fa33-5c772e7ab770"
   },
   "outputs": [],
   "source": [
    "# Prediction from the logits\n",
    "torch.argmax(logits, dim=-1), tokenizer.decode(torch.argmax(logits, dim=-1)[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZmLia78cvgbL"
   },
   "source": [
    "## Examining the activations\n",
    "\n",
    "The activations of each component in the transformer are stored in the `cache` object. It's basically a dict from which you choose which component to look at.\n",
    "\n",
    "Here, we print all possible component keys for layer 0 in the decoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T18:14:33.586444800Z",
     "start_time": "2025-06-23T18:14:33.532019900Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q_OcHoLnvOyA",
    "outputId": "d0759b7a-d623-4726-e5f2-4e70205e7e10"
   },
   "outputs": [],
   "source": [
    "for key in cache.keys():\n",
    "  if key.startswith('decoder.0.'):\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "22RgxkfDwm1R"
   },
   "source": [
    "We choose to look at the output of the MLP in layer 19 of the decoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-06-23T18:14:33.533025200Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vfjtRp-pvl2I",
    "outputId": "69c417e6-a16e-4bfb-bd27-7ed671219998"
   },
   "outputs": [],
   "source": [
    "cache['decoder.19.hook_mlp_out'], cache['decoder.19.hook_mlp_out'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vhL0VlzwwYrk"
   },
   "source": [
    "Take a look at where the MLP hooks are computed: https://github.com/TransformerLensOrg/TransformerLens/blob/main/transformer_lens/components/mlps/mlp.py\n",
    "\n",
    "`hook_pre`: Before activation,\n",
    "`hook_post`: After applying activation"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
