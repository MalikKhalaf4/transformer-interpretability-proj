{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Community Detection from Activation Maps\n",
    "\n",
    "This notebook performs **community detection** on the neuron graph derived from activation maps produced in data prep.\n",
    "It groups neurons into communities based on shared activation patterns for downstream theme analysis.\n",
    "\n",
    "**Pipeline**\n",
    "1. Load activation-derived data.\n",
    "2. Construct a graph (NetworkX).\n",
    "3. Run community detection (Leiden).\n",
    "4. Post-process communities (filtering, sizes/coverage).\n",
    "5. Save community assignments and summary statistics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import Dict, Hashable, List, Optional, Tuple, Union\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "from __future__ import annotations\n",
    "import faiss\n",
    "import igraph as ig\n",
    "import leidenalg as la\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load data\n",
    "activations = {}\n",
    "for layer in range(17, 24):\n",
    "    layer_dict = torch.load(f\"../layer_activations_no_test/layer_{layer}.json\")\n",
    "    activations[f\"layer_{layer}\"] = {qid : d[\"post\"] for qid, d in layer_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_layers_to_common_prompts(\n",
    "    layer_to_map: Dict[Hashable, Dict[Hashable, np.ndarray]],\n",
    "    *,\n",
    "    dtype: np.dtype = np.float32,\n",
    ") -> Tuple[List[Hashable], List[Union[np.ndarray, sp.csr_matrix]], List[Hashable], dict]:\n",
    "    \"\"\"\n",
    "    Align per-layer {prompt_id -> vector} dicts to a common prompt set and row order.\n",
    "\n",
    "    Args\n",
    "    ----\n",
    "    layer_to_map: dict[layer] -> {prompt_id: 1D activation vector for that layer}\n",
    "    dtype: output dtype\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    ordered_prompt_ids: list of prompt_ids (row 0..P-1)\n",
    "    X_list: list of matrices, one per layer (aligned rows)\n",
    "    layer_numbers: list of layers in the order processed (sorted by key)\n",
    "    stats: dict with counters (P_all, P_common, removed_per_layer)\n",
    "    \"\"\"\n",
    "    layer_numbers = list(layer_to_map.keys())\n",
    "    layer_numbers = sorted(layer_numbers)\n",
    "\n",
    "    # compute common prompt set (prompts present in ALL layers)\n",
    "    sets = [set(layer_to_map[L].keys()) for L in layer_numbers]\n",
    "    common = set.intersection(*sets)\n",
    "    P_common = len(common)\n",
    "\n",
    "    # choose a global row order for the prompts\n",
    "    ordered_query_ids = sorted(common)\n",
    "\n",
    "    # verify vector lengths and build aligned matrices\n",
    "    removed_per_layer = {}\n",
    "    X_list: List[Union[np.ndarray, sp.csr_matrix]] = []\n",
    "\n",
    "    for layer in layer_numbers:\n",
    "        m = layer_to_map[layer]\n",
    "        # get any vector to infer dimensionality\n",
    "        first_qid = next(iter(m.keys()))\n",
    "        D = int(np.asarray(m[first_qid]).shape[0])        \n",
    "        removed_per_layer[layer] = len(m) - P_common\n",
    "\n",
    "        # Build CSR row-by-row without densifying the whole layer\n",
    "        data: List[float] = []\n",
    "        indices: List[int] = []\n",
    "        indptr: List[int] = [0]\n",
    "        for qid in ordered_query_ids:\n",
    "            v = np.asarray(m[qid], dtype=dtype, order=\"C\")\n",
    "            nz = np.flatnonzero(v) \n",
    "            if nz.size:\n",
    "                data.extend(v[nz].tolist())\n",
    "                indices.extend(nz.astype(np.int32).tolist())\n",
    "            indptr.append(len(data))\n",
    "        X_csr = sp.csr_matrix((np.asarray(data, dtype=dtype),\n",
    "                                np.asarray(indices, dtype=np.int32),\n",
    "                                np.asarray(indptr, dtype=np.int64)),\n",
    "                                shape=(P_common, D), dtype=dtype)\n",
    "        X_list.append(X_csr)\n",
    "        \n",
    "\n",
    "    stats = dict(\n",
    "        P_all={L: len(layer_to_map[L]) for L in layer_numbers},\n",
    "        P_common=P_common,\n",
    "        removed_per_layer=removed_per_layer\n",
    "    )\n",
    "    return ordered_query_ids, X_list, layer_numbers, stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_ids, X_list, layers, stats = align_layers_to_common_prompts(\n",
    "    activations,\n",
    "    order=\"sorted\",      # or \"ref\", ref_layer=0\n",
    "    as_sparse=True,      # recommended\n",
    "    zero_tol=0.0,        # set e.g. 1e-6 if you want to prune tiny values\n",
    "    dtype=np.float32,\n",
    ")\n",
    "\n",
    "print(\"common prompts:\", stats[\"P_common\"])\n",
    "print(\"dims per layer:\", stats[\"dims_per_layer\"])\n",
    "print(\"removed per layer:\", stats[\"removed_per_layer\"])\n",
    "# X_list is now aligned: for every layer L, row i corresponds to prompt ordered_ids[i]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _ensure_csr_float32(X) -> sp.csr_matrix:\n",
    "    \"\"\"Ensure SciPy CSR float32 without densifying.\"\"\"\n",
    "    if sp.issparse(X):\n",
    "        X = X.tocsr()\n",
    "        if X.dtype != np.float32:\n",
    "            X = X.astype(np.float32, copy=False)\n",
    "        return X\n",
    "    # dense path: convert to CSR without copying huge intermediates\n",
    "    X = np.asarray(X)\n",
    "    if X.dtype != np.float32:\n",
    "        X = X.astype(np.float32, copy=False)\n",
    "    return sp.csr_matrix(X)  # respects zeros; safe if you have many zeros\n",
    "\n",
    "\n",
    "def _idf_vector_from_csr(X_csr: sp.csr_matrix, smooth: float = 1.0) -> np.ndarray:\n",
    "    \"\"\"Compute column-wise IDF: log((P+smooth)/(df+smooth))+1, float32.\"\"\"\n",
    "    P, N = X_csr.shape\n",
    "    # df: nonzero count per column\n",
    "    df = np.diff(X_csr.tocsc().indptr).astype(np.float32, copy=False)  # length N\n",
    "    idf = np.log((np.float32(P) + smooth) / (df + smooth)) + np.float32(1.0)\n",
    "    return idf.astype(np.float32, copy=False)\n",
    "\n",
    "\n",
    "def _apply_tfidf_rowl2(X_csr: sp.csr_matrix, idf: np.ndarray) -> sp.csr_matrix:\n",
    "    \"\"\"Right-multiply by diag(idf) then row L2 normalize (all sparse).\"\"\"\n",
    "    X = X_csr @ sp.diags(idf, dtype=np.float32, format=\"csr\")\n",
    "    # row L2 norms\n",
    "    sq = X.multiply(X).sum(axis=1).A1.astype(np.float32, copy=False)\n",
    "    inv = np.float32(1.0) / np.sqrt(np.maximum(sq, np.float32(1e-12)))\n",
    "    return sp.diags(inv, dtype=np.float32, format=\"csr\") @ X\n",
    "\n",
    "\n",
    "def _countsketch_batch_to_dense(\n",
    "    sub_csr: sp.csr_matrix,\n",
    "    buckets: np.ndarray,   # (N_l,) int32 in [0, out_dim)\n",
    "    signs:   np.ndarray,   # (N_l,) int8  in {-1, +1}\n",
    "    out_dim: int,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    CountSketch current batch (rows of CSR) into a small dense (rows x out_dim).\n",
    "    We use a sparse COO to accumulate into buckets, then .toarray().\n",
    "    \"\"\"\n",
    "    coo = sub_csr.tocoo(copy=False)\n",
    "    if coo.nnz == 0:\n",
    "        return np.zeros((sub_csr.shape[0], out_dim), dtype=np.float32)\n",
    "    hb = buckets[coo.col].astype(np.int32, copy=False)\n",
    "    sg = signs[coo.col].astype(np.float32, copy=False)\n",
    "    vals = (coo.data.astype(np.float32, copy=False) * sg)\n",
    "    sketch = sp.coo_matrix((vals, (coo.row.astype(np.int32, copy=False), hb)),\n",
    "                           shape=(sub_csr.shape[0], out_dim), dtype=np.float32)\n",
    "    sketch.sum_duplicates()\n",
    "    return sketch.toarray()  # tiny dense (e.g., 2048 x 64)\n",
    "\n",
    "\n",
    "def _mutualize_topk_numpy(\n",
    "    idx_k: np.ndarray, sim_k: np.ndarray, P: int\n",
    ") -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Turn directed top-k neighbors into mutual, undirected edges.\n",
    "    Returns unique (src, dst, weight) with src<dst and weight = mean of both dirs.\n",
    "    idx_k: (P, k) int32 neighbors; sim_k: (P, k) float32 similarities (cosines).\n",
    "    \"\"\"\n",
    "    k = idx_k.shape[1]\n",
    "    src = np.repeat(np.arange(P, dtype=np.int32), k)\n",
    "    dst = idx_k.reshape(-1).astype(np.int32, copy=False)\n",
    "    w   = sim_k.reshape(-1).astype(np.float32, copy=False)\n",
    "\n",
    "    # drop self and invalid (-1)\n",
    "    mask = (src != dst) & (dst >= 0)\n",
    "    a = np.minimum(src[mask], dst[mask])\n",
    "    b = np.maximum(src[mask], dst[mask])\n",
    "    w = w[mask]\n",
    "\n",
    "    order = np.lexsort((b, a))\n",
    "    a, b, w = a[order], b[order], w[order]\n",
    "    eq = (a[1:] == a[:-1]) & (b[1:] == b[:-1])\n",
    "    hit = np.where(eq)[0]  # indices of second in each pair\n",
    "\n",
    "    src_u = a[hit]\n",
    "    dst_u = b[hit]\n",
    "    w_u   = 0.5 * (w[hit] + w[hit + 1])\n",
    "    return src_u, dst_u, w_u\n",
    "\n",
    "\n",
    "def _leiden_cpu_chunked(\n",
    "    src: np.ndarray, dst: np.ndarray, weight: np.ndarray, n: int,\n",
    "    resolution: float = 1.0, seed: int = 42, use_weights: bool = True,\n",
    "    edge_chunk: int = 200_000\n",
    ") -> Tuple[np.ndarray, float]:\n",
    "    \"\"\"Build graph and run Leiden, adding edges in chunks to limit RAM spikes.\"\"\"\n",
    "    g = ig.Graph()\n",
    "    g.add_vertices(int(n))\n",
    "    m = int(src.shape[0])\n",
    "    off = 0\n",
    "    for s in range(0, m, edge_chunk):\n",
    "        e = min(s + edge_chunk, m)\n",
    "        g.add_edges(zip(src[s:e], dst[s:e]))\n",
    "        if use_weights:\n",
    "            g.es[off:off+(e-s)][\"weight\"] = weight[s:e].tolist()\n",
    "        off += (e - s)\n",
    "\n",
    "    if use_weights:\n",
    "        part = la.find_partition(g, la.RBConfigurationVertexPartition,\n",
    "                                 weights=\"weight\", resolution_parameter=resolution, seed=seed)\n",
    "    else:\n",
    "        part = la.find_partition(g, la.RBConfigurationVertexPartition,\n",
    "                                 resolution_parameter=resolution, seed=seed)\n",
    "    return np.asarray(part.membership, dtype=np.int32), float(part.quality())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Pipeline Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Stable, memory-safe prompt community detection across layers.\n",
    "\n",
    "- Input: list of L layer matrices X_list, each shape (P, N_l).\n",
    "         Dense NumPy (float32) or SciPy CSR is accepted.\n",
    "- Output: community labels for P prompts, mutual-kNN graph, and diagnostics.\n",
    "\n",
    "Design:\n",
    "- Per layer: TF-IDF (column IDF + row L2) in sparse, then CountSketch to rp_dim.\n",
    "- Concatenate per-layer sketches -> (P, D_total), final row L2 normalization.\n",
    "- FAISS (CPU) HNSW kNN on the compact features (batched).\n",
    "- Mutual-kNN graph + Leiden (CPU), with chunked edge loads to limit RAM spikes.\n",
    "\"\"\"\n",
    "def cluster_prompts_multilayer_cpu_safe(\n",
    "    X_list: List[Union[np.ndarray, sp.spmatrix]],  # L layers, each (P, N_l)\n",
    "    rp_dim: Union[int, List[int]] = 64,            # per-layer sketch dim (int or list[L])\n",
    "    k: int = 40,                                   \n",
    "    hnsw_M: int = 32,                               # HNSW graph degree\n",
    "    efC: int = 200,                                 # HNSW construction ef\n",
    "    efS: int = 64,                                  # HNSW search ef\n",
    "    batch_rows: int = 2048,                         # rows per batch for sketching\n",
    "    resolution: float = 1.0,                        # Leiden resolution\n",
    "    seed: int = 42,\n",
    "    layer_weights: Optional[List[float]] = None,    # reliability weights per layer (√w scaling)\n",
    "    edge_chunk: int = 200_000,                      # edges per chunk to add to igraph\n",
    ") -> dict:\n",
    "\n",
    "    assert len(X_list) >= 1, \"Provide at least one layer.\"\n",
    "    P = X_list[0].shape[0]\n",
    "    for Xl in X_list:\n",
    "        assert Xl.shape[0] == P, \"All layers must have the same number of prompts.\"\n",
    "\n",
    "    if isinstance(rp_dim, int):\n",
    "        rp_dims = [int(rp_dim)] * len(X_list)\n",
    "    else:\n",
    "        assert len(rp_dim) == len(X_list)\n",
    "        rp_dims = [int(d) for d in rp_dim]\n",
    "\n",
    "    rng = np.random.RandomState(seed)\n",
    "    F_parts = []\n",
    "\n",
    "    # --- Per layer: sparse TF-IDF + CountSketch (batched) ---\n",
    "    for li, (Xl, dli) in enumerate(zip(X_list, rp_dims)):\n",
    "        X = _ensure_csr_float32(Xl)                      # CSR float32\n",
    "        idf = _idf_vector_from_csr(X)                    # (N_l, )\n",
    "\n",
    "        # hash functions\n",
    "        buckets = rng.randint(0, dli, size=X.shape[1], dtype=np.int32)\n",
    "        signs   = rng.choice(np.array([-1, 1], dtype=np.int8), size=X.shape[1])\n",
    "\n",
    "        F_l = np.empty((P, dli), dtype=np.float32)\n",
    "        for s in range(0, P, batch_rows):\n",
    "            e = min(s + batch_rows, P)\n",
    "            sub = X[s:e]                                 # CSR slice\n",
    "            sub = _apply_tfidf_rowl2(sub, idf)           # TF-IDF + row L2 (sparse)\n",
    "            F_batch = _countsketch_batch_to_dense(sub, buckets, signs, dli)\n",
    "            # optional layer weight (√w scaling)\n",
    "            if layer_weights is not None:\n",
    "                F_batch *= np.sqrt(np.float32(layer_weights[li]))\n",
    "            F_l[s:e] = F_batch\n",
    "\n",
    "        # light per-layer row L2 (helps if layer coverage differs)\n",
    "        norms = np.linalg.norm(F_l, axis=1, keepdims=True)\n",
    "        norms[norms < 1e-8] = 1e-8\n",
    "        F_l /= norms\n",
    "        F_parts.append(F_l)\n",
    "\n",
    "    # --- Fuse across layers: concat + FINAL row L2 normalization ---\n",
    "    F_host = np.concatenate(F_parts, axis=1).astype(np.float32)  # (P, D_total)\n",
    "    del F_parts\n",
    "    norms = np.linalg.norm(F_host, axis=1, keepdims=True)\n",
    "    norms[norms < 1e-8] = 1e-8\n",
    "    F_host /= norms\n",
    "    d_total = int(F_host.shape[1])\n",
    "\n",
    "    # --- kNN via FAISS (CPU) HNSW (cosine via L2 on unit vectors) ---\n",
    "    # For unit vectors, cosine(x,y) = 1 - 0.5 * ||x - y||^2\n",
    "    index = faiss.IndexHNSWFlat(d_total, hnsw_M)  # L2 by default\n",
    "    index.hnsw.efConstruction = int(efC)\n",
    "    index.hnsw.efSearch = int(efS)\n",
    "\n",
    "    index.add(F_host)  # build graph; multi-threaded in FAISS\n",
    "    kk = min(max(2, k + 8), P)  # ask a few extras; we'll drop self\n",
    "\n",
    "    # batched search to limit RAM\n",
    "    I = np.empty((P, kk), dtype=np.int64)\n",
    "    D = np.empty((P, kk), dtype=np.float32)  # squared L2 distances\n",
    "    bs = 8192\n",
    "    for s in range(0, P, bs):\n",
    "        e = min(s + bs, P)\n",
    "        Di, Ii = index.search(F_host[s:e], kk)\n",
    "        D[s:e] = Di\n",
    "        I[s:e] = Ii\n",
    "\n",
    "    # Convert distances to cosine sims, drop self/invalid, keep top-k\n",
    "    idx_k = np.empty((P, k), dtype=np.int32)\n",
    "    sim_k = np.empty((P, k), dtype=np.float32)\n",
    "    for i in range(P):\n",
    "        neigh = I[i]\n",
    "        dist2 = D[i]\n",
    "        # FAISS returns -1 when no neighbor; remove invalids and self\n",
    "        good = (neigh >= 0) & (neigh != i)\n",
    "        neigh = neigh[good]\n",
    "        dist2 = dist2[good]\n",
    "        sim = 1.0 - 0.5 * dist2  # cosine for unit vectors\n",
    "        # take top-k by similarity\n",
    "        take = min(k, neigh.shape[0])\n",
    "        if take == 0:\n",
    "            idx_k[i, :] = i\n",
    "            sim_k[i, :] = 0.0\n",
    "        else:\n",
    "            order = np.argsort(-sim)[:take]\n",
    "            idx_k[i, :take] = neigh[order].astype(np.int32, copy=False)\n",
    "            sim_k[i, :take] = sim[order].astype(np.float32, copy=False)\n",
    "            if take < k:\n",
    "                idx_k[i, take:] = idx_k[i, :1]\n",
    "                sim_k[i, take:] = sim_k[i, :1]\n",
    "\n",
    "    # --- Mutual kNN graph (unique undirected edges) ---\n",
    "    src, dst, weight = _mutualize_topk_numpy(idx_k, sim_k, P)\n",
    "\n",
    "    # --- Leiden (chunked edges to avoid RAM spikes) ---\n",
    "    labels, quality = _leiden_cpu_chunked(\n",
    "        src, dst, weight, n=P, resolution=resolution, seed=seed,\n",
    "        use_weights=True, edge_chunk=edge_chunk\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"labels\": labels,                    # (P,) community id per prompt\n",
    "        \"edges\": (src, dst, weight),         # mutual-kNN graph\n",
    "        \"quality\": quality,                  # modularity-like score\n",
    "        \"params\": {\n",
    "            \"P\": P, \"layer_dims\": [X.shape[1] for X in X_list], \"rp_dims\": rp_dims,\n",
    "            \"k\": int(k), \"hnsw_M\": int(hnsw_M), \"efC\": int(efC), \"efS\": int(efS),\n",
    "            \"batch_rows\": int(batch_rows), \"resolution\": float(resolution),\n",
    "        },\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = cluster_prompts_multilayer_cpu_safe(\n",
    "    X_list,\n",
    "    rp_dim=64,         # try 64 first; if stability is low, try 96\n",
    "    k=40,\n",
    "    hnsw_M=32,         # larger M => better recall/slower build; 32–48 good\n",
    "    efC=200,           # construction quality\n",
    "    efS=64,            # search quality (raise to 96 for higher recall)\n",
    "    batch_rows=2048,   # reduce to 1024 if RAM is tight during sketching\n",
    "    resolution=1.0,\n",
    "    seed=42,\n",
    ")\n",
    "labels = out[\"labels\"]\n",
    "print(\"Leiden quality:\", out[\"quality\"], \"num clusters:\", len(set(labels)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting the Communities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============== Basics & Graph Construction =================\n",
    "\n",
    "def partition_summary(labels: np.ndarray) -> Dict:\n",
    "    labels = np.asarray(labels, dtype=np.int32)\n",
    "    comms, counts = np.unique(labels, return_counts=True)\n",
    "    order = np.argsort(-counts)\n",
    "    return {\n",
    "        \"n_nodes\": labels.size,\n",
    "        \"n_comms\": comms.size,\n",
    "        \"sizes_sorted\": counts[order],\n",
    "        \"comms_sorted\": comms[order],\n",
    "    }\n",
    "\n",
    "# =============== Prototypes (per community & layer) =================\n",
    "\n",
    "def community_prototypes_by_layer(\n",
    "    X_list: List[Union[np.ndarray, sp.spmatrix]],\n",
    "    labels: np.ndarray,\n",
    "    normalize: bool = False  # if True: z-score neurons across prompts before averaging\n",
    ") -> List[np.ndarray]:\n",
    "    \"\"\"\n",
    "    For each layer ℓ: return a (C, N_ℓ) array, mean activation per community.\n",
    "    Optional: z-score across prompts first to remove neuron bias.\n",
    "    \"\"\"\n",
    "    labels = labels.astype(np.int32)\n",
    "    comms, inv = np.unique(labels, return_inverse=True)\n",
    "    C = comms.size\n",
    "    protos = []\n",
    "    for X in X_list:\n",
    "        if sp.issparse(X):\n",
    "            X = X.tocsr()\n",
    "            if normalize:\n",
    "                # z-score columns using sparse stats\n",
    "                mu = X.mean(axis=0).A1\n",
    "                Xc = X - sp.csr_matrix(np.broadcast_to(mu, X.shape))\n",
    "                # var: E[(x-mu)^2]; approximate via sparse (ignore implicit zeros correction)\n",
    "                var = (Xc.multiply(Xc)).mean(axis=0).A1\n",
    "                sd = np.sqrt(np.maximum(var, 1e-8))\n",
    "                Xn = (Xc @ sp.diags(1.0/sd, dtype=np.float32))\n",
    "            else:\n",
    "                Xn = X\n",
    "            # aggregate by community: sum rows per community then divide by counts\n",
    "            counts = np.bincount(inv, minlength=C).astype(np.float32)\n",
    "            # build indicator sparse matrix for communities\n",
    "            S = sp.csr_matrix((np.ones(Xn.shape[0], dtype=np.float32), (np.arange(Xn.shape[0]), inv)), shape=(Xn.shape[0], C))\n",
    "            M = (S.T @ Xn).toarray()  # (C, N)\n",
    "            M = (M.T / counts).T\n",
    "        else:\n",
    "            X = np.asarray(X, dtype=np.float32)\n",
    "            if normalize:\n",
    "                mu = X.mean(axis=0, keepdims=True); sd = X.std(axis=0, keepdims=True) + 1e-8\n",
    "                Xn = (X - mu) / sd\n",
    "            else:\n",
    "                Xn = X\n",
    "            C = comms.size\n",
    "            M = np.zeros((C, Xn.shape[1]), dtype=np.float32)\n",
    "            for c_idx, c in enumerate(comms):\n",
    "                M[c_idx] = Xn[labels == c].mean(axis=0)\n",
    "        protos.append(M.astype(np.float32))\n",
    "    return protos  # list of length L, each (C, N_l)\n",
    "\n",
    "def top_neurons_for_community(\n",
    "    layer_protos: np.ndarray,  # (C, N_l) for one layer\n",
    "    community_index: int, topk: int = 20,\n",
    "    compare_to: str = \"global\"  # \"global\" or \"others\"\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Rank neurons for one community: returns (indices, scores).\n",
    "    'global': score = mean_c - global_mean\n",
    "    'others': score = mean_c - mean_over_other_communities\n",
    "    \"\"\"\n",
    "    C, N = layer_protos.shape\n",
    "    mean_c = layer_protos[community_index]\n",
    "    if compare_to == \"global\":\n",
    "        base = layer_protos.mean(axis=0)\n",
    "    else:\n",
    "        base = (layer_protos.sum(axis=0) - mean_c) / max(C-1, 1)\n",
    "    scores = mean_c - base\n",
    "    idx = np.argsort(-scores)[:topk]\n",
    "    return idx, scores[idx]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "edges  = out[\"edges\"]             \n",
    "P = len(labels)\n",
    "src, dst, w = edges\n",
    "\n",
    "# Basic summary\n",
    "summ = partition_summary(labels)\n",
    "print(\"nodes:\", summ[\"n_nodes\"], \"communities:\", summ[\"n_comms\"])\n",
    "print(\"largest sizes:\", summ[\"sizes_sorted\"][:10])\n",
    "\n",
    "# Number of isolated components\n",
    "deg = np.zeros(P, dtype=np.int32)\n",
    "np.add.at(deg, src, 1); np.add.at(deg, dst, 1)\n",
    "print(\"deg=0:\", (deg==0).sum(), \"deg=1:\", (deg==1).sum())\n",
    "print(\"deg quantiles:\", np.quantile(deg, [0, .1, .25, .5, .75, .9, .99, 1.0]))\n",
    "\n",
    "g = ig.Graph(n=P, edges=list(zip(src.tolist(), dst.tolist())), directed=False)\n",
    "print(\"components:\", g.components().summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back Into Query IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ids_by_community(labels: np.ndarray, prompt_ids: np.ndarray, min_size: int = 2, sort_by_size: bool = True):\n",
    "    \"\"\"\n",
    "    prompt_ids: array-like of length P (strings/ints). Returns {community_id: np.ndarray(ids)}.\n",
    "    \"\"\"\n",
    "    labels = np.asarray(labels, dtype=np.int32)\n",
    "    prompt_ids = np.asarray(prompt_ids)\n",
    "    comms, counts = np.unique(labels, return_counts=True)\n",
    "    keep = counts >= min_size\n",
    "    comms_k = comms[keep]\n",
    "    counts_k = counts[keep]\n",
    "    order = np.argsort(-counts_k) if sort_by_size else np.arange(len(comms_k))\n",
    "    comms_k = comms_k[order]\n",
    "\n",
    "    groups = {int(c): prompt_ids[labels == c] for c in comms_k}\n",
    "    sizes  = {int(c): int((labels == c).sum()) for c in comms_k}\n",
    "    return groups, sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "community_to_ids, sizes = ids_by_community(labels, ordered_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Saving community -> queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qid_to_queries = torch.load(\"../q_ids_to_queries.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "community_to_queries = {com : [qid_to_queries[id] for id in community_to_ids[com]] for com in community_to_ids}\n",
    "\n",
    "with open(\"communities_to_queries.json\",\"w\") as f:\n",
    "    json.dump(community_to_queries, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
