{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-16T12:09:31.624113500Z",
     "start_time": "2025-08-16T12:09:31.624113500Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XwreR5PRr_ux",
    "outputId": "e6ab6ffe-2143-4339-e93d-2ca1c84fc364"
   },
   "outputs": [],
   "source": [
    "#%pip install transformer_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-16T12:09:41.919945200Z",
     "start_time": "2025-08-16T12:09:31.624113500Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ye2RpUerr4im",
    "outputId": "8e6acad9-13ec-46eb-c733-a06cc6f37897"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shir.mey/miniconda3/envs/Transformers/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7fd5365aeb50>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformer_lens import HookedEncoderDecoder\n",
    "import transformer_lens.utils as utils\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from transformer_lens.loading_from_pretrained import OFFICIAL_MODEL_NAMES\n",
    "\n",
    "import torch\n",
    "\n",
    "torch.set_grad_enabled(False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "doufxfLpvbGN"
   },
   "source": [
    "## Loading the Model in TransformerLens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_QyxxC8cw0PF"
   },
   "source": [
    "Please download the model first: https://cloud.anja.re/s/Qpo8CZ6yRzDH7ZF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-16T12:11:54.215759100Z",
     "start_time": "2025-08-16T12:09:41.918943400Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "5xCPM0GUr0d5",
    "outputId": "7056f183-d1af-448e-980d-f135eb390b73"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for T5 in TransformerLens is currently experimental, until such a time when it has feature parity with HookedTransformer and has been tested on real research tasks. Until then, backward compatibility is not guaranteed. Please see the docs for information on the limitations of the current implementation.\n",
      "If using T5 for interpretability research, keep in mind that T5 has some significant architectural differences to GPT. The major one is that T5 is an Encoder-Decoder modelAlso, it uses relative positional embeddings, different types of Attention (without bias) and LayerNorm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model ../DSI-large-TriviaQA into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "# !wget \"https://cloud.anja.re/s/qckH8GQPyN6YK8w/download?path=%2F&files=DSI-large-TriviaQA.zip\"\n",
    "# !unzip \"download?path=%2F&files=DSI-large-TriviaQA.zip\"\n",
    "checkpoint = \"../DSI-large-TriviaQA\"\n",
    "\n",
    "OFFICIAL_MODEL_NAMES.append(checkpoint)\n",
    "\n",
    "hf_model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "device = utils.get_device()\n",
    "model = HookedEncoderDecoder.from_pretrained(checkpoint, hf_model=hf_model, device=device)\n",
    "\n",
    "tokenizer_t5 = AutoTokenizer.from_pretrained('google-t5/t5-large')\n",
    "\n",
    "\n",
    "# Our model has a new token for each document id that we trained it on.\n",
    "\n",
    "# token id of first document that was added\n",
    "first_added_doc_id = len(tokenizer_t5)\n",
    "# token id of the last document that was added\n",
    "last_added_doc_id = len(tokenizer_t5) + (len(tokenizer) - len(tokenizer_t5))\n",
    "del tokenizer_t5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GXJ_CdZaveA2"
   },
   "source": [
    "## Running a sample query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-16T12:11:54.695157500Z",
     "start_time": "2025-08-16T12:11:54.429401800Z"
    },
    "id": "WjIsOKZ8Etj4"
   },
   "outputs": [],
   "source": [
    "#wget \"https://cloud.anja.re/s/qckH8GQPyN6YK8w/download?path=%2FGenIR-Data&files=TriviaQAData.zip\"\n",
    "#unzip \"download?path=%2FGenIR-Data&files=TriviaQAData.zip\"\n",
    "\n",
    "import json\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "with open(\"../TriviaQAData/train_queries_trivia_qa.json\", mode='r') as f:\n",
    "  train_data = json.load(f)\n",
    "with open(\"../TriviaQAData/val_queries_trivia_qa.json\", mode='r') as f:\n",
    "  val_data = json.load(f)\n",
    "data = train_data + val_data\n",
    "\n",
    "class QuestionsDataset(Dataset):\n",
    "    def __init__(self, inputs, targets, ids):\n",
    "        self.inputs = inputs\n",
    "        self.targets = targets\n",
    "        self.ids = ids\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.inputs[idx], self.targets[idx], self.ids[idx]\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        input_texts, target_texts, ids = zip(*batch)\n",
    "        return input_texts, target_texts, ids\n",
    "\n",
    "\n",
    "queries = [entry['query'] for entry in data]\n",
    "ground_truths = [entry['relevant_docs'] for entry in data]\n",
    "query_ids = [entry['id'] for entry in data]\n",
    "q_ids_to_qs = {id : query for id, query in zip(query_ids, queries)}\n",
    "dataset = QuestionsDataset(queries, ground_truths, query_ids)\n",
    "data_loader = DataLoader(dataset, batch_size=16, shuffle = False, collate_fn=dataset.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-16T12:11:54.695157500Z",
     "start_time": "2025-08-16T12:11:54.695157500Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_activated_neurons(hooks, layer_indices):\n",
    "    result_dict = {}\n",
    "    for layer_id in layer_indices:\n",
    "        hook_post = hooks[f'decoder.{layer_id}.mlp.hook_post']\n",
    "        activated_neuron_indices = (hook_post > 0).nonzero(as_tuple=False)\n",
    "        result_dict[f'layer_{layer_id}'] = activated_neuron_indices\n",
    "    return result_dict\n",
    "\n",
    "def pad_relevant_docs(relevant_docs):\n",
    "    relevant_docs = [[int(item) for item in sublist] for sublist in relevant_docs]\n",
    "    max_len = 0\n",
    "    for sublist in relevant_docs:\n",
    "        if len(sublist) > max_len:\n",
    "            max_len = len(sublist)\n",
    "    padded_relevant_docs = []\n",
    "    for sublist in relevant_docs:\n",
    "        # Calculate how many padding elements are needed\n",
    "        num_padding = max_len - len(sublist)\n",
    "        # Create the padded sublist\n",
    "        padded_sublist = sublist + [-1] * num_padding\n",
    "        padded_relevant_docs.append(padded_sublist)\n",
    "    return padded_relevant_docs\n",
    "\n",
    "def extract_doc_ids_from_output(decoder_output):\n",
    "    doc_out_ids = []\n",
    "    for out in doc_out:\n",
    "        doc_id = re.findall(r\"@DOC_ID_([0-9]+)@\", decoder_output)\n",
    "        assert len(doc_id) <= 1\n",
    "        doc_out_ids.append(doc_id[0] if len(doc_id) else '-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-16T12:21:57.236403700Z",
     "start_time": "2025-08-16T12:21:57.137006100Z"
    }
   },
   "outputs": [],
   "source": [
    "def make_mlp_hook_function(target_token_pos, target_neuron_index, new_activation_value):\n",
    "    def modify_mlp_neuron_hook(\n",
    "        activation_tensor: torch.Tensor, \n",
    "        hook\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        A hook function to modify a specific MLP neuron's activation.\n",
    "        activation_tensor shape: [batch, position, n_mlp_neurons]\n",
    "        \"\"\"\n",
    "        # print(f\"Hook fired at {hook.name}. Original activation value at \"\n",
    "        #       f\"pos {target_token_pos}, neuron {target_neuron_index}: \"\n",
    "        #       f\"{activation_tensor[:, target_token_pos, target_neuron_index]}\")\n",
    "              # f\"{activation_tensor[0, target_token_pos, target_neuron_index].item():.4f}\")\n",
    "    \n",
    "        # Modify the specific neuron's activation in-place\n",
    "        # We use [0] for batch dimension assuming a single prompt\n",
    "        activation_tensor[:, target_token_pos, target_neuron_index] = new_activation_value\n",
    "        # activation_tensor[0, target_token_pos, target_neuron_index] = new_activation_value\n",
    "    \n",
    "        # print(f\"Modified activation to: \"\n",
    "        #       f\"{activation_tensor[:, target_token_pos, target_neuron_index]}\")\n",
    "              # f\"{activation_tensor[0, target_token_pos, target_neuron_index].item():.4f}\")\n",
    "    \n",
    "        return activation_tensor # Always return the modified tensor\n",
    "\n",
    "    return modify_mlp_neuron_hook\n",
    "\n",
    "def run_model_with_activation_hook(model, prompt, mlp_hook_name, neuron_index, neuron_new_value):\n",
    "    # mlp_hook_name = f\"blocks.{target_layer}.mlp.hook_post\"\n",
    "    # Now, run with the hook\n",
    "    modified_logits = model.run_with_hooks(\n",
    "        prompt,\n",
    "        fwd_hooks=[(mlp_hook_name, make_mlp_hook_function(0, neuron_index, neuron_new_value))]\n",
    "    )\n",
    "    hook_result = tokenizer.batch_decode(torch.argmax(modified_logits, dim=-1).squeeze(-1))\n",
    "\n",
    "    logits = model(prompt)\n",
    "    orig_result = tokenizer.batch_decode(torch.argmax(logits, dim=-1).squeeze(-1))\n",
    "    \n",
    "    print(f'original result:{orig_result}, and after using the hook:{hook_result}')\n",
    "    correct_count = 0\n",
    "    for i in range(len(orig_result)):\n",
    "        if orig_result[i] == hook_result[i]:\n",
    "            correct_count += 1\n",
    "    print(f'Total correct answered:{correct_count}/ {len(orig_result)}')\n",
    "\n",
    "def get_affected_prompts(model, queries_dict, mlp_hook_name, layer_index, neuron_index, neuron_new_value):\n",
    "    layer_activated_neuron_inputs, layer_activated_neurons_correct_doc_ids = [], []\n",
    "    for key in queries_dict:\n",
    "        if neuron_index in queries_dict[key]['activated_neurons'][f'layer_{layer_id}']:\n",
    "            layer_activated_neuron_inputs.append(results_copy[key]['input'])\n",
    "            layer_activated_neurons_correct_doc_ids.append(results_copy[key]['correct_doc_id'])\n",
    "    return run_model_with_activation_hook(model, layer_activated_neuron_inputs, mlp_hook_name, neuron_index, neuron_new_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Examining the activations\n",
    "\n",
    "The activations of each component in the transformer are stored in the `cache` object. It's basically a dict from which you choose which component to look at.\n",
    "\n",
    "Here, we print all possible component keys for layer 0 in the decoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-16T12:11:54.779011800Z",
     "start_time": "2025-08-16T12:11:54.706278100Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for key in cache.keys():\n",
    "#   if key.startswith('decoder.0.'):\n",
    "#     print(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "22RgxkfDwm1R"
   },
   "source": [
    "We choose to look at the output of the MLP in layer 19 of the decoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-16T12:11:54.811015100Z",
     "start_time": "2025-08-16T12:11:54.779011800Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vfjtRp-pvl2I",
    "outputId": "69c417e6-a16e-4bfb-bd27-7ed671219998"
   },
   "outputs": [],
   "source": [
    "# cache['decoder.19.hook_mlp_out'], cache['decoder.19.hook_mlp_out'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vhL0VlzwwYrk"
   },
   "source": [
    "Take a look at where the MLP hooks are computed: https://github.com/TransformerLensOrg/TransformerLens/blob/main/transformer_lens/components/mlps/mlp.py\n",
    "\n",
    "`hook_pre`: Before activation,\n",
    "`hook_post`: After applying activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shir's Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-16T12:11:54.811015100Z",
     "start_time": "2025-08-16T12:11:54.808002800Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_logits(model, prompt, mlp_hook_name, neuron_index):\n",
    "    modified_logits = model.run_with_hooks(\n",
    "        prompt,\n",
    "        fwd_hooks=[(mlp_hook_name, make_mlp_hook_function(0, neuron_index, 0.0))]\n",
    "    )\n",
    "    logits = model(prompt)\n",
    "    return logits, modified_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking change of logits after neuron zeroing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-16T12:11:54.811015100Z",
     "start_time": "2025-08-16T12:11:54.808002800Z"
    }
   },
   "outputs": [],
   "source": [
    "def change_by_neuron_zeroing(logits, modified_logits):\n",
    "    l2_change = torch.linalg.vector_norm(logits - modified_logits)\n",
    "    cosine_similarity = torch.nn.functional.cosine_similarity(logits, modified_logits)\n",
    "    return l2_change, cosine_similarity\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the ratio between the max and second max logits (first and second prediction of doc ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-16T12:18:35.485381Z",
     "start_time": "2025-08-16T12:18:35.385948500Z"
    }
   },
   "outputs": [],
   "source": [
    "def logit_top2_difference(logits, modified_logits):\n",
    "    \n",
    "    ## return the difference caused to the original (unmodified) top 2 logits by modifying the neurons\n",
    "    \n",
    "    top_2_logits, top_2_logits_indices = torch.topk(logits, k=2)\n",
    "    top_2_logits = top_2_logits.squeeze(0)\n",
    "    top_2_logits_indices = top_2_logits_indices.squeeze(0)\n",
    "    modified_logits = modified_logits.squeeze(0)\n",
    "    difference = top_2_logits - modified_logits[top_2_logits_indices]\n",
    "    return difference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check layer output similarity to final logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-16T12:11:54.896878300Z",
     "start_time": "2025-08-16T12:11:54.895870900Z"
    }
   },
   "outputs": [],
   "source": [
    "def similarity_between_layer_output_and_logits(logits, layer_output_hook): \n",
    "    # I don't think is makes sense to run the model again as we have a saved dictionary of all hooks per layer, \n",
    "    # so this function is pretty simple right now.\n",
    "    return torch.nn.functional.cosine_similarity(logits, layer_output_hook)\n",
    "    # project layer output into logits space\n",
    "    # check if layer output is normalized\n",
    "    # talk to Anja about what is the best way to measure the contribution to the final logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Project into vocabulary dimension - copying Anja's code into here. I don't really understand how to use it though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-16T12:11:54.896878300Z",
     "start_time": "2025-08-16T12:11:54.895870900Z"
    }
   },
   "outputs": [],
   "source": [
    "def process_resid(resid, model):\n",
    "    decoder_resid = model.decoder_final_ln(resid)\n",
    "\n",
    "    if model.cfg.tie_word_embeddings:\n",
    "        # Rescale output before projecting on vocab\n",
    "        # See https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/transformer/transformer.py#L586\n",
    "        decoder_resid *= model.cfg.d_model**-0.5\n",
    "\n",
    "    return decoder_resid\n",
    "\n",
    "def logit_lens_decoder(resid, model):\n",
    "    decoder_resid = process_resid(resid, model=model)\n",
    "    logits = model.unembed(decoder_resid)\n",
    "\n",
    "    return logits\n",
    "\n",
    "    # check what decoder_final_ln is and if we have it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-16T12:12:12.763234500Z",
     "start_time": "2025-08-16T12:11:54.982418400Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('../activated_neurons_train_val_copy.json', 'r') as f:\n",
    "        activated_neurons = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-16T12:12:47.164535200Z",
     "start_time": "2025-08-16T12:12:12.894391900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('../neuron_activation_stats_with_entries.json', 'r') as f:\n",
    "    neuron_stats = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-16T12:37:34.963497500Z",
     "start_time": "2025-08-16T12:36:58.527595Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modifying neuron 1\n",
      "modifying neuron 513\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     15\u001b[39m batches = [(queries[i : i+\u001b[32m32\u001b[39m], ids[i : i+\u001b[32m32\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(queries), \u001b[32m32\u001b[39m)]\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m q_batch, ids_batch \u001b[38;5;129;01min\u001b[39;00m batches:\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m     b_logits, b_modified_logits = get_logits(model, q_batch, \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mdecoder.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.mlp.hook_post\u001b[39m\u001b[33m'\u001b[39m, \u001b[38;5;28mint\u001b[39m(neuron))\n\u001b[32m     18\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m logits, modified_logits, \u001b[38;5;28mid\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(b_logits, b_modified_logits, ids_batch):\n\u001b[32m     19\u001b[39m         l2, cos_sim = change_by_neuron_zeroing(logits, modified_logits)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 2\u001b[39m, in \u001b[36mget_logits\u001b[39m\u001b[34m(model, prompt, mlp_hook_name, neuron_index)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_logits\u001b[39m(model, prompt, mlp_hook_name, neuron_index):\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     modified_logits = model.run_with_hooks(\n\u001b[32m      3\u001b[39m         prompt,\n\u001b[32m      4\u001b[39m         fwd_hooks=[(mlp_hook_name, make_mlp_hook_function(\u001b[32m0\u001b[39m, neuron_index, \u001b[32m0.0\u001b[39m))]\n\u001b[32m      5\u001b[39m     )\n\u001b[32m      6\u001b[39m     logits = model(prompt)\n\u001b[32m      7\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m logits, modified_logits\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/Transformers/lib/python3.11/site-packages/transformer_lens/hook_points.py:447\u001b[39m, in \u001b[36mHookedRootModule.run_with_hooks\u001b[39m\u001b[34m(self, fwd_hooks, bwd_hooks, reset_hooks_end, clear_contexts, *model_args, **model_kwargs)\u001b[39m\n\u001b[32m    442\u001b[39m     logging.warning(\n\u001b[32m    443\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWARNING: Hooks will be reset at the end of run_with_hooks. This removes the backward hooks before a backward pass can occur.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    444\u001b[39m     )\n\u001b[32m    446\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.hooks(fwd_hooks, bwd_hooks, reset_hooks_end, clear_contexts) \u001b[38;5;28;01mas\u001b[39;00m hooked_model:\n\u001b[32m--> \u001b[39m\u001b[32m447\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m hooked_model.forward(*model_args, **model_kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/Transformers/lib/python3.11/site-packages/transformer_lens/HookedEncoderDecoder.py:284\u001b[39m, in \u001b[36mHookedEncoderDecoder.forward\u001b[39m\u001b[34m(self, input, decoder_input, return_type, one_zero_attention_mask)\u001b[39m\n\u001b[32m    277\u001b[39m decoder_positional_bias = cast(\n\u001b[32m    278\u001b[39m     T5Block, \u001b[38;5;28mself\u001b[39m.decoder[\u001b[32m0\u001b[39m]\n\u001b[32m    279\u001b[39m ).attn.compute_relative_attention_bias(\n\u001b[32m    280\u001b[39m     decoder_query_len, decoder_key_len, device=\u001b[38;5;28mself\u001b[39m.cfg.device\n\u001b[32m    281\u001b[39m )\n\u001b[32m    283\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m decoder_block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.decoder:\n\u001b[32m--> \u001b[39m\u001b[32m284\u001b[39m     decoder_resid = decoder_block(\n\u001b[32m    285\u001b[39m         resid_pre=decoder_resid,\n\u001b[32m    286\u001b[39m         position_bias=decoder_positional_bias,\n\u001b[32m    287\u001b[39m         encoder_hidden_states=encoder_resid,\n\u001b[32m    288\u001b[39m         encoder_additive_attention_mask=additive_attention_mask,\n\u001b[32m    289\u001b[39m     )\n\u001b[32m    291\u001b[39m decoder_resid = \u001b[38;5;28mself\u001b[39m.decoder_final_ln(decoder_resid)\n\u001b[32m    293\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cfg.tie_word_embeddings:\n\u001b[32m    294\u001b[39m     \u001b[38;5;66;03m# Rescale output before projecting on vocab\u001b[39;00m\n\u001b[32m    295\u001b[39m     \u001b[38;5;66;03m# See https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/transformer/transformer.py#L586\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/Transformers/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_impl(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/Transformers/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(*args, **kwargs)\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/Transformers/lib/python3.11/site-packages/transformer_lens/components/t5_block.py:146\u001b[39m, in \u001b[36mT5Block.forward\u001b[39m\u001b[34m(self, resid_pre, additive_attention_mask, encoder_additive_attention_mask, position_bias, encoder_hidden_states, past_kv_cache_entry)\u001b[39m\n\u001b[32m    138\u001b[39m     resid_mid_cross = \u001b[38;5;28mself\u001b[39m.hook_resid_mid_cross(resid_mid + cross_attn_out)\n\u001b[32m    140\u001b[39m     mlp_in = (\n\u001b[32m    141\u001b[39m         resid_mid_cross\n\u001b[32m    142\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cfg.use_hook_mlp_in\n\u001b[32m    143\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.hook_mlp_in(resid_mid_cross.clone())\n\u001b[32m    144\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m146\u001b[39m     normalized_resid_mid = \u001b[38;5;28mself\u001b[39m.ln3(mlp_in)\n\u001b[32m    147\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    148\u001b[39m     mlp_in = (\n\u001b[32m    149\u001b[39m         resid_mid \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cfg.use_hook_mlp_in \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.hook_mlp_in(resid_mid.clone())\n\u001b[32m    150\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/Transformers/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_impl(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/Transformers/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(*args, **kwargs)\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/Transformers/lib/python3.11/site-packages/transformer_lens/components/rms_norm.py:44\u001b[39m, in \u001b[36mRMSNorm.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     40\u001b[39m     x = x.to(torch.float32)\n\u001b[32m     41\u001b[39m scale: Float[torch.Tensor, \u001b[33m\"\u001b[39m\u001b[33mbatch pos 1\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mself\u001b[39m.hook_scale(\n\u001b[32m     42\u001b[39m     (x.pow(\u001b[32m2\u001b[39m).mean(-\u001b[32m1\u001b[39m, keepdim=\u001b[38;5;28;01mTrue\u001b[39;00m) + \u001b[38;5;28mself\u001b[39m.eps).sqrt()\n\u001b[32m     43\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m x = \u001b[38;5;28mself\u001b[39m.hook_normalized(x / scale).to(\u001b[38;5;28mself\u001b[39m.cfg.dtype)  \u001b[38;5;66;03m# [batch, pos, length]\u001b[39;00m\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m x.device != \u001b[38;5;28mself\u001b[39m.w.device:\n\u001b[32m     47\u001b[39m     \u001b[38;5;28mself\u001b[39m.to(x.device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/Transformers/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_impl(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/Transformers/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(*args, **kwargs)\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/Transformers/lib/python3.11/site-packages/transformer_lens/hook_points.py:156\u001b[39m, in \u001b[36mHookPoint.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    153\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ctx\n\u001b[32m    154\u001b[39m     \u001b[38;5;28mself\u001b[39m.ctx = {}\n\u001b[32m--> \u001b[39m\u001b[32m156\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) -> Tensor:\n\u001b[32m    157\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mlayer\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    160\u001b[39m     \u001b[38;5;66;03m# Returns the layer index if the name has the form 'blocks.{layer}.{...}'\u001b[39;00m\n\u001b[32m    161\u001b[39m     \u001b[38;5;66;03m# Helper function that's mainly useful on HookedTransformer\u001b[39;00m\n\u001b[32m    162\u001b[39m     \u001b[38;5;66;03m# If it doesn't have this form, raises an error -\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "logit_changes = defaultdict(lambda: {})\n",
    "for layer in neuron_stats['by_layer'].keys():\n",
    "    layer_id = int(''.join(c for c in layer if c.isdigit()))\n",
    "    layer_dict = neuron_stats['by_layer'][layer]\n",
    "    for neuron in layer_dict.keys():\n",
    "        l2_sum = 0\n",
    "        cos_sim_sum = 0\n",
    "        top_2_diff_sum = [0,0]\n",
    "        print(f\"modifying neuron {neuron}\")\n",
    "        ids = [id for id in layer_dict[neuron]['entries']]\n",
    "        queries = [q_ids_to_qs[id] for id in ids]\n",
    "        n = len(queries)\n",
    "        batches = [(queries[i : i+32], ids[i : i+32]) for i in range(0, len(queries), 32)]\n",
    "        for q_batch, ids_batch in batches:\n",
    "            b_logits, b_modified_logits = get_logits(model, q_batch, f'decoder.{layer_id}.mlp.hook_post', int(neuron))\n",
    "            for logits, modified_logits, id in zip(b_logits, b_modified_logits, ids_batch):\n",
    "                l2, cos_sim = change_by_neuron_zeroing(logits, modified_logits)\n",
    "                top2_diff = logit_top2_difference(logits, modified_logits)\n",
    "                l2_sum += l2\n",
    "                cos_sim_sum += cos_sim\n",
    "                top_2_diff_sum[0] += top2_diff[0]\n",
    "                top_2_diff_sum[1] += top2_diff[1]\n",
    "                \n",
    "        logit_changes[layer][neuron] = {\"avg l2\": l2_sum / n, \"avg cosine similarity\": cos_sim_sum / n, \"avg top2_difference\": [i/n for i in top_2_diff_sum]}    \n",
    "                \n",
    "with open(\"logit_changes_by_zeroing.json\", \"w\") as f:\n",
    "    json.dump(logit_changes, f)\n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-16T12:37:37.711302500Z",
     "start_time": "2025-08-16T12:37:37.629330800Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'avg l2': tensor(17.3075, device='cuda:0'),\n",
       " 'avg cosine similarity': tensor([1.], device='cuda:0'),\n",
       " 'avg top2_difference': [tensor(-0.0008, device='cuda:0'),\n",
       "  tensor(-1.8029e-05, device='cuda:0')]}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit_changes[\"layer_17\"][\"1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-08-16T12:13:13.246907300Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
